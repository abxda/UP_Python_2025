{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKsUa5x1i6qMK1YX3RATn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abxda/UP_Python_2025/blob/main/Semana_05_02_Lunes_UP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# SECCIÓN 1: INTRODUCCIÓN A LOS EMBEDDINGS DE TEXTO\n",
        "\n",
        "¿Qué son los Embeddings de Texto?\n",
        "\n",
        "Imagina que quieres que una computadora \"entienda\" el significado de las palabras y frases\n",
        "de la misma forma (o de una forma aproximada) a como lo hacemos los humanos.\n",
        "Los embeddings de texto son una manera de lograr esto.\n",
        "\n",
        "En esencia, un embedding de texto es una representación numérica de una porción de texto\n",
        "(puede ser una palabra, una frase, un párrafo o incluso un documento entero).\n",
        "Esta representación es un vector de números de punto flotante (por ejemplo, un array de 384, 768 o 1024 números).\n",
        "\n",
        "Características Clave de los Embeddings:\n",
        "1.  **Densos**: A diferencia de representaciones más antiguas como \"one-hot encoding\" que son dispersas\n",
        "    (muchos ceros), los embeddings son vectores densos (la mayoría de los valores son no nulos).\n",
        "2.  **Semántica Capturada**: Lo más importante es que estos vectores están diseñados de tal manera\n",
        "    que textos con significados similares tendrán vectores numéricos (embeddings) cercanos\n",
        "    en el espacio vectorial. Por ejemplo, el embedding de \"rey\" estará cerca del embedding de \"reina\",\n",
        "    y el embedding de \"perro\" estará cerca del de \"gato\".\n",
        "3.  **Dimensionalidad**: La cantidad de números en el vector se llama la \"dimensionalidad\" del embedding.\n",
        "    Modelos más grandes y complejos suelen producir embeddings de mayor dimensionalidad.\n",
        "\n",
        "¿Por qué son útiles?\n",
        "Los embeddings permiten a los algoritmos de machine learning trabajar con texto de una manera\n",
        "mucho más sofisticada:\n",
        "-   **Búsqueda Semántica**: Encontrar documentos o frases que significan algo similar, no solo\n",
        "    que comparten las mismas palabras clave.\n",
        "-   **Clasificación de Texto**: Determinar el tema de un texto, analizar sentimientos (positivo/negativo).\n",
        "-   **Clustering de Textos**: Agrupar automáticamente textos similares.\n",
        "-   **Sistemas de Recomendación**: Recomendar artículos o productos basados en descripciones textuales.\n",
        "-   **Traducción Automática**: Ayudan a mapear significados entre diferentes idiomas.\n",
        "\n",
        "En este cuaderno, usaremos un modelo moderno llamado BGE-M3 para generar estos embeddings.\n"
      ],
      "metadata": {
        "id": "KlctkvHfbvOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELDA 2: SECCIÓN 2 - INSTALACIÓN DE BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "# `transformers`: Biblioteca de Hugging Face para modelos de NLP.\n",
        "# `FlagEmbedding`: Para modelos de embedding específicos como BGE.\n",
        "# `seaborn`: Para gráficos estéticos.\n",
        "# `sentence-transformers`: Para una amplia gama de modelos de embedding de frases.\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"CELDA 2: Instalando bibliotecas...\")\n",
        "!pip install -q transformers\n",
        "!pip install -q FlagEmbedding\n",
        "!pip install -q seaborn\n",
        "!pip install -q sentence-transformers\n",
        "print(\"Bibliotecas base instaladas.\")\n"
      ],
      "metadata": {
        "id": "M1282YUpb5bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELDA 3: SECCIÓN 2 - IMPORTACIÓN DE BIBLIOTECAS\n",
        "# ==============================================================================\n",
        "print(\"\\nCELDA 3: Importando bibliotecas...\")\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Modelos de embedding\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"Bibliotecas importadas exitosamente.\")"
      ],
      "metadata": {
        "id": "nm9UZ45QdJAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELDA 4: SECCIÓN 3 - CARGA DE MODELOS DE EMBEDDINGS\n",
        "# ==============================================================================\n",
        "# Se cargarán dos modelos multilingües para comparación.\n",
        "# Esto puede tardar unos minutos la primera vez debido a la descarga de los modelos.\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nCELDA 4: Cargando modelos de embeddings...\")\n",
        "\n",
        "# --- Modelo 1: BAAI/bge-m3 (FlagEmbedding) ---\n",
        "print(\"Cargando el modelo BGE-M3 ('BAAI/bge-m3')...\")\n",
        "try:\n",
        "    bge_model_instance = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False) # `use_fp16=True` si tienes GPU compatible\n",
        "    print(\"Modelo BGE-M3 cargado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar BGE-M3: {e}. Asegúrate de tener conexión a internet.\")\n",
        "    bge_model_instance = None\n",
        "\n",
        "# --- Modelo 2: paraphrase-multilingual-MiniLM-L12-v2 (Sentence-Transformers) ---\n",
        "st_model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "print(f\"\\nCargando el modelo Sentence-Transformer: '{st_model_name}'...\")\n",
        "try:\n",
        "    st_model_instance = SentenceTransformer(st_model_name)\n",
        "    print(f\"Modelo '{st_model_name}' cargado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar '{st_model_name}': {e}. Asegúrate de tener conexión a internet.\")\n",
        "    st_model_instance = None\n",
        "\n",
        "print(\"\\nCarga de modelos completada (o intentada).\")\n"
      ],
      "metadata": {
        "id": "eka9Xih5eEAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELDA 5: SECCIÓN 4 - DEFINICIÓN DE FUNCIONES PARA GENERAR EMBEDDINGS\n",
        "# ==============================================================================\n",
        "print(\"\\nCELDA 5: Definiendo funciones para generar embeddings...\")\n",
        "\n",
        "def generar_embedding_bge(texto, modelo_bge_local):\n",
        "    \"\"\"Genera embedding usando BGE-M3.\"\"\"\n",
        "    if modelo_bge_local is None: return None\n",
        "    try:\n",
        "        return modelo_bge_local.encode([texto])['dense_vecs'][0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error BGE en '{texto[:20]}...': {e}\")\n",
        "        return None\n",
        "\n",
        "def generar_embedding_st(texto, modelo_st_local):\n",
        "    \"\"\"Genera embedding usando Sentence-Transformer.\"\"\"\n",
        "    if modelo_st_local is None: return None\n",
        "    try:\n",
        "        return modelo_st_local.encode([texto])[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error ST en '{texto[:20]}...': {e}\")\n",
        "        return None\n",
        "print(\"Funciones de embedding definidas.\")\n"
      ],
      "metadata": {
        "id": "7cyNmiCJeEYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELDA 6: SECCIÓN 4 - TEXTOS DE EJEMPLO\n",
        "# ==============================================================================\n",
        "# Grupos de textos con temas distintos para observar diferencias y similitudes.\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nCELDA 6: Definiendo textos de ejemplo...\")\n",
        "textos_ejemplo = {\n",
        "    \"Deportes\": [\n",
        "        \"El equipo local ganó el campeonato de fútbol.\",\n",
        "        \"La estrella del baloncesto anotó 50 puntos anoche.\",\n",
        "        \"El partido de tenis fue muy emocionante y duró cinco sets.\"\n",
        "    ],\n",
        "    \"Tecnología\": [\n",
        "        \"La inteligencia artificial está transformando muchas industrias.\",\n",
        "        \"El nuevo teléfono inteligente tiene una cámara increíble.\",\n",
        "        \"La computación cuántica promete resolver problemas complejos.\"\n",
        "    ],\n",
        "    \"Naturaleza y Clima\": [\n",
        "        \"El sol brilla intensamente en un cielo despejado.\",\n",
        "        \"Se espera una fuerte tormenta de nieve para mañana.\",\n",
        "        \"Los bosques tropicales albergan una gran biodiversidad.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "lista_todos_textos_global = []\n",
        "lista_categorias_global = []\n",
        "for categoria, textos_cat in textos_ejemplo.items():\n",
        "    for texto in textos_cat:\n",
        "        lista_todos_textos_global.append(texto)\n",
        "        lista_categorias_global.append(categoria)\n",
        "\n",
        "print(f\"Total de textos de ejemplo: {len(lista_todos_textos_global)}\")\n",
        "print(\"Textos de ejemplo definidos.\")\n"
      ],
      "metadata": {
        "id": "35kz834JeOy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELDA 7: SECCIÓN 4 - SELECCIÓN DEL MODELO Y GENERACIÓN DE EMBEDDINGS\n",
        "# ==============================================================================\n",
        "# Cambia el valor de MODELO_A_USAR_GLOBAL para probar diferentes modelos.\n",
        "# Opciones: \"BGE-M3\" o \"SentenceTransformer_MiniLM\"\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nCELDA 7: Seleccionando modelo y generando embeddings...\")\n",
        "\n",
        "MODELO_A_USAR_GLOBAL = \"SentenceTransformer_MiniLM\" #BGE-M3\" # O\n",
        "\n",
        "print(f\"--- Usando el modelo: {MODELO_A_USAR_GLOBAL} ---\")\n",
        "\n",
        "lista_embeddings_generados_global = []\n",
        "if MODELO_A_USAR_GLOBAL == \"BGE-M3\":\n",
        "    if bge_model_instance:\n",
        "        print(\"Generando embeddings con BGE-M3...\")\n",
        "        for texto in lista_todos_textos_global:\n",
        "            lista_embeddings_generados_global.append(generar_embedding_bge(texto, bge_model_instance))\n",
        "    else:\n",
        "        print(\"Modelo BGE-M3 no disponible.\")\n",
        "elif MODELO_A_USAR_GLOBAL == \"SentenceTransformer_MiniLM\":\n",
        "    if st_model_instance:\n",
        "        print(f\"Generando embeddings con SentenceTransformer ({st_model_name})...\")\n",
        "        for texto in lista_todos_textos_global:\n",
        "            lista_embeddings_generados_global.append(generar_embedding_st(texto, st_model_instance))\n",
        "    else:\n",
        "        print(f\"Modelo Sentence-Transformer ({st_model_name}) no disponible.\")\n",
        "else:\n",
        "    print(f\"Modelo '{MODELO_A_USAR_GLOBAL}' no reconocido.\")\n",
        "\n",
        "# Filtrar resultados None (si algún embedding falló) y preparar datos para análisis\n",
        "textos_validos_global = [lista_todos_textos_global[i] for i, emb in enumerate(lista_embeddings_generados_global) if emb is not None]\n",
        "categorias_validas_global = [lista_categorias_global[i] for i, emb in enumerate(lista_embeddings_generados_global) if emb is not None]\n",
        "embeddings_validos_global = [emb for emb in lista_embeddings_generados_global if emb is not None]\n",
        "\n",
        "matriz_embeddings_global = np.array([]) # Inicializar como array vacío\n",
        "\n",
        "if embeddings_validos_global:\n",
        "    matriz_embeddings_global = np.array(embeddings_validos_global)\n",
        "    print(f\"\\nMatriz de embeddings creada. Forma: {matriz_embeddings_global.shape}\")\n",
        "    if matriz_embeddings_global.size > 0 : #Verificar si no esta vacio\n",
        "      print(f\"Dimensionalidad del embedding: {matriz_embeddings_global.shape[1]}\")\n",
        "else:\n",
        "    print(\"No se pudieron generar embeddings válidos con el modelo seleccionado.\")\n",
        "\n",
        "print(\"Generación de embeddings (o intento) completada.\")\n"
      ],
      "metadata": {
        "id": "RzTy7-3xeO96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELDA 8: SECCIÓN 5 - VISUALIZACIÓN DE EMBEDDINGS CON T-SNE\n",
        "# ==============================================================================\n",
        "# t-SNE reduce la dimensionalidad para visualizar los embeddings en 2D.\n",
        "# Observa si textos con temas similares se agrupan.\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nCELDA 8: Visualizando embeddings con t-SNE...\")\n",
        "\n",
        "if matriz_embeddings_global.shape[0] > 1: # Se necesita más de un punto\n",
        "    print(\"Reduciendo dimensionalidad con t-SNE...\")\n",
        "    tsne_reducer = TSNE(\n",
        "        n_components=2,\n",
        "        random_state=42, # Para reproducibilidad\n",
        "        perplexity=min(5, matriz_embeddings_global.shape[0] - 1), # Ajustar perplexity\n",
        "        n_iter=500, # Menos iteraciones para ejemplos rápidos\n",
        "        init='pca',\n",
        "        learning_rate='auto'\n",
        "    )\n",
        "    embeddings_2d_visual = tsne_reducer.fit_transform(matriz_embeddings_global)\n",
        "\n",
        "    df_tsne_visual = pd.DataFrame({\n",
        "        'x': embeddings_2d_visual[:, 0], 'y': embeddings_2d_visual[:, 1],\n",
        "        'texto': textos_validos_global, 'categoria': categorias_validas_global\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(12, 8)) # Tamaño ajustado\n",
        "    sns.scatterplot(data=df_tsne_visual, x='x', y='y', hue='categoria', palette='viridis', s=150, alpha=0.9)\n",
        "    for i in range(df_tsne_visual.shape[0]):\n",
        "        plt.text(df_tsne_visual.loc[i, 'x'] + 0.03, df_tsne_visual.loc[i, 'y'] + 0.03,\n",
        "                 df_tsne_visual.loc[i, 'texto'][:25] + \"...\", # Mostrar menos caracteres\n",
        "                 fontsize=8, alpha=0.75)\n",
        "\n",
        "    plt.title(f'Visualización t-SNE (Modelo: {MODELO_A_USAR_GLOBAL})', fontsize=15)\n",
        "    plt.xlabel('Componente t-SNE 1', fontsize=11)\n",
        "    plt.ylabel('Componente t-SNE 2', fontsize=11)\n",
        "    plt.legend(title='Categoría', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "    plt.grid(True, linestyle=':', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No hay suficientes embeddings para la visualización t-SNE.\")\n",
        "print(\"Visualización t-SNE (o intento) completada.\")\n"
      ],
      "metadata": {
        "id": "auQ73ucFeWiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELDA 9: SECCIÓN 6 - APLICACIÓN: SIMILITUD DE COSENO\n",
        "# ==============================================================================\n",
        "# Calculamos cuán similares son los textos entre sí usando sus embeddings.\n",
        "# La similitud del coseno mide el ángulo entre los vectores de embedding.\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\nCELDA 9: Calculando similitud de coseno...\")\n",
        "\n",
        "if matriz_embeddings_global.shape[0] > 1:\n",
        "    idx_referencia_sim = 0 # Tomamos el primer texto como referencia\n",
        "    if not textos_validos_global: # Chequeo por si la lista está vacía\n",
        "        print(\"No hay textos válidos para calcular similitud.\")\n",
        "    else:\n",
        "        texto_referencia_sim = textos_validos_global[idx_referencia_sim]\n",
        "        embedding_referencia_sim = matriz_embeddings_global[idx_referencia_sim].reshape(1, -1)\n",
        "\n",
        "        print(f\"Texto de referencia para similitud: \\\"{texto_referencia_sim}\\\" (Modelo: {MODELO_A_USAR_GLOBAL})\")\n",
        "\n",
        "        similitudes_cos = cosine_similarity(embedding_referencia_sim, matriz_embeddings_global)\n",
        "        similitudes_array_cos = similitudes_cos.flatten()\n",
        "\n",
        "        df_similitud_cos = pd.DataFrame({\n",
        "            'texto': textos_validos_global,\n",
        "            'similitud_con_referencia': similitudes_array_cos,\n",
        "            'categoria': categorias_validas_global\n",
        "        })\n",
        "        df_similitud_ordenada_cos = df_similitud_cos.sort_values(by='similitud_con_referencia', ascending=False)\n",
        "\n",
        "        print(\"\\nTextos más similares al de referencia (por similitud del coseno):\")\n",
        "        from IPython.display import display\n",
        "        display(df_similitud_ordenada_cos.head(5)) # Mostrar los 5 más similares\n",
        "else:\n",
        "    print(\"No hay suficientes embeddings para calcular similitudes.\")\n",
        "print(\"Cálculo de similitud (o intento) completado.\")\n"
      ],
      "metadata": {
        "id": "r7iG10_uefGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUryy1kJtL_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}