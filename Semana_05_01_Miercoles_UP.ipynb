{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaThs6ytcqa5Ov6hXe0isa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abxda/UP_Python_2025/blob/main/Semana_05_01_Miercoles_UP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYbuvBiDZnYw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1: Instalación de Dependencias\n",
        "# -------------------------------------\n",
        "# Se instalan todas las bibliotecas necesarias para el proyecto.\n",
        "# Es importante reiniciar el entorno de ejecución después de instalar spaCy y su modelo en español\n",
        "# si se ejecuta por primera vez en una sesión de Colab.\n",
        "\n",
        "!pip install -U duckduckgo_search -q\n",
        "!pip install wordcloud matplotlib -q\n",
        "!pip install transformers -q\n",
        "!pip install sentence-transformers -q  # Incluye sentence-transformers\n",
        "!pip install flagembedding -q\n",
        "!pip install newspaper3k -q\n",
        "!pip install lxml_html_clean -q # Módulo para limpieza de HTML, aunque no se usa explícitamente en el código provisto\n",
        "\n",
        "# Instalar spaCy y su modelo en español\n",
        "!pip install -U spacy -q\n",
        "!python -m spacy download es_core_news_sm -q\n",
        "\n",
        "# NLTK (descargas de recursos se harán más adelante cuando se necesiten)\n",
        "\n",
        "# Para RAG y LLMs locales\n",
        "!pip install -q ollama chromadb\n",
        "\n",
        "print(\"--- Instalaciones completadas ---\")\n",
        "print(\"NOTA: Si es la primera vez que instalas 'es_core_news_sm' en esta sesión,\")\n",
        "print(\"      POR FAVOR, REINICIA EL ENTORNO DE EJECUCIÓN (Runtime > Restart runtime)\")\n",
        "print(\"      antes de continuar con las siguientes celdas.\")"
      ],
      "metadata": {
        "id": "qHJUI443Z7lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2: Importación de Bibliotecas\n",
        "# -----------------------------------\n",
        "# Se importan todos los módulos necesarios después de la instalación.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import json\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from duckduckgo_search import DDGS\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.manifold import TSNE\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Para RAG con Ollama y ChromaDB\n",
        "import ollama\n",
        "import chromadb\n",
        "\n",
        "print(\"--- Bibliotecas importadas ---\")"
      ],
      "metadata": {
        "id": "K5YRGgbgZ9qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 3: Descarga de Recursos NLTK y Carga Modelo spaCy\n",
        "# -------------------------------------------------------\n",
        "# Descargamos los recursos necesarios de NLTK (stopwords y tokenizador punkt)\n",
        "# y cargamos el modelo de spaCy en español.\n",
        "\n",
        "import nltk # Asegurarse que nltk está importado en esta celda si se ejecuta independientemente\n",
        "import spacy # Asegurarse que spacy está importado\n",
        "\n",
        "print(\"--- Descargando recursos de NLTK (stopwords y punkt) si es necesario ---\")\n",
        "try:\n",
        "    nltk.download('stopwords', quiet=True) # quiet=True para menos verbosidad si ya está descargado\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"Recursos NLTK 'stopwords' y 'punkt' verificados/descargados.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al descargar recursos de NLTK: {e}\")\n",
        "    print(\"El procesamiento de texto que dependa de estos recursos podría fallar.\")\n",
        "\n",
        "# Cargar el modelo de spaCy en español\n",
        "# Este paso puede tardar un momento si el modelo es grande o se carga por primera vez.\n",
        "nlp_spacy = None # Inicializar a None\n",
        "try:\n",
        "    nlp_spacy = spacy.load(\"es_core_news_sm\")\n",
        "    print(\"--- Modelo spaCy 'es_core_news_sm' cargado correctamente ---\")\n",
        "except OSError:\n",
        "    print(\"Error: El modelo 'es_core_news_sm' no se pudo cargar.\")\n",
        "    print(\"Asegúrate de haberlo descargado (Celda 1) y reiniciado el entorno si era necesario.\")\n",
        "    print(\"Intenta ejecutar: !python -m spacy download es_core_news_sm\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error inesperado al cargar el modelo spaCy: {e}\")\n",
        "\n",
        "\n",
        "# Definir stopwords en español para usar globalmente\n",
        "# Solo se define si NLTK y el recurso stopwords están disponibles\n",
        "stop_words_spanish = set()\n",
        "try:\n",
        "    from nltk.corpus import stopwords\n",
        "    stop_words_spanish = set(stopwords.words('spanish'))\n",
        "    if not stop_words_spanish: # Si por alguna razón devuelve un set vacío pero no hay error\n",
        "        print(\"Advertencia: La lista de stopwords en español está vacía. Verifica la descarga de NLTK.\")\n",
        "    else:\n",
        "        print(\"Stopwords en español cargadas.\")\n",
        "except LookupError:\n",
        "    print(\"Error: Recurso 'stopwords' de NLTK para español no encontrado. La eliminación de stopwords no funcionará.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar stopwords en español: {e}\")"
      ],
      "metadata": {
        "id": "gWoXSJVWa8LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4: Configuración de Búsqueda y Funciones de Obtención de Noticias\n",
        "# ----------------------------------------------------------------------\n",
        "# Define los parámetros para la búsqueda de noticias y la función para realizarla.\n",
        "\n",
        "# Configuraciones para la búsqueda de noticias\n",
        "# Ejemplo: Buscamos noticias sobre \"México aranceles\" en español de México.\n",
        "configuraciones_busqueda = [\n",
        "    {\"idioma\": \"Español\", \"keywords\": \"Mexico Finanzas\", \"region\": \"mx-es\", \"max_results\": 100}\n",
        "]\n",
        "\n",
        "# Diccionario para almacenar los resultados de noticias por configuración (o idioma si es único)\n",
        "resultados_noticias_por_config = {}\n",
        "\n",
        "def buscar_noticias_ddgs(keywords: str, region: str, max_results: int = 25) -> list:\n",
        "    \"\"\"\n",
        "    Busca noticias utilizando la API síncrona de DuckDuckGo Search (DDGS).\n",
        "\n",
        "    Args:\n",
        "        keywords (str): Palabras clave para la búsqueda.\n",
        "        region (str): Código de región (ej. 'mx-es' para México en español).\n",
        "        max_results (int): Número máximo de resultados a obtener.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de diccionarios, donde cada diccionario representa una noticia.\n",
        "              Retorna una lista vacía si ocurre un error o no se encuentran noticias.\n",
        "    \"\"\"\n",
        "    print(f\"Buscando noticias con keywords='{keywords}', region='{region}', max_results={max_results}...\")\n",
        "    try:\n",
        "        with DDGS() as ddgs:\n",
        "            noticias = list(ddgs.news(\n",
        "                keywords=keywords,\n",
        "                region=region,\n",
        "                safesearch='moderate',\n",
        "                max_results=max_results\n",
        "            ))\n",
        "        print(f\"Se encontraron {len(noticias)} noticias.\")\n",
        "        return noticias\n",
        "    except Exception as e:\n",
        "        print(f\"Error al buscar noticias ({keywords}, {region}): {e}\")\n",
        "        return []\n",
        "\n",
        "def procesar_busquedas_noticias(configs: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Itera sobre las configuraciones de búsqueda, obtiene noticias y las consolida en un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        configs (list): Lista de diccionarios de configuración de búsqueda.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame con todas las noticias encontradas, o DataFrame vacío si no hay resultados.\n",
        "    \"\"\"\n",
        "    todas_las_noticias = []\n",
        "    for i, conf in enumerate(configs):\n",
        "        print(f\"\\nProcesando configuración {i+1}/{len(configs)}: {conf['keywords']}\")\n",
        "        noticias = buscar_noticias_ddgs(\n",
        "            keywords=conf['keywords'],\n",
        "            region=conf['region'],\n",
        "            max_results=conf.get('max_results', 25) # .get para default si no está\n",
        "        )\n",
        "        # Añadir información de la configuración a cada noticia (opcional, pero útil)\n",
        "        for noticia in noticias:\n",
        "            noticia['query_keywords'] = conf['keywords']\n",
        "            noticia['query_region'] = conf['region']\n",
        "        todas_las_noticias.extend(noticias)\n",
        "\n",
        "    if not todas_las_noticias:\n",
        "        print(\"No se encontraron noticias para ninguna configuración.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_noticias_raw = pd.DataFrame(todas_las_noticias)\n",
        "    print(f\"\\nTotal de noticias consolidadas: {len(df_noticias_raw)}\")\n",
        "    if not df_noticias_raw.empty:\n",
        "        print(\"Primeras noticias encontradas (títulos):\")\n",
        "        for title in df_noticias_raw['title'].head().tolist():\n",
        "            print(f\"- {title}\")\n",
        "    return df_noticias_raw\n",
        "\n",
        "# --- Ejecución de la búsqueda de noticias ---\n",
        "df_noticias = procesar_busquedas_noticias(configuraciones_busqueda)\n",
        "\n",
        "# Mostrar un resumen del DataFrame si no está vacío\n",
        "if not df_noticias.empty:\n",
        "    print(\"\\n--- Resumen del DataFrame de Noticias (df_noticias) ---\")\n",
        "    df_noticias.info()\n",
        "    display(df_noticias.head())\n",
        "    print(f\"Dimensiones del DataFrame: {df_noticias.shape}\")\n",
        "else:\n",
        "    print(\"\\n--- No se generó el DataFrame de noticias ya que no se encontraron resultados. ---\")"
      ],
      "metadata": {
        "id": "guPPt0ohbAFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4.1 (Corregida): Enriquecimiento de Noticias con Newspaper3k\n",
        "# -----------------------------------------------------------------\n",
        "# Esta celda toma el DataFrame 'df_noticias' (creado en Celda 4)\n",
        "# e intenta descargar el contenido completo de cada noticia usando su URL\n",
        "# con la biblioteca newspaper3k. El contenido se usará para reemplazar\n",
        "# o complementar la columna 'body'.\n",
        "\n",
        "from newspaper import Article, Config # Asegurarse de la importación\n",
        "import pandas as pd # Para trabajar con el DataFrame\n",
        "import time # Para añadir pequeños delays y ser cortés con los servidores\n",
        "\n",
        "print(\"\\n--- Iniciando Enriquecimiento de Noticias con Newspaper3k ---\")\n",
        "\n",
        "# Verificar si df_noticias existe y tiene la columna 'url'\n",
        "if 'df_noticias' in globals() and not globals()['df_noticias'].empty and 'url' in globals()['df_noticias'].columns:\n",
        "    df_to_enrich = globals()['df_noticias']\n",
        "\n",
        "    df_to_enrich['full_text_newspaper'] = None\n",
        "    df_to_enrich['newspaper_download_error'] = None\n",
        "\n",
        "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'\n",
        "    config = Config()\n",
        "    config.browser_user_agent = user_agent\n",
        "    config.request_timeout = 15\n",
        "    config.fetch_images = False\n",
        "    config.memoize_articles = True\n",
        "    config.verbose = False\n",
        "\n",
        "    print(f\"Procesando {len(df_to_enrich)} URLs para extraer texto completo...\")\n",
        "\n",
        "    for index, row in df_to_enrich.iterrows():\n",
        "        url = row['url']\n",
        "        print(f\"\\nProcesando URL ({index + 1}/{len(df_to_enrich)}): {url}\")\n",
        "\n",
        "        if not url or not isinstance(url, str) or not url.startswith(('http://', 'https://')):\n",
        "            print(f\"  URL inválida o vacía: '{url}'. Omitiendo.\")\n",
        "            df_to_enrich.loc[index, 'newspaper_download_error'] = \"URL inválida o vacía\"\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            article = Article(url, config=config)\n",
        "\n",
        "            print(\"  Descargando HTML...\")\n",
        "            article.download()\n",
        "\n",
        "            print(\"  Parseando artículo...\")\n",
        "            article.parse()\n",
        "\n",
        "            extracted_text = article.text\n",
        "\n",
        "            if extracted_text and len(extracted_text) > 50:\n",
        "                preview_text = extracted_text[:100].replace('\\n', ' ')\n",
        "                print(f\"  Texto extraído (primeros 100 chars): {preview_text}...\")\n",
        "                print(f\"  Longitud del texto extraído: {len(extracted_text)}\")\n",
        "                df_to_enrich.loc[index, 'full_text_newspaper'] = extracted_text\n",
        "            else:\n",
        "                print(\"  No se pudo extraer texto significativo o el texto es muy corto.\")\n",
        "                df_to_enrich.loc[index, 'newspaper_download_error'] = \"Texto extraído vacío o corto\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error al procesar URL '{url}': {e}\")\n",
        "            df_to_enrich.loc[index, 'newspaper_download_error'] = str(e)\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    print(\"\\n--- Enriquecimiento completado. Verificando resultados... ---\")\n",
        "\n",
        "    df_to_enrich['body_enriquecido'] = df_to_enrich['body']\n",
        "\n",
        "    for index, row in df_to_enrich.iterrows():\n",
        "        texto_newspaper = row['full_text_newspaper']\n",
        "        texto_body_original = row['body']\n",
        "\n",
        "        if pd.notna(texto_newspaper) and len(texto_newspaper) > (len(str(texto_body_original)) if pd.notna(texto_body_original) else 0):\n",
        "            df_to_enrich.loc[index, 'body_enriquecido'] = texto_newspaper\n",
        "\n",
        "    actualizados = (df_to_enrich['body_enriquecido'] != df_to_enrich['body']).sum()\n",
        "    print(f\"\\nSe actualizaron/mejoraron los cuerpos de {actualizados} noticias (de un total de {len(df_to_enrich)}).\")\n",
        "\n",
        "    print(\"\\nEjemplo de 'body', 'full_text_newspaper' y 'body_enriquecido':\")\n",
        "    display(df_to_enrich[['url', 'title', 'body', 'full_text_newspaper', 'body_enriquecido', 'newspaper_download_error']].head(10))\n",
        "\n",
        "    globals()['df_noticias'] = df_to_enrich\n",
        "    print(\"\\nDataFrame 'df_noticias' actualizado con contenido enriquecido.\")\n",
        "\n",
        "else:\n",
        "    if 'df_noticias' not in globals() or globals()['df_noticias'].empty:\n",
        "        print(\"El DataFrame 'df_noticias' no existe o está vacío. No se puede enriquecer.\")\n",
        "    elif 'url' not in globals()['df_noticias'].columns:\n",
        "        print(\"La columna 'url' no existe en 'df_noticias'. No se puede enriquecer.\")\n",
        "\n",
        "print(\"\\n--- Fin del Proceso de Enriquecimiento de Noticias ---\")"
      ],
      "metadata": {
        "id": "gqE5H1L2Aoia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 5: Función para Generar Nube de Palabras y Visualización Inicial\n",
        "# ---------------------------------------------------------------------\n",
        "# Función reutilizable para generar nubes de palabras y una visualización\n",
        "# inicial si hay noticias.\n",
        "\n",
        "def generar_nube_de_palabras_wc(texto_completo: str, titulo_grafico: str = \"Nube de Palabras\"):\n",
        "    \"\"\"\n",
        "    Genera y muestra una nube de palabras a partir de un texto.\n",
        "\n",
        "    Args:\n",
        "        texto_completo (str): El string concatenado de todos los textos a visualizar.\n",
        "        titulo_grafico (str): Título para el gráfico de la nube de palabras.\n",
        "    \"\"\"\n",
        "    if not texto_completo.strip():\n",
        "        print(f\"No hay suficiente texto para generar la nube de palabras: '{titulo_grafico}'.\")\n",
        "        return\n",
        "\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_completo)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(titulo_grafico)\n",
        "    plt.show()\n",
        "\n",
        "# Generar nube de palabras del texto original de las noticias (columna 'body')\n",
        "if not df_noticias.empty and 'body_enriquecido' in df_noticias.columns:\n",
        "    # Asegurarse de que los cuerpos de las noticias sean strings y manejar NaNs\n",
        "    textos_originales = df_noticias['body_enriquecido'].fillna('').astype(str)\n",
        "    news_text_original = ' '.join(textos_originales)\n",
        "    generar_nube_de_palabras_wc(news_text_original, \"Nube de Palabras (Texto Original 'body')\")\n",
        "else:\n",
        "    print(\"No se puede generar la nube de palabras inicial: DataFrame vacío o sin columna 'body'.\")"
      ],
      "metadata": {
        "id": "P7eybpKmboFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 6: Limpieza de Texto y Lemmatización\n",
        "# ------------------------------------------\n",
        "# Funciones para limpiar texto (minúsculas, puntuación, stopwords) y lematizar.\n",
        "# Se aplican estas transformaciones al DataFrame.\n",
        "\n",
        "def limpiar_texto(texto: str) -> str:\n",
        "    \"\"\"\n",
        "    Limpia un texto: convierte a minúsculas, elimina puntuación y stopwords.\n",
        "\n",
        "    Args:\n",
        "        texto (str): Texto a limpiar.\n",
        "\n",
        "    Returns:\n",
        "        str: Texto limpio.\n",
        "    \"\"\"\n",
        "    if not isinstance(texto, str): # Manejar posibles NaNs o no-strings\n",
        "        return \"\"\n",
        "    texto_lower = texto.lower()\n",
        "    texto_sin_puntuacion = texto_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "    palabras = texto_sin_puntuacion.split()\n",
        "    palabras_filtradas = [palabra for palabra in palabras if palabra not in stop_words_spanish]\n",
        "    return ' '.join(palabras_filtradas)\n",
        "\n",
        "def lematizar_texto(texto: str) -> str:\n",
        "    \"\"\"\n",
        "    Lematiza un texto utilizando spaCy.\n",
        "\n",
        "    Args:\n",
        "        texto (str): Texto a lematizar (generalmente ya limpio de stopwords y puntuación).\n",
        "\n",
        "    Returns:\n",
        "        str: Texto lematizado.\n",
        "    \"\"\"\n",
        "    if not isinstance(texto, str) or not nlp_spacy: # Manejar no-strings o si spaCy no cargó\n",
        "        return \"\"\n",
        "    doc = nlp_spacy(texto)\n",
        "    return ' '.join([token.lemma_ for token in doc if token.lemma_.strip()]) # Evitar lemas vacíos\n",
        "\n",
        "if not df_noticias.empty and 'body' in df_noticias.columns:\n",
        "    print(\"\\n--- Limpiando y Lematizando Textos ---\")\n",
        "    # 1. Crear columna 'clean_body'\n",
        "    df_noticias['clean_body'] = df_noticias['body'].fillna('').astype(str).apply(limpiar_texto)\n",
        "    print(\"Columna 'clean_body' creada.\")\n",
        "\n",
        "    # 2. Crear columna 'lemmatized_body' a partir de 'clean_body'\n",
        "    df_noticias['lemmatized_body'] = df_noticias['clean_body'].apply(lematizar_texto)\n",
        "    print(\"Columna 'lemmatized_body' creada.\")\n",
        "\n",
        "    display(df_noticias[['title', 'body', 'clean_body', 'lemmatized_body']].head())\n",
        "\n",
        "    # Generar nube de palabras con texto limpio\n",
        "    textos_limpios_concat = ' '.join(df_noticias['clean_body'])\n",
        "    generar_nube_de_palabras_wc(textos_limpios_concat, \"Nube de Palabras (Texto Limpio 'clean_body')\")\n",
        "\n",
        "    # Generar nube de palabras con texto lematizado\n",
        "    textos_lematizados_concat = ' '.join(df_noticias['lemmatized_body'])\n",
        "    generar_nube_de_palabras_wc(textos_lematizados_concat, \"Nube de Palabras (Texto Lematizado 'lemmatized_body')\")\n",
        "else:\n",
        "    print(\"No se puede procesar texto: DataFrame vacío o sin columna 'body'.\")"
      ],
      "metadata": {
        "id": "l2xlW6HzbzU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 7: Análisis de Fuentes de Noticias\n",
        "# ----------------------------------------\n",
        "# Contar y graficar la cantidad de noticias por fuente.\n",
        "\n",
        "if not df_noticias.empty and 'source' in df_noticias.columns:\n",
        "    print(\"\\n--- Análisis de Fuentes de Noticias ---\")\n",
        "    conteo_fuentes = df_noticias['source'].value_counts()\n",
        "\n",
        "    plt.figure(figsize=(12, max(8, len(conteo_fuentes) * 0.5))) # Ajustar altura dinámicamente\n",
        "    conteo_fuentes.plot(kind='barh', color='skyblue') # 'barh' para mejor lectura de muchas fuentes\n",
        "    plt.title('Cantidad de Noticias por Fuente')\n",
        "    plt.xlabel('Cantidad de Noticias')\n",
        "    plt.ylabel('Fuente')\n",
        "    plt.gca().invert_yaxis() # La fuente más común arriba\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nConteo de noticias por fuente:\")\n",
        "    print(conteo_fuentes)\n",
        "else:\n",
        "    print(\"No se puede analizar fuentes: DataFrame vacío o sin columna 'source'.\")"
      ],
      "metadata": {
        "id": "NUovhUVeb4Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 8: Funciones para Clustering y Visualización de Clusters\n",
        "# -------------------------------------------------------------\n",
        "# Define funciones para preprocesar texto específicamente para algunos modelos (si es necesario),\n",
        "# y una función robusta para visualizar los clusters con nubes de palabras y títulos.\n",
        "\n",
        "def preprocess_text_for_tfidf_like_models(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocesa texto para modelos como TF-IDF: lematiza y elimina stopwords/puntuación.\n",
        "    Es similar a la combinación de limpiar_texto y lematizar_texto pero en un solo paso.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not nlp_spacy:\n",
        "        return \"\"\n",
        "    doc = nlp_spacy(text.lower()) # Asegurar minúsculas antes de procesar con spaCy\n",
        "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.lemma_.strip()])\n",
        "\n",
        "def generar_nube_palabras_para_cluster(textos_cluster: list, ax):\n",
        "    \"\"\"\n",
        "    Genera una nube de palabras para un conjunto de textos y la dibuja en un subplot (ax).\n",
        "    \"\"\"\n",
        "    if not textos_cluster:\n",
        "        ax.text(0.5, 0.5, \"No hay texto para esta nube\", ha='center', va='center')\n",
        "        ax.axis('off')\n",
        "        return\n",
        "\n",
        "    texto_completo = ' '.join(textos_cluster)\n",
        "    if not texto_completo.strip():\n",
        "        ax.text(0.5, 0.5, \"Texto vacío para esta nube\", ha='center', va='center')\n",
        "        ax.axis('off')\n",
        "        return\n",
        "\n",
        "    wordcloud = WordCloud(width=400, height=200, background_color='white', max_words=50).generate(texto_completo)\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis('off')\n",
        "\n",
        "def visualizar_clusters_detalle(df_original: pd.DataFrame, clusters_labels: np.ndarray, texto_col_for_wc: str, titulo_general: str):\n",
        "    \"\"\"\n",
        "    Visualiza clusters mostrando una nube de palabras y los títulos de las noticias para cada cluster.\n",
        "\n",
        "    Args:\n",
        "        df_original (pd.DataFrame): DataFrame original que contiene 'title', 'source', y la columna de texto para la WC.\n",
        "        clusters_labels (np.ndarray): Array con las etiquetas de cluster para cada noticia.\n",
        "        texto_col_for_wc (str): Nombre de la columna en df_original a usar para generar las nubes de palabras (ej: 'lemmatized_body').\n",
        "        titulo_general (str): Título general para la visualización.\n",
        "    \"\"\"\n",
        "    df_copy = df_original.copy()\n",
        "    df_copy['cluster'] = clusters_labels\n",
        "\n",
        "    # Contar noticias por cluster, excluyendo el cluster -1 (ruido en DBSCAN)\n",
        "    cluster_counts = df_copy[df_copy['cluster'] != -1]['cluster'].value_counts()\n",
        "    num_clusters_validos = len(cluster_counts)\n",
        "\n",
        "    if num_clusters_validos == 0:\n",
        "        print(f\"{titulo_general}: No se formaron clusters válidos (todos podrían ser ruido -1).\")\n",
        "        print(f\"Distribución de clusters (incluye ruido): \\n{pd.Series(clusters_labels).value_counts()}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- {titulo_general} ---\")\n",
        "    print(f\"Distribución de clusters: \\n{pd.Series(clusters_labels).value_counts().sort_index()}\")\n",
        "\n",
        "    # Crear una figura con subplots: una fila por cluster, dos columnas (WC, Títulos)\n",
        "    # Ajustar el tamaño de la figura dinámicamente\n",
        "    fig_height = max(10, num_clusters_validos * 4) # 4 pulgadas de alto por cluster\n",
        "    fig, axs = plt.subplots(num_clusters_validos, 2, figsize=(20, fig_height), squeeze=False) # squeeze=False para asegurar axs 2D\n",
        "\n",
        "    valid_cluster_indices = cluster_counts.index.sort_values()\n",
        "\n",
        "    for i, cluster_id in enumerate(valid_cluster_indices):\n",
        "        cluster_data = df_copy[df_copy['cluster'] == cluster_id]\n",
        "        textos_cluster = cluster_data[texto_col_for_wc].tolist()\n",
        "\n",
        "        # Limitar el número de títulos a mostrar para no saturar\n",
        "        titulos_cluster = [f\"{idx}. {row['title']} ({row['source']})\" for idx, (_, row) in enumerate(cluster_data[['title', 'source']].head(10).iterrows(), 1)]\n",
        "\n",
        "        # Nube de palabras en la primera columna del subplot\n",
        "        generar_nube_palabras_para_cluster(textos_cluster, axs[i, 0])\n",
        "        axs[i, 0].set_title(f'Cluster {cluster_id} - Nube ({len(textos_cluster)} noticias)')\n",
        "\n",
        "        # Títulos en la segunda columna del subplot\n",
        "        if titulos_cluster:\n",
        "            axs[i, 1].text(0.01, 0.99, \"\\n\".join(titulos_cluster), verticalalignment='top', horizontalalignment='left', fontsize=9, family='monospace', wrap=True)\n",
        "        else:\n",
        "            axs[i, 1].text(0.5, 0.5, \"Sin títulos para mostrar.\", ha='center', va='center')\n",
        "        axs[i, 1].axis('off')\n",
        "        axs[i, 1].set_title(f'Cluster {cluster_id} - Títulos y Fuentes (primeros 10)')\n",
        "\n",
        "    plt.suptitle(titulo_general, fontsize=16, y=1.01) # y>1 para que no solape con subplots\n",
        "    plt.tight_layout(pad=2.0)\n",
        "    plt.show()\n",
        "\n",
        "# Preprocesar texto para TF-IDF si el DataFrame existe y tiene 'body'\n",
        "if not df_noticias.empty and 'body_enriquecido' in df_noticias.columns:\n",
        "    df_noticias['processed_for_tfidf'] = df_noticias['body_enriquecido'].fillna('').astype(str).apply(preprocess_text_for_tfidf_like_models)\n",
        "    print(\"Columna 'processed_for_tfidf' creada para TF-IDF.\")\n",
        "else:\n",
        "    print(\"No se puede crear 'processed_for_tfidf': DataFrame vacío o sin columna 'body_enriquecido'.\")"
      ],
      "metadata": {
        "id": "fx1SSQO3b-Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 9: Clustering con TF-IDF\n",
        "# -------------------------------\n",
        "# Vectorización con TF-IDF (usando n-gramas de caracteres) y aplicación de DBSCAN y KMeans.\n",
        "\n",
        "if not df_noticias.empty and 'lemmatized_body' in df_noticias.columns and not df_noticias['lemmatized_body'].str.strip().eq('').all():\n",
        "    print(\"\\n--- Clustering con TF-IDF (sobre 'lemmatized_body') ---\")\n",
        "\n",
        "    # Usaremos 'lemmatized_body' que ya está bastante procesado.\n",
        "    # Para TF-IDF con n-gramas de caracteres, el texto original o mínimamente procesado puede ser mejor.\n",
        "    # Aquí se usa 'lemmatized_body' como en el original, pero considera 'clean_body' o 'body' si los resultados no son buenos.\n",
        "    textos_para_tfidf = df_noticias['lemmatized_body'].tolist()\n",
        "\n",
        "    if not any(textos_para_tfidf): # Chequea si todos los textos están vacíos\n",
        "        print(\"Los textos para TF-IDF están vacíos. Saltando clustering TF-IDF.\")\n",
        "    else:\n",
        "        # Vectorización con TF-IDF (n-gramas de caracteres)\n",
        "        vectorizer_tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), min_df=2) # min_df=2 para ignorar términos muy raros\n",
        "        X_tfidf_char = vectorizer_tfidf_char.fit_transform(textos_para_tfidf)\n",
        "        X_tfidf_char_normalized = normalize(X_tfidf_char)\n",
        "        print(f\"Dimensiones de la matriz TF-IDF (char n-grams): {X_tfidf_char_normalized.shape}\")\n",
        "\n",
        "        if X_tfidf_char_normalized.shape[0] > 0 and X_tfidf_char_normalized.shape[1] > 0: # Asegurarse que la matriz no está vacía\n",
        "            # DBSCAN con TF-IDF\n",
        "            # Ajustar 'eps' y 'min_samples' según la densidad de tus datos\n",
        "            dbscan_tfidf = DBSCAN(eps=0.8, min_samples=2, metric='cosine')\n",
        "            clusters_dbscan_tfidf = dbscan_tfidf.fit_predict(X_tfidf_char_normalized)\n",
        "            visualizar_clusters_detalle(df_noticias, clusters_dbscan_tfidf, 'lemmatized_body', \"Clusters TF-IDF + DBSCAN (char n-grams)\")\n",
        "\n",
        "            # KMeans con TF-IDF\n",
        "            # Definir el número de clusters (k). Podría estimarse con Elbow method, Silhouette score, etc.\n",
        "            num_k_clusters_tfidf = min(8, X_tfidf_char_normalized.shape[0]) # No más clusters que muestras\n",
        "            if num_k_clusters_tfidf > 1:\n",
        "                kmeans_tfidf = KMeans(n_clusters=num_k_clusters_tfidf, random_state=42, n_init='auto')\n",
        "                clusters_kmeans_tfidf = kmeans_tfidf.fit_predict(X_tfidf_char_normalized)\n",
        "                visualizar_clusters_detalle(df_noticias, clusters_kmeans_tfidf, 'lemmatized_body', f\"Clusters TF-IDF + KMeans (k={num_k_clusters_tfidf}, char n-grams)\")\n",
        "            else:\n",
        "                print(\"No hay suficientes muestras para KMeans con TF-IDF (se necesitan al menos 2).\")\n",
        "        else:\n",
        "            print(\"La matriz TF-IDF está vacía o tiene dimensiones cero. Saltando clustering TF-IDF.\")\n",
        "else:\n",
        "    print(\"No se puede realizar clustering TF-IDF: DataFrame vacío, sin 'lemmatized_body' o columna vacía.\")"
      ],
      "metadata": {
        "id": "9143s1jub-US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 10: Preparación de Embeddings BGE-M3\n",
        "# -----------------------------------------\n",
        "# Carga del modelo BGE-M3 y generación de embeddings para los textos limpios.\n",
        "\n",
        "# Cargar el modelo BGE-M3\n",
        "try:\n",
        "    model_bge_m3 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True) # use_fp16 para acelerar si GPU disponible\n",
        "    print(\"--- Modelo BGE-M3 ('BAAI/bge-m3') cargado ---\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo BGE-M3: {e}\")\n",
        "    model_bge_m3 = None\n",
        "\n",
        "def generar_embeddings_bge_m3(textos: list, model) -> np.ndarray:\n",
        "    \"\"\"Genera embeddings BGE-M3 para una lista de textos.\"\"\"\n",
        "    if not model or not textos:\n",
        "        return np.array([])\n",
        "    print(f\"Generando embeddings BGE-M3 para {len(textos)} textos...\")\n",
        "    # BGE-M3 no requiere prefijos como \"passage:\" o \"query:\" para la codificación directa\n",
        "    embeddings_output = model.encode(textos, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "    return embeddings_output['dense_vecs']\n",
        "\n",
        "\n",
        "embeddings_bge_m3_list = []\n",
        "if model_bge_m3 and not df_noticias.empty and 'clean_body' in df_noticias.columns and not df_noticias['clean_body'].str.strip().eq('').all():\n",
        "    textos_para_bge_m3 = df_noticias['clean_body'].tolist()\n",
        "    if not any(textos_para_bge_m3):\n",
        "        print(\"Los textos para BGE-M3 están vacíos. Saltando generación de embeddings BGE-M3.\")\n",
        "    else:\n",
        "        embeddings_bge_m3_list = generar_embeddings_bge_m3(textos_para_bge_m3, model_bge_m3)\n",
        "        if embeddings_bge_m3_list.size > 0:\n",
        "            X_bge_m3_normalized = normalize(embeddings_bge_m3_list, norm='l2', axis=1) # Normalizar es buena práctica para distancias coseno\n",
        "            print(f\"Embeddings BGE-M3 generados y normalizados. Dimensiones: {X_bge_m3_normalized.shape}\")\n",
        "        else:\n",
        "            print(\"No se generaron embeddings BGE-M3.\")\n",
        "            X_bge_m3_normalized = np.array([]) # Asegurar que es un array vacío\n",
        "else:\n",
        "    print(\"No se pueden generar embeddings BGE-M3: modelo no cargado, DataFrame vacío, sin 'clean_body' o columna vacía.\")\n",
        "    X_bge_m3_normalized = np.array([])"
      ],
      "metadata": {
        "id": "o6hZVHPkcQUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 11: Clustering con Embeddings BGE-M3\n",
        "# ------------------------------------------\n",
        "# Aplicación de KMeans y DBSCAN a los embeddings BGE-M3.\n",
        "\n",
        "if 'X_bge_m3_normalized' in locals() and X_bge_m3_normalized.size > 0:\n",
        "    print(\"\\n--- Clustering con Embeddings BGE-M3 ---\")\n",
        "\n",
        "    # KMeans con BGE-M3\n",
        "    num_k_clusters_bge = min(8, X_bge_m3_normalized.shape[0]) # No más clusters que muestras\n",
        "    if num_k_clusters_bge > 1:\n",
        "        kmeans_bge = KMeans(n_clusters=num_k_clusters_bge, random_state=42, n_init='auto')\n",
        "        clusters_kmeans_bge = kmeans_bge.fit_predict(X_bge_m3_normalized)\n",
        "        visualizar_clusters_detalle(df_noticias, clusters_kmeans_bge, 'clean_body', f\"Clusters BGE-M3 + KMeans (k={num_k_clusters_bge})\")\n",
        "    else:\n",
        "        print(\"No hay suficientes muestras para KMeans con BGE-M3 (se necesitan al menos 2).\")\n",
        "\n",
        "    # DBSCAN con BGE-M3\n",
        "    # 'eps' para DBSCAN es sensible. Puede requerir ajuste.\n",
        "    # Para embeddings normalizados, 'cosine' o 'euclidean' pueden funcionar.\n",
        "    # Si se usa 'euclidean' con L2-normalized vectors, la distancia euclidiana está relacionada con la similaridad coseno.\n",
        "    # d_euc(A,B)^2 = 2 - 2 * cos_sim(A,B)\n",
        "    # Un 'eps' típico para embeddings normalizados con 'euclidean' podría estar entre 0.5 y 1.2\n",
        "    dbscan_bge = DBSCAN(eps=0.85, min_samples=2, metric='euclidean') # Usar 'cosine' y ajustar eps si es preferible.\n",
        "    clusters_dbscan_bge = dbscan_bge.fit_predict(X_bge_m3_normalized)\n",
        "    visualizar_clusters_detalle(df_noticias, clusters_dbscan_bge, 'clean_body', \"Clusters BGE-M3 + DBSCAN\")\n",
        "else:\n",
        "    print(\"No se pueden realizar clustering con BGE-M3: embeddings no generados.\")"
      ],
      "metadata": {
        "id": "M-Bfq6nKca5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 12: Visualización t-SNE de Embeddings BGE-M3\n",
        "# -------------------------------------------------\n",
        "# Reducción de dimensionalidad con t-SNE y visualización de los embeddings BGE-M3.\n",
        "\n",
        "if 'X_bge_m3_normalized' in locals() and X_bge_m3_normalized.size > 0 and 'title' in df_noticias.columns:\n",
        "    print(\"\\n--- Visualización t-SNE de Embeddings BGE-M3 ---\")\n",
        "    num_samples_bge = X_bge_m3_normalized.shape[0]\n",
        "    perplexity_bge = min(30, max(1, num_samples_bge - 1)) # Ajustar perplexity si hay pocos datos\n",
        "\n",
        "    if num_samples_bge > 1 : # t-SNE necesita más de 1 muestra\n",
        "        tsne_bge = TSNE(n_components=2, random_state=42, perplexity=perplexity_bge, n_iter=1000, metric='cosine')\n",
        "        X_bge_2d = tsne_bge.fit_transform(X_bge_m3_normalized)\n",
        "\n",
        "        df_tsne_bge = pd.DataFrame(X_bge_2d, columns=['x', 'y'])\n",
        "        # Asegurarse que el índice coincida si df_noticias fue filtrado o tiene saltos\n",
        "        df_tsne_bge.index = df_noticias.index[:len(df_tsne_bge)] # Alinear con las primeras N filas de df_noticias\n",
        "        df_tsne_bge['titulo'] = df_noticias['title']\n",
        "\n",
        "        plt.figure(figsize=(18, 12))\n",
        "        sns.scatterplot(data=df_tsne_bge, x='x', y='y', hue='titulo', palette='viridis', legend=False, alpha=0.7)\n",
        "        for i in df_tsne_bge.index:\n",
        "            plt.text(df_tsne_bge.loc[i, 'x'], df_tsne_bge.loc[i, 'y'], df_tsne_bge.loc[i, 'titulo'],\n",
        "                     fontsize=8, color='black', alpha=0.7,\n",
        "                     bbox=dict(facecolor='white', alpha=0.3, edgecolor='none', pad=0.1))\n",
        "        plt.title('t-SNE de Embeddings de Noticias (BGE-M3)')\n",
        "        plt.xlabel('Componente t-SNE 1')\n",
        "        plt.ylabel('Componente t-SNE 2')\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No hay suficientes muestras para la visualización t-SNE de BGE-M3 (se necesita >1).\")\n",
        "else:\n",
        "    print(\"No se puede realizar t-SNE de BGE-M3: embeddings no disponibles o DataFrame sin 'title'.\")"
      ],
      "metadata": {
        "id": "YvzpJjrPcemM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 13 (Modificada): Carga del Modelo de Embedding Sentence-Transformer\n",
        "# -----------------------------------------------------------------------\n",
        "# Carga del modelo 'paraphrase-multilingual-MiniLM-L12-v2' para RAG.\n",
        "\n",
        "from sentence_transformers import SentenceTransformer # Asegurar importación\n",
        "import numpy as np # Para manipulación de arrays\n",
        "from sklearn.preprocessing import normalize # Para normalizar embeddings\n",
        "\n",
        "print(\"\\n--- Cargando Modelo de Embedding para RAG ---\")\n",
        "\n",
        "# Nombre del modelo Sentence-Transformer a utilizar\n",
        "embedding_model_name_for_rag = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "active_st_model_instance = None # Variable para el modelo cargado\n",
        "X_st_embeddings_normalized = np.array([]) # Para los embeddings de documentos\n",
        "\n",
        "print(f\"Intentando cargar el modelo Sentence-Transformer: '{embedding_model_name_for_rag}'...\")\n",
        "try:\n",
        "    active_st_model_instance = SentenceTransformer(embedding_model_name_for_rag)\n",
        "    print(f\"Modelo '{embedding_model_name_for_rag}' cargado exitosamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar '{embedding_model_name_for_rag}': {e}.\")\n",
        "    print(\"Asegúrate de tener conexión a internet si el modelo no está cacheado.\")\n",
        "    active_st_model_instance = None\n",
        "\n",
        "# Si el modelo se cargó y tenemos datos, generamos los embeddings para los documentos\n",
        "if active_st_model_instance and 'df_noticias' in globals() and not globals()['df_noticias'].empty and \\\n",
        "   'clean_body' in globals()['df_noticias'].columns and \\\n",
        "   not globals()['df_noticias']['clean_body'].str.strip().eq('').all():\n",
        "\n",
        "    print(f\"\\nGenerando embeddings con '{embedding_model_name_for_rag}' para los documentos...\")\n",
        "    textos_documentos = globals()['df_noticias']['clean_body'].tolist()\n",
        "\n",
        "    # Estos modelos generalmente no requieren prefijos como \"passage:\"\n",
        "    # Pero si se usa un modelo tipo E5, aquí se añadiría el prefijo.\n",
        "    # Para MiniLM, no se necesita.\n",
        "\n",
        "    if not any(textos_documentos): # Chequea si todos los textos están vacíos\n",
        "        print(\"Los textos para generar embeddings están vacíos.\")\n",
        "    else:\n",
        "        try:\n",
        "            # Generar embeddings\n",
        "            raw_embeddings = active_st_model_instance.encode(textos_documentos, show_progress_bar=True)\n",
        "\n",
        "            # Normalizar los embeddings (buena práctica para distancia coseno)\n",
        "            X_st_embeddings_normalized = normalize(raw_embeddings, norm='l2', axis=1)\n",
        "            print(f\"Embeddings generados y normalizados con '{embedding_model_name_for_rag}'. Dimensiones: {X_st_embeddings_normalized.shape}\")\n",
        "        except Exception as e_embed_docs:\n",
        "            print(f\"Error al generar embeddings para documentos con '{embedding_model_name_for_rag}': {e_embed_docs}\")\n",
        "            X_st_embeddings_normalized = np.array([]) # Asegurar que es un array vacío si falla\n",
        "else:\n",
        "    if not active_st_model_instance:\n",
        "        print(f\"El modelo '{embedding_model_name_for_rag}' no se pudo cargar. No se generarán embeddings de documentos.\")\n",
        "    else:\n",
        "        print(\"No se pueden generar embeddings para documentos: DataFrame 'df_noticias' no está listo o 'clean_body' está vacío.\")\n",
        "\n",
        "# Esta celda ahora define 'active_st_model_instance' y 'X_st_embeddings_normalized'\n",
        "# que serán usados por la Celda 17."
      ],
      "metadata": {
        "id": "wRD71GpncraN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 14 (Adaptada): Clustering con Embeddings MiniLM ('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "# -----------------------------------------------------------------------------------------\n",
        "# Aplicación de KMeans y DBSCAN a los embeddings MiniLM generados en Celda 13.\n",
        "\n",
        "# Necesitarás importar KMeans, DBSCAN, y la función visualizar_clusters_detalle si no están ya en el scope.\n",
        "# from sklearn.cluster import KMeans, DBSCAN\n",
        "# from sklearn.preprocessing import normalize # Ya debería estar, pero por si acaso\n",
        "# (asumimos que visualizar_clusters_detalle ya está definida en una celda anterior, como Celda 8)\n",
        "\n",
        "if 'X_st_embeddings_normalized' in globals() and globals()['X_st_embeddings_normalized'].size > 0 and \\\n",
        "   'df_noticias' in globals() and not globals()['df_noticias'].empty:\n",
        "\n",
        "    print(\"\\n--- Clustering con Embeddings MiniLM ('paraphrase-multilingual-MiniLM-L12-v2') ---\")\n",
        "\n",
        "    current_embeddings_for_clustering = globals()['X_st_embeddings_normalized']\n",
        "    df_for_visualization = globals()['df_noticias']\n",
        "\n",
        "    # KMeans con MiniLM\n",
        "    # Ajustar num_k_clusters según el número de muestras y la estructura esperada.\n",
        "    num_k_clusters_minilm = min(8, current_embeddings_for_clustering.shape[0])\n",
        "    if num_k_clusters_minilm > 1:\n",
        "        print(f\"\\nEjecutando KMeans con k={num_k_clusters_minilm}...\")\n",
        "        kmeans_minilm = KMeans(n_clusters=num_k_clusters_minilm, random_state=42, n_init='auto')\n",
        "        clusters_kmeans_minilm = kmeans_minilm.fit_predict(current_embeddings_for_clustering)\n",
        "        visualizar_clusters_detalle(df_for_visualization,\n",
        "                                    clusters_kmeans_minilm,\n",
        "                                    'clean_body', # Columna de texto para nubes de palabras\n",
        "                                    f\"Clusters MiniLM + KMeans (k={num_k_clusters_minilm})\")\n",
        "    else:\n",
        "        print(\"No hay suficientes muestras para KMeans con MiniLM (se necesitan al menos 2).\")\n",
        "\n",
        "    # DBSCAN con MiniLM\n",
        "    # El valor de 'eps' para DBSCAN es muy sensible y depende de la escala/densidad de tus embeddings.\n",
        "    # Para embeddings L2 normalizados, la distancia euclidiana está relacionada con la similaridad coseno.\n",
        "    # eps_euc ~ sqrt(2 - 2*cos_sim_threshold). Si quieres cos_sim > 0.7, eps ~ sqrt(2-1.4) = sqrt(0.6) ~ 0.77\n",
        "    # Necesitarás experimentar con 'eps' y 'min_samples'.\n",
        "    eps_dbscan_minilm = 0.75 # VALOR DE EJEMPLO, AJUSTAR\n",
        "    min_samples_dbscan_minilm = 2\n",
        "    print(f\"\\nEjecutando DBSCAN con eps={eps_dbscan_minilm}, min_samples={min_samples_dbscan_minilm}...\")\n",
        "    dbscan_minilm = DBSCAN(eps=eps_dbscan_minilm, min_samples=min_samples_dbscan_minilm, metric='euclidean') # 'cosine' también es una opción\n",
        "    clusters_dbscan_minilm = dbscan_minilm.fit_predict(current_embeddings_for_clustering)\n",
        "    visualizar_clusters_detalle(df_for_visualization,\n",
        "                                clusters_dbscan_minilm,\n",
        "                                'clean_body',\n",
        "                                f\"Clusters MiniLM + DBSCAN (eps={eps_dbscan_minilm})\")\n",
        "else:\n",
        "    print(\"No se pueden realizar clustering con MiniLM: embeddings no generados o DataFrame no disponible.\")"
      ],
      "metadata": {
        "id": "gwtrHy_0crdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 15 (Adaptada): Visualización t-SNE de Embeddings MiniLM ('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# Reducción de dimensionalidad con t-SNE y visualización de los embeddings MiniLM.\n",
        "\n",
        "# Necesitarás importar TSNE, matplotlib.pyplot, seaborn si no están ya en el scope.\n",
        "# from sklearn.manifold import TSNE\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import pandas as pd\n",
        "\n",
        "if 'X_st_embeddings_normalized' in globals() and globals()['X_st_embeddings_normalized'].size > 0 and \\\n",
        "   'df_noticias' in globals() and 'title' in globals()['df_noticias'].columns:\n",
        "\n",
        "    print(\"\\n--- Visualización t-SNE de Embeddings MiniLM ('paraphrase-multilingual-MiniLM-L12-v2') ---\")\n",
        "\n",
        "    current_embeddings_for_tsne = globals()['X_st_embeddings_normalized']\n",
        "    df_for_labels = globals()['df_noticias']\n",
        "\n",
        "    num_samples_minilm = current_embeddings_for_tsne.shape[0]\n",
        "    # Perplexity debe ser menor que el número de muestras. Comúnmente entre 5 y 50.\n",
        "    perplexity_minilm = min(30, max(1, num_samples_minilm - 1))\n",
        "\n",
        "    if num_samples_minilm > 1: # t-SNE necesita más de 1 muestra\n",
        "        print(f\"Ejecutando t-SNE con perplexity={perplexity_minilm}...\")\n",
        "        tsne_minilm = TSNE(n_components=2,\n",
        "                           random_state=42,\n",
        "                           perplexity=perplexity_minilm,\n",
        "                           n_iter=1000,\n",
        "                           metric='cosine') # 'cosine' es bueno para embeddings de texto normalizados\n",
        "        X_minilm_2d = tsne_minilm.fit_transform(current_embeddings_for_tsne)\n",
        "\n",
        "        df_tsne_minilm = pd.DataFrame(X_minilm_2d, columns=['x', 'y'])\n",
        "        # Asegurar que el índice coincida si df_noticias fue filtrado o tiene saltos\n",
        "        # Tomar solo los títulos correspondientes a los embeddings procesados\n",
        "        df_tsne_minilm.index = df_for_labels.index[:len(df_tsne_minilm)]\n",
        "        df_tsne_minilm['titulo'] = df_for_labels['title']\n",
        "\n",
        "        plt.figure(figsize=(18, 12))\n",
        "        sns.scatterplot(data=df_tsne_minilm, x='x', y='y', hue='titulo', palette='viridis', legend=False, alpha=0.7)\n",
        "\n",
        "        # Ajustar el número de etiquetas para evitar sobrecargar el gráfico si hay muchos puntos\n",
        "        num_labels_to_show = min(len(df_tsne_minilm), 75) # Mostrar hasta 75 etiquetas\n",
        "        indices_to_label = df_tsne_minilm.sample(n=num_labels_to_show, random_state=42).index if len(df_tsne_minilm) > num_labels_to_show else df_tsne_minilm.index\n",
        "\n",
        "        for i in indices_to_label:\n",
        "            if i in df_tsne_minilm.index: # Doble chequeo\n",
        "                plt.text(df_tsne_minilm.loc[i, 'x'], df_tsne_minilm.loc[i, 'y'], df_tsne_minilm.loc[i, 'titulo'],\n",
        "                         fontsize=8, color='black', alpha=0.7,\n",
        "                         bbox=dict(facecolor='white', alpha=0.3, edgecolor='none', pad=0.1))\n",
        "\n",
        "        plt.title(\"t-SNE de Embeddings de Noticias (paraphrase-multilingual-MiniLM-L12-v2)\")\n",
        "        plt.xlabel('Componente t-SNE 1')\n",
        "        plt.ylabel('Componente t-SNE 2')\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No hay suficientes muestras para la visualización t-SNE de MiniLM (se necesita >1).\")\n",
        "else:\n",
        "    print(\"No se puede realizar t-SNE de MiniLM: embeddings no disponibles, DataFrame sin 'title' o vacío.\")"
      ],
      "metadata": {
        "id": "uOizv5HIcrgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "#!sudo apt-get update && sudo apt-get install -y cuda-drivers"
      ],
      "metadata": {
        "id": "3fpVNZF4lokH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})"
      ],
      "metadata": {
        "id": "asJDx-meevuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!nvidia-smi"
      ],
      "metadata": {
        "id": "0gNYFIEulziI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "E9ZNUlM7eBT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SI MARCO ERRORES ES QUE NO ESTAS USANDO UN ENTORNO GPU T4"
      ],
      "metadata": {
        "id": "dyTrFtPonFAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 16 (Ajustada): Inicio de Ollama Server y Descarga de Modelos LLM\n",
        "# -------------------------------------------------------------------------\n",
        "# Asumimos que Ollama ya fue instalado en una celda anterior.\n",
        "# Esta celda se enfoca en iniciar el servidor Ollama y descargar los modelos LLM necesarios.\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import ollama # Asegurarse que ollama está importado\n",
        "\n",
        "print(\"--- Configurando e iniciando Ollama Server (asumiendo Ollama ya instalado) ---\")\n",
        "ollama_process = None # Inicializar la variable\n",
        "\n",
        "# Intentar iniciar el servidor Ollama si no está corriendo\n",
        "try:\n",
        "    # Verificar si ya hay un proceso ollama sirviendo\n",
        "    # Usamos 'pgrep ollama' que es más general que 'pgrep -f ollama serve'\n",
        "    # ya que el proceso principal de Ollama podría no incluir 'serve' en su nombre exacto en todas las plataformas\n",
        "    result = subprocess.run(['pgrep', 'ollama'], capture_output=True, text=True)\n",
        "    if result.stdout.strip():\n",
        "        print(\"Un proceso 'ollama' ya parece estar corriendo. Asumiendo que es el servidor.\")\n",
        "        # Intentar listar para confirmar que es el servidor y está respondiendo\n",
        "        try:\n",
        "            ollama.list()\n",
        "            print(\"Ollama server está respondiendo.\")\n",
        "        except Exception:\n",
        "            print(\"Proceso 'ollama' encontrado, pero no responde como servidor. Intentando iniciar uno nuevo.\")\n",
        "            # Si pgrep encuentra algo pero no es el servidor, podríamos intentar matarlo y reiniciar,\n",
        "            # pero eso es más complejo y riesgoso. Por ahora, intentaremos iniciar uno.\n",
        "            # Si el puerto ya está en uso, Popen fallará o el nuevo no iniciará correctamente.\n",
        "            ollama_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            print(\"Intentando iniciar un nuevo Ollama server en segundo plano...\")\n",
        "            time.sleep(5) # Darle tiempo para que inicie\n",
        "    else:\n",
        "        # Si no hay proceso ollama corriendo, iniciar uno.\n",
        "        ollama_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        print(\"Intentando iniciar Ollama server en segundo plano...\")\n",
        "        time.sleep(5) # Darle tiempo para que inicie\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: El comando 'ollama' no se encontró. Asegúrate que la instalación en la celda anterior fue correcta y está en el PATH.\")\n",
        "    # No continuar si ollama no se encuentra\n",
        "    raise RuntimeError(\"Ollama no encontrado. Verificar instalación.\")\n",
        "except Exception as e:\n",
        "    print(f\"Excepción al verificar/intentar iniciar ollama serve: {e}\")\n",
        "    # Continuar de todas formas para intentar conectar, podría haber sido un problema temporal con pgrep.\n",
        "\n",
        "# Esperar un poco más y verificar conexión con el servidor\n",
        "print(\"Esperando a que el servidor Ollama esté completamente listo (10-15 segundos más)...\")\n",
        "time.sleep(15) # Aumentado el tiempo de espera por si acaso\n",
        "\n",
        "try:\n",
        "    ollama.list() # Verifica la conexión y lista modelos si alguno está descargado\n",
        "    print(\"--- Conexión con Ollama server establecida. ---\")\n",
        "except Exception as e:\n",
        "    print(f\"--- No se pudo conectar a Ollama server: {e} ---\")\n",
        "    print(\"--- Por favor, asegúrate de que 'ollama serve' esté ejecutándose correctamente. ---\")\n",
        "    print(\"--- Puedes intentar ejecutar '!ollama serve &' en una celda separada o verificar los logs de Ollama. ---\")\n",
        "    # Considerar levantar una excepción si la conexión es crucial para los siguientes pasos\n",
        "    # raise RuntimeError(\"Fallo al conectar con Ollama server.\")\n",
        "\n",
        "# Función para descargar modelos de Ollama (reutilizada)\n",
        "def pull_ollama_model(model_name: str):\n",
        "    \"\"\"Descarga un modelo de Ollama si no está ya presente.\"\"\"\n",
        "    print(f\"\\nVerificando/Descargando modelo Ollama: {model_name}...\")\n",
        "    try:\n",
        "        models_info = ollama.list()\n",
        "        if 'models' in models_info:\n",
        "            model_found = any(m['name'].startswith(model_name) for m in models_info['models'])\n",
        "            if model_found:\n",
        "                print(f\"Modelo '{model_name}' ya disponible localmente.\")\n",
        "                return True\n",
        "        else: # Si 'models' no está en la respuesta, algo raro pasa\n",
        "            print(f\"Advertencia: La respuesta de ollama.list() no contiene la clave 'models'. {models_info}\")\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo listar modelos de Ollama (el servidor podría no estar completamente listo o accesible): {e}\")\n",
        "        print(\"Se intentará descargar el modelo de todas formas.\")\n",
        "\n",
        "    print(f\"Descargando modelo '{model_name}'... esto puede tardar.\")\n",
        "    try:\n",
        "        current_digest = \"\"\n",
        "        status_line = \"\"\n",
        "        for progress in ollama.pull(model_name, stream=True):\n",
        "            if 'digest' in progress:\n",
        "                current_digest = progress['digest']\n",
        "\n",
        "            # Construir una línea de estado más informativa\n",
        "            status_parts = []\n",
        "            if progress.get('status'):\n",
        "                status_parts.append(progress.get('status'))\n",
        "            if progress.get('total') and progress.get('completed'):\n",
        "                completed_gb = progress.get('completed', 0) / (1024**3)\n",
        "                total_gb = progress.get('total', 0) / (1024**3)\n",
        "                if total_gb > 0 :\n",
        "                    percentage = (progress.get('completed', 0) / progress.get('total', 0)) * 100\n",
        "                    status_parts.append(f\"{completed_gb:.2f}/{total_gb:.2f} GB ({percentage:.1f}%)\")\n",
        "                else:\n",
        "                    status_parts.append(f\"{completed_gb:.2f} GB descargados\")\n",
        "\n",
        "\n",
        "            new_status_line = \" \".join(status_parts)\n",
        "            if new_status_line != status_line: # Solo imprimir si cambia para evitar spam\n",
        "                status_line = new_status_line\n",
        "                print(f\"\\rDescargando {model_name}: {status_line}\", end=\"\")\n",
        "\n",
        "            if progress.get('status') == 'success':\n",
        "                print(f\"\\nModelo '{model_name}' descargado/verificado correctamente.\")\n",
        "                return True\n",
        "        # Si el stream termina sin un 'success' explícito pero sin errores, asumimos que está bien.\n",
        "        print(f\"\\nProceso de descarga/verificación de '{model_name}' completado.\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- Error descargando el modelo '{model_name}': {e} ---\")\n",
        "        return False\n",
        "\n",
        "# Nombres de los modelos LLM a utilizar (ajusta según tus necesidades y disponibilidad)\n",
        "# Es buena idea usar modelos más pequeños para pruebas iniciales en Colab si los recursos son limitados.\n",
        "OLLAMA_MODEL_DEEPSEEK = 'deepseek-r1:1.5b'\n",
        "OLLAMA_MODEL_GEMMA = 'gemma3:4b'\n",
        "OLLAMA_QWEN = 'qwen3:1.7b'\n",
        "# Descargar los modelos LLM definidos\n",
        "print(\"\\n--- Iniciando descarga/verificación de modelos LLM ---\")\n",
        "modelos_a_descargar = [OLLAMA_MODEL_DEEPSEEK, OLLAMA_MODEL_GEMMA, OLLAMA_QWEN]\n",
        "modelos_descargados_ok = {}\n",
        "\n",
        "for model_tag in modelos_a_descargar:\n",
        "    if model_tag: # Asegurarse que la variable no es None o vacía\n",
        "        success = pull_ollama_model(model_tag)\n",
        "        modelos_descargados_ok[model_tag] = success\n",
        "    else:\n",
        "        print(f\"Advertencia: Una de las variables de modelo está vacía, se omite la descarga.\")\n",
        "\n",
        "print(\"\\n--- Resumen de descarga de modelos ---\")\n",
        "for model_tag, status in modelos_descargados_ok.items():\n",
        "    print(f\"Modelo {model_tag}: {'Descargado/Verificado OK' if status else 'FALLÓ LA DESCARGA/VERIFICACIÓN'}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Configuración de Ollama y descarga de modelos completada (o intentada). ---\")"
      ],
      "metadata": {
        "id": "5SeVwzfocrit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 17 (Modificada): Configuración de ChromaDB para RAG con MiniLM\n",
        "# -------------------------------------------------------------------\n",
        "# Prepara la base de datos vectorial ChromaDB con los embeddings de 'paraphrase-multilingual-MiniLM-L12-v2'.\n",
        "# La colección se re-creará cada vez que se ejecute esta celda.\n",
        "\n",
        "import chromadb # Asegurarse que chromadb está importado\n",
        "\n",
        "print(\"\\n--- Configurando ChromaDB para RAG con Embeddings MiniLM ---\")\n",
        "\n",
        "# Nombre de la colección y cliente ChromaDB\n",
        "COLLECTION_NAME_RAG_MINILM = \"news_rag_minilm\" # Nuevo nombre para la colección\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Variables globales que esta celda establecerá para la Celda 18\n",
        "active_rag_collection = None\n",
        "data_available_for_rag = False\n",
        "active_rag_embedding_model = None # Se establecerá con la instancia de MiniLM\n",
        "\n",
        "# Verificar si los componentes necesarios están disponibles\n",
        "# active_st_model_instance (de Celda 13)\n",
        "# X_st_embeddings_normalized (de Celda 13)\n",
        "# df_noticias y 'clean_body' (de Celda 6)\n",
        "\n",
        "minilm_model_loaded_celda13 = 'active_st_model_instance' in globals() and globals()['active_st_model_instance'] is not None\n",
        "minilm_embeddings_available_celda13 = 'X_st_embeddings_normalized' in globals() and globals()['X_st_embeddings_normalized'].size > 0\n",
        "dataframe_ready_celda6 = (\n",
        "    'df_noticias' in globals() and not globals()['df_noticias'].empty and\n",
        "    'clean_body' in globals()['df_noticias'].columns and\n",
        "    not globals()['df_noticias']['clean_body'].str.strip().eq('').all()\n",
        ")\n",
        "\n",
        "if minilm_model_loaded_celda13 and minilm_embeddings_available_celda13 and dataframe_ready_celda6:\n",
        "    current_minilm_model = globals()['active_st_model_instance']\n",
        "    news_embeddings_for_rag = globals()['X_st_embeddings_normalized']\n",
        "\n",
        "    df_noticias_ref = globals()['df_noticias']\n",
        "    news_texts_for_rag = df_noticias_ref['clean_body'].tolist()\n",
        "\n",
        "    num_docs_to_index = min(len(news_texts_for_rag), news_embeddings_for_rag.shape[0])\n",
        "\n",
        "    if num_docs_to_index == 0:\n",
        "        print(\"No hay documentos o embeddings (MiniLM) para indexar.\")\n",
        "    else:\n",
        "        news_texts_for_rag = news_texts_for_rag[:num_docs_to_index]\n",
        "        news_embeddings_for_rag_final = news_embeddings_for_rag[:num_docs_to_index, :] # Renombrar para claridad\n",
        "        news_ids_for_rag = [f\"news_minilm_{i}\" for i in range(num_docs_to_index)]\n",
        "\n",
        "        print(f\"Se utilizarán {num_docs_to_index} noticias/embeddings MiniLM para indexar en ChromaDB.\")\n",
        "\n",
        "        print(f\"Intentando re-crear la colección ChromaDB: '{COLLECTION_NAME_RAG_MINILM}'...\")\n",
        "        try:\n",
        "            if COLLECTION_NAME_RAG_MINILM in [c.name for c in chroma_client.list_collections()]:\n",
        "                chroma_client.delete_collection(name=COLLECTION_NAME_RAG_MINILM)\n",
        "                print(f\"Colección '{COLLECTION_NAME_RAG_MINILM}' existente eliminada.\")\n",
        "\n",
        "            active_rag_collection = chroma_client.create_collection(\n",
        "                name=COLLECTION_NAME_RAG_MINILM,\n",
        "                metadata={\"hnsw:space\": \"cosine\"} # Coseno es bueno para MiniLM normalizado\n",
        "            )\n",
        "            print(f\"Colección '{COLLECTION_NAME_RAG_MINILM}' creada/re-creada exitosamente.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al crear/re-crear la colección ChromaDB '{COLLECTION_NAME_RAG_MINILM}': {e}\")\n",
        "            active_rag_collection = None\n",
        "\n",
        "        if active_rag_collection:\n",
        "            print(\"Añadiendo documentos a ChromaDB...\")\n",
        "            try:\n",
        "                active_rag_collection.add(\n",
        "                    embeddings=news_embeddings_for_rag_final.tolist(),\n",
        "                    documents=news_texts_for_rag,\n",
        "                    ids=news_ids_for_rag\n",
        "                )\n",
        "                print(f\"--- Datos indexados en ChromaDB. Colección '{COLLECTION_NAME_RAG_MINILM}' lista con {active_rag_collection.count()} docs. ---\")\n",
        "                data_available_for_rag = True\n",
        "                active_rag_embedding_model = current_minilm_model # Guardar referencia al modelo MiniLM\n",
        "            except Exception as e:\n",
        "                print(f\"Error al añadir documentos a ChromaDB: {e}\")\n",
        "                data_available_for_rag = False\n",
        "                try:\n",
        "                    chroma_client.delete_collection(name=COLLECTION_NAME_RAG_MINILM)\n",
        "                except: pass\n",
        "        else:\n",
        "            print(f\"--- No se pudo utilizar la colección ChromaDB '{COLLECTION_NAME_RAG_MINILM}'. RAG no disponible. ---\")\n",
        "else:\n",
        "    print(\"--- ADVERTENCIA: No se pueden preparar datos para RAG con MiniLM. ---\")\n",
        "    # ... (mensajes de error detallados) ...\n",
        "    data_available_for_rag = False\n",
        "\n",
        "# Las variables globales 'active_rag_embedding_model' y 'active_rag_collection'\n",
        "# ahora deberían estar listas para que la Celda 18 las use.\n",
        "if not data_available_for_rag: # Si falló, asegurarse que las globales estén None\n",
        "    active_rag_embedding_model = None\n",
        "    active_rag_collection = None\n",
        "    print(\"\\n--- RAG con MiniLM no se configuró. Variables 'active_rag_...' pueden ser None. ---\")\n",
        "else:\n",
        "    print(f\"\\n--- RAG configurado con: Modelo='{type(active_rag_embedding_model)}', Colección='{active_rag_collection.name if active_rag_collection else 'N/A'}' ---\")"
      ],
      "metadata": {
        "id": "1UFpN82rcrk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "qDAGznpBkOwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 18 (Modificada): Prueba de Modelos LLM con RAG (usando MiniLM para RAG)\n",
        "# ---------------------------------------------------------------------------\n",
        "import ollama\n",
        "import traceback\n",
        "\n",
        "# --- 1. FUNCIÓN RAG INTERNA (AJUSTADA PARA SENTENCETRANSFORMER COMO DEFAULT) ---\n",
        "def _internal_ask_llm_with_rag(query: str, ollama_model_name: str, embedding_model_for_query, db_collection, k_results: int = 3) -> tuple[str, list[str]]:\n",
        "    print(f\"\\n--- (Interno DEBUG) Iniciando _internal_ask_llm_with_rag para: '{query}' con {ollama_model_name} ---\")\n",
        "    context_text = \"Contexto no disponible o RAG no configurado.\"\n",
        "    retrieved_docs_list = []\n",
        "\n",
        "    rag_componentes_listos = (\n",
        "        embedding_model_for_query is not None and\n",
        "        db_collection is not None and\n",
        "        globals().get('data_available_for_rag', False)\n",
        "    )\n",
        "    # ... (prints de debug iniciales) ...\n",
        "    if embedding_model_for_query:\n",
        "        print(f\"(Interno DEBUG) Tipo de embedding_model_for_query: {type(embedding_model_for_query)}\") # Debería ser SentenceTransformer\n",
        "    # ...\n",
        "\n",
        "    if rag_componentes_listos:\n",
        "        # Para MiniLM (y muchos SentenceTransformers), no se necesita prefijo de consulta.\n",
        "        # Si usaras E5, aquí agregarías \"query: \".\n",
        "        query_for_embedding = query\n",
        "        print(f\"(Interno DEBUG) query_for_embedding: '{query_for_embedding}'\")\n",
        "\n",
        "        query_embedding = None\n",
        "        try:\n",
        "            print(\"(Interno DEBUG) Intentando generar embedding para la consulta...\")\n",
        "\n",
        "            # --- LÓGICA DE CODIFICACIÓN ---\n",
        "            if hasattr(embedding_model_for_query, 'encode') and callable(getattr(embedding_model_for_query, 'encode')):\n",
        "                # Esta es la rama principal para SentenceTransformer (como MiniLM)\n",
        "                print(f\"(Interno DEBUG) Usando método .encode() de {type(embedding_model_for_query)}...\")\n",
        "                query_embedding_raw = embedding_model_for_query.encode([query_for_embedding]) # Devuelve ndarray directamente\n",
        "\n",
        "                if isinstance(query_embedding_raw, np.ndarray) and query_embedding_raw.ndim == 2 and query_embedding_raw.shape[0] == 1:\n",
        "                    query_embedding = query_embedding_raw[0] # Obtener el vector 1D\n",
        "                    print(f\"(Interno DEBUG) Embedding de consulta generado. Shape: {query_embedding.shape}, dtype: {query_embedding.dtype}, Primeros 3: {query_embedding[:3]}\")\n",
        "                elif isinstance(query_embedding_raw, list) and len(query_embedding_raw) > 0 and isinstance(query_embedding_raw[0], np.ndarray): # Por si devuelve lista de arrays\n",
        "                    query_embedding = query_embedding_raw[0]\n",
        "                    print(f\"(Interno DEBUG) Embedding de consulta (de lista) gen. Shape: {query_embedding.shape}, Primeros 3: {query_embedding[:3]}\")\n",
        "                else:\n",
        "                    print(f\"(Interno DEBUG) ERROR: Salida de .encode() no es el formato esperado (ndarray 2D de 1 fila). Tipo: {type(query_embedding_raw)}, Shape: {getattr(query_embedding_raw, 'shape', 'N/A')}\")\n",
        "                    raise ValueError(\"Salida inesperada del modelo de embedding.\")\n",
        "\n",
        "            elif 'FlagEmbedding' in str(type(embedding_model_for_query)): # Si por alguna razón se pasa BGE\n",
        "                print(\"(Interno DEBUG) Usando lógica de codificación para FlagEmbedding (BGE)...\")\n",
        "                encoded_output = embedding_model_for_query.encode([query_for_embedding], return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "                # ... (lógica para BGE como antes) ...\n",
        "                if isinstance(encoded_output, dict) and 'dense_vecs' in encoded_output:\n",
        "                    query_embedding_list = encoded_output['dense_vecs']\n",
        "                    if query_embedding_list and len(query_embedding_list) > 0:\n",
        "                        query_embedding = query_embedding_list[0]\n",
        "                    else: raise ValueError(\"'dense_vecs' vacío.\")\n",
        "                else: raise ValueError(\"Salida inesperada BGE.\")\n",
        "            else:\n",
        "                print(\"(Interno DEBUG) ERROR: embedding_model_for_query no es válido.\")\n",
        "                raise ValueError(\"Modelo de embedding para consulta no es válido.\")\n",
        "\n",
        "        except Exception as e_embed:\n",
        "            print(f\"(Interno DEBUG) EXCEPCIÓN al generar embedding de consulta: {e_embed}\")\n",
        "            traceback.print_exc()\n",
        "            llm_answer = f\"Error crítico al generar embedding para la consulta: {e_embed}\"\n",
        "            return llm_answer, retrieved_docs_list\n",
        "\n",
        "        if query_embedding is not None:\n",
        "            try:\n",
        "                # ... (consulta a ChromaDB como antes) ...\n",
        "                print(f\"(Interno DEBUG) Intentando consultar ChromaDB con embedding de shape: {query_embedding.shape}...\")\n",
        "                results = db_collection.query(\n",
        "                    query_embeddings=[query_embedding.tolist()],\n",
        "                    n_results=k_results,\n",
        "                    include=['documents', 'distances', 'metadatas']\n",
        "                )\n",
        "                # ... (procesamiento de resultados como antes) ...\n",
        "                if results and results.get('documents') and results.get('documents')[0]:\n",
        "                    retrieved_docs_list = results['documents'][0]\n",
        "                    context_text = \"\\n\\n---\\n\\n\".join(retrieved_docs_list)\n",
        "                # ...\n",
        "            except Exception as e_query_db:\n",
        "                print(f\"(Interno DEBUG) EXCEPCIÓN al consultar ChromaDB: {e_query_db}\")\n",
        "                traceback.print_exc()\n",
        "        # ...\n",
        "    # ... (resto de la función: prompt, llamada a ollama.chat) ...\n",
        "    # El prompt y la llamada a ollama.chat no cambian\n",
        "    prompt = f\"\"\"Utiliza únicamente la siguiente información de contexto para responder la pregunta.\n",
        "Si la información no está en el contexto, indica explícitamente que no puedes responder con la información proporcionada.\n",
        "No inventes respuestas. Sé conciso.\n",
        "\n",
        "Contexto Recuperado (resumen):\n",
        "---\n",
        "{context_text[:500]}... (y posiblemente más)\n",
        "---\n",
        "\n",
        "Pregunta: {query}\n",
        "\n",
        "Respuesta:\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ... (llamada a ollama.chat como antes) ...\n",
        "        response_ollama = ollama.chat(\n",
        "            model=ollama_model_name,\n",
        "            messages=[{'role': 'user', 'content': prompt}]\n",
        "        )\n",
        "        llm_answer = response_ollama['message']['content']\n",
        "    except Exception as e_llm:\n",
        "        # ... (manejo de error LLM como antes) ...\n",
        "        print(f\"(Interno DEBUG) EXCEPCIÓN al comunicarse con Ollama ({ollama_model_name}): {e_llm}\")\n",
        "        traceback.print_exc()\n",
        "        llm_answer = f\"Error: No se pudo obtener respuesta del modelo LLM ({ollama_model_name}). Detalle: {e_llm}\"\n",
        "\n",
        "    return llm_answer, retrieved_docs_list\n",
        "\n",
        "\n",
        "# --- 2. FUNCIÓN PRINCIPAL REUTILIZABLE (obtener_respuesta_llm) ---\n",
        "# ESTA FUNCIÓN NO NECESITA CAMBIOS SIGNIFICATIVOS, YA QUE USA LAS VARIABLES GLOBALES\n",
        "# 'active_rag_embedding_model' y 'active_rag_collection' que la Celda 17 ahora establece.\n",
        "def obtener_respuesta_llm(nombre_modelo_ollama: str, pregunta_usuario: str, k_documentos_rag: int = 3, mostrar_docs: bool = True):\n",
        "    print(f\"\\n>>> Solicitando respuesta del modelo: [{nombre_modelo_ollama}]\")\n",
        "    print(f\"    Pregunta: '{pregunta_usuario}'\")\n",
        "\n",
        "    embedding_model = globals().get('active_rag_embedding_model')\n",
        "    db_collection_rag = globals().get('active_rag_collection')\n",
        "\n",
        "    if embedding_model:\n",
        "        print(f\"    (Debug RAG en obtener_respuesta_llm) Usando modelo embedding: {type(embedding_model)}\")\n",
        "    else:\n",
        "        print(\"    (Debug RAG en obtener_respuesta_llm) ADVERTENCIA: active_rag_embedding_model NO encontrado.\")\n",
        "    if db_collection_rag:\n",
        "        print(f\"    (Debug RAG en obtener_respuesta_llm) Usando colección DB: {db_collection_rag.name}\")\n",
        "    else:\n",
        "        print(\"    (Debug RAG en obtener_respuesta_llm) ADVERTENCIA: active_rag_collection NO encontrada.\")\n",
        "\n",
        "    # ... (validaciones de nombre_modelo_ollama y pregunta_usuario como antes) ...\n",
        "    if not nombre_modelo_ollama or not isinstance(nombre_modelo_ollama, str):\n",
        "        error_msg = \"Error: 'nombre_modelo_ollama' debe ser un string no vacío.\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "    if not pregunta_usuario or not isinstance(pregunta_usuario, str):\n",
        "        error_msg = \"Error: 'pregunta_usuario' debe ser un string no vacío.\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n",
        "    respuesta_llm, documentos_recuperados = _internal_ask_llm_with_rag(\n",
        "        query=pregunta_usuario,\n",
        "        ollama_model_name=nombre_modelo_ollama,\n",
        "        embedding_model_for_query=embedding_model,\n",
        "        db_collection=db_collection_rag,\n",
        "        k_results=k_documentos_rag\n",
        "    )\n",
        "\n",
        "    # ... (código para mostrar documentos y la respuesta del LLM como antes) ...\n",
        "    if mostrar_docs and documentos_recuperados:\n",
        "        print(\"\\n--- Documentos Recuperados por RAG: ---\")\n",
        "        for i, doc in enumerate(documentos_recuperados):\n",
        "            print(f\"Documento RAG [{i+1}/{len(documentos_recuperados)}]:\")\n",
        "            print(f\"{doc[:300]}...\")\n",
        "            print(\"-\" * 20)\n",
        "    elif mostrar_docs:\n",
        "        print(\"\\n--- No se recuperaron documentos por RAG (o RAG no está activo o no encontró resultados). ---\")\n",
        "\n",
        "    print(f\"\\n<<< Respuesta de [{nombre_modelo_ollama}]:\\n{respuesta_llm}\")\n",
        "    print(\"-\" * 70)\n",
        "    return respuesta_llm\n",
        "\n",
        "# --- 3. EJEMPLOS DE USO DE LA FUNCIÓN PRINCIPAL ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Iniciando Pruebas de Modelos LLM (usando MiniLM para RAG) ---\")\n",
        "\n",
        "    PREGUNTA_CALOR = \"¿Que se sabe sobre la visita de Marco Rubio?\"\n",
        "\n",
        "    # Usar el modelo LLM más pequeño que tienes.\n",
        "    # De tu lista: 'deepseek-r1:1.5b' y 'qwen3:1.7b' son los más pequeños.\n",
        "    # Voy a usar 'deepseek-r1:1.5b' como ejemplo.\n",
        "    MODELO_LLM_PEQUENO = 'gemma3:4b'\n",
        "    # O podrías usar MODELO_LLM_PEQUENO = 'qwen3:1.7b'\n",
        "\n",
        "    print(f\"Probando con LLM pequeño: {MODELO_LLM_PEQUENO} y embeddings MiniLM para RAG.\")\n",
        "    obtener_respuesta_llm(nombre_modelo_ollama=MODELO_LLM_PEQUENO,\n",
        "                          pregunta_usuario=PREGUNTA_CALOR,\n",
        "                          k_documentos_rag=5,\n",
        "                          mostrar_docs=True)\n",
        "\n",
        "    # Puedes probar otro LLM también si quieres comparar\n",
        "    MODELO_LLM_OTRO = 'qwen3:1.7b'\n",
        "    print(f\"\\nComparando con LLM: {MODELO_LLM_OTRO} y embeddings MiniLM para RAG.\")\n",
        "    obtener_respuesta_llm(nombre_modelo_ollama=MODELO_LLM_OTRO,\n",
        "                          pregunta_usuario=PREGUNTA_CALOR,\n",
        "                          k_documentos_rag=5,\n",
        "                          mostrar_docs=True)\n",
        "\n",
        "    print(\"\\n--- Todas las pruebas de la celda principal completadas. ---\")"
      ],
      "metadata": {
        "id": "7VgrBkblgOEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWB-n6wglIdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "pYzDAd3WjRcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0VqWOGM9iCcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EkWAeGB4gU41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}