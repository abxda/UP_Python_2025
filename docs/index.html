<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pipeline de An√°lisis de Noticias y RAG con Python</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f4f7f6;
            color: #333;
            line-height: 1.6;
        }
        .container {
            max-width: 900px;
            margin: auto;
            background-color: #fff;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        h1, h2, h3 {
            color: #2c3e50;
            margin-bottom: 0.5em;
        }
        h1 {
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 1em;
            border-bottom: 3px solid #3498db;
            padding-bottom: 0.5em;
        }
        h2 {
            font-size: 1.8em;
            color: #3498db;
            margin-top: 1.5em;
        }
        h3 {
            font-size: 1.3em;
            color: #16a085;
        }
        .flowchart-node {
            background-color: #ecf0f1;
            border: 1px solid #bdc3c7;
            border-left: 5px solid #3498db;
            padding: 20px;
            margin-bottom: 25px;
            border-radius: 8px;
            box-shadow: 0 3px 6px rgba(0,0,0,0.05);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        .flowchart-node:hover {
            transform: translateY(-5px);
            box-shadow: 0 6px 12px rgba(0,0,0,0.1);
        }
        .arrow {
            text-align: center;
            font-size: 2em;
            color: #3498db;
            margin: 10px 0;
            font-weight: bold;
        }
        .code-mention {
            font-family: 'Courier New', Courier, monospace;
            background-color: #e8f6fd;
            color: #2980b9;
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .concept-box {
            background-color: #e8f6fd;
            border: 1px solid #aed6f1;
            padding: 15px;
            margin-top: 15px;
            border-radius: 6px;
        }
        .concept-box strong {
            color: #2980b9;
            display: block;
            margin-bottom: 5px;
        }
        .lib-list {
            list-style: none;
            padding-left: 0;
        }
        .lib-list li {
            background-color: #f9f9f9;
            margin-bottom: 8px;
            padding: 8px 12px;
            border-radius: 4px;
            border: 1px solid #eee;
            font-size: 0.95em;
        }
        .lib-list li .icon {
            margin-right: 10px;
            font-size: 1.1em;
        }
        .conclusion {
            background-color: #d4efdf;
            border-left: 5px solid #27ae60;
            padding: 20px;
            border-radius: 8px;
            margin-top: 30px;
        }
        .conclusion h2 {
            color: #27ae60;
        }
        @media (max-width: 600px) {
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; }
            .flowchart-node { padding: 15px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üìä Pipeline de An√°lisis de Noticias y RAG con Python ‚öôÔ∏è</h1>
        <p>Este documento describe el flujo de trabajo de un script de Python dise√±ado para recopilar noticias, procesarlas, analizarlas y utilizar Grandes Modelos de Lenguaje (LLMs) con t√©cnicas de Generaci√≥n Aumentada por Recuperaci√≥n (RAG) para responder preguntas basadas en la informaci√≥n obtenida.</p>

        <div class="flowchart-node">
            <h2><span class="icon">üõ†Ô∏è</span> Fase 1: Preparaci√≥n del Entorno y Dependencias</h2>
            <p>(Celdas 1-3 del script)</p>
            <p>El primer paso es configurar el entorno de trabajo instalando todas las bibliotecas necesarias. Estas herramientas proporcionan las funcionalidades para cada etapa del proceso. Posteriormente, se importan estas bibliotecas y se cargan recursos ling√º√≠sticos esenciales.</p>
            <h3>Bibliotecas Clave Instaladas:</h3>
            <ul class="lib-list">
                <li><span class="icon">üîç</span> <code class="code-mention">duckduckgo_search</code>: Para buscar noticias en la web.</li>
                <li><span class="icon">‚òÅÔ∏è</span> <code class="code-mention">wordcloud</code>, <code class="code-mention">matplotlib</code>: Para crear nubes de palabras y gr√°ficos.</li>
                <li><span class="icon">üß†</span> <code class="code-mention">transformers</code>, <code class="code-mention">sentence-transformers</code>, <code class="code-mention">flagembedding</code>: Para trabajar con modelos de lenguaje avanzados que comprenden el texto (embeddings).</li>
                <li><span class="icon">üá™üá∏</span> <code class="code-mention">spacy</code>, <code class="code-mention">nltk</code>: Para procesamiento de lenguaje natural (limpieza, tokenizaci√≥n, lemmatizaci√≥n).</li>
                <li><span class="icon">üè†</span> <code class="code-mention">ollama</code>: Para ejecutar LLMs localmente.</li>
                <li><span class="icon">üíæ</span> <code class="code-mention">chromadb</code>: Para crear una base de datos vectorial para RAG.</li>
                <li><span class="icon">üì∞</span> <code class="code-mention">newspaper3k</code>: Para extraer el contenido completo de art√≠culos de noticias.</li>
                <li><span class="icon">üìä</span> <code class="code-mention">pandas</code>, <code class="code-mention">numpy</code>, <code class="code-mention">scikit-learn</code>: Para manipulaci√≥n de datos, c√°lculos num√©ricos y machine learning (clustering, TF-IDF).</li>
            </ul>
            <div class="concept-box">
                <strong>Concepto: Recursos Ling√º√≠sticos</strong>
                Se descargan componentes como <code class="code-mention">stopwords</code> (palabras comunes como "el", "la", "de") de NLTK y el modelo <code class="code-mention">es_core_news_sm</code> de spaCy. Estos son cruciales para que el software entienda la estructura y particularidades del idioma espa√±ol, permitiendo una limpieza y an√°lisis de texto m√°s precisos.
            </div>
        </div>

        <div class="arrow">‚¨áÔ∏è</div>

        <div class="flowchart-node">
            <h2><span class="icon">üåê</span> Fase 2: Recolecci√≥n y Enriquecimiento de Noticias</h2>
            <p>(Celdas 4 y 4.1 del script)</p>
            <p>Una vez preparado el entorno, el script procede a recolectar datos. Esto implica buscar noticias en internet y luego intentar obtener el texto completo de cada art√≠culo.</p>
            <h3>Procesos:</h3>
            <ol>
                <li><strong>B√∫squeda de Noticias (Celda 4):</strong> Se utiliza la biblioteca <code class="code-mention">duckduckgo_search</code> para encontrar art√≠culos basados en palabras clave (<code class="code-mention">keywords</code>), regi√≥n e idioma especificados. Esto genera una lista inicial de noticias con URLs, t√≠tulos y res√∫menes.</li>
                <li><strong>Enriquecimiento de Contenido (Celda 4.1):</strong> Para un an√°lisis profundo, se emplea <code class="code-mention">newspaper3k</code>. Esta herramienta visita cada URL obtenida y trata de extraer el texto completo del art√≠culo, superando la limitaci√≥n de los res√∫menes iniciales. El texto extra√≠do se almacena en <code class="code-mention">df_noticias['body_enriquecido']</code>.</li>
            </ol>
            <div class="concept-box">
                <strong>Concepto: Importancia del Texto Completo</strong>
                Mientras que los t√≠tulos y res√∫menes ofrecen una idea general, el texto completo de una noticia contiene la riqueza de detalles, contextos y matices necesarios para un an√°lisis sem√°ntico exhaustivo y para que los modelos de lenguaje puedan construir respuestas m√°s informadas.
            </div>
        </div>

        <div class="arrow">‚¨áÔ∏è</div>

        <div class="flowchart-node">
            <h2><span class="icon">üßπ</span> Fase 3: Preprocesamiento y Exploraci√≥n Inicial del Texto</h2>
            <p>(Celdas 5-7 del script)</p>
            <p>El texto "crudo" recolectado necesita ser limpiado y transformado para facilitar el an√°lisis computacional. Adem√°s, se realizan exploraciones iniciales para entender la naturaleza de los datos.</p>
            <h3>T√©cnicas Aplicadas:</h3>
            <ul>
                <li><strong>Nubes de Palabras (Celda 5):</strong> Se generan visualizaciones (<code class="code-mention">WordCloud</code>) de las palabras m√°s frecuentes en los cuerpos de las noticias. Esto ofrece una primera impresi√≥n visual de los temas dominantes antes y despu√©s del preprocesamiento.</li>
                <li><strong>Limpieza de Texto (Celda 6):</strong>
                    <ul>
                        <li>Conversi√≥n a min√∫sculas.</li>
                        <li>Eliminaci√≥n de signos de puntuaci√≥n.</li>
                        <li>Remoci√≥n de stopwords (palabras comunes sin gran valor sem√°ntico).</li>
                        <li>El resultado se guarda en <code class="code-mention">df_noticias['clean_body']</code>.</li>
                    </ul>
                </li>
                <li><strong>Lematizaci√≥n (Celda 6):</strong> Usando <code class="code-mention">spaCy</code>, las palabras se reducen a su forma base o lema (ej: "corriendo" -> "correr"). Esto ayuda a agrupar palabras con el mismo significado ra√≠z. El resultado se guarda en <code class="code-mention">df_noticias['lemmatized_body']</code>.</li>
                <li><strong>An√°lisis de Fuentes (Celda 7):</strong> Se cuenta y visualiza la cantidad de noticias por cada fuente (<code class="code-mention">source</code>), permitiendo identificar los or√≠genes m√°s prominentes de la informaci√≥n.</li>
            </ul>
            <div class="concept-box">
                <strong>Concepto: Lemmatizaci√≥n vs. Stemming</strong>
                La lemmatizaci√≥n es un proceso m√°s sofisticado que el stemming. Mientras el stemming simplemente corta las terminaciones de las palabras (pudiendo generar formas no existentes), la lemmatizaci√≥n utiliza un diccionario y an√°lisis morfol√≥gico para devolver la forma can√≥nica (lema) de la palabra, siendo m√°s precisa para el an√°lisis sem√°ntico.
            </div>
        </div>

        <div class="arrow">‚¨áÔ∏è</div>

        <div class="flowchart-node">
            <h2><span class="icon">üî¢</span> Fase 4: Vectorizaci√≥n del Texto</h2>
            <p>(Celdas 8, 9, 10, 13 del script)</p>
            <p>Las computadoras procesan n√∫meros, no texto directamente. La vectorizaci√≥n convierte el texto preprocesado en representaciones num√©ricas (vectores) que pueden ser utilizadas por algoritmos de machine learning.</p>
            <h3>M√©todos de Vectorizaci√≥n Empleados:</h3>
            <ul>
                <li><strong>TF-IDF (Term Frequency-Inverse Document Frequency) (Celda 9):</strong>
                    <p>Calcula la importancia de una palabra en un documento en relaci√≥n con una colecci√≥n de documentos. El script utiliza TF-IDF sobre n-gramas de caracteres (<code class="code-mention">TfidfVectorizer(analyzer='char')</code>) del texto lematizado. Esto puede capturar patrones subyacentes incluso con variaciones morfol√≥gicas o peque√±os errores.</p>
                </li>
                <li><strong>Embeddings Sem√°nticos (Celdas 10, 13):</strong>
                    <p>Representaciones vectoriales densas que capturan el significado y contexto de las palabras o frases. Palabras con significados similares tendr√°n vectores cercanos en un espacio multidimensional. El script utiliza dos tipos de modelos avanzados:</p>
                    <ul>
                        <li><code class="code-mention">BGE-M3 ('BAAI/bge-m3')</code> (Celda 10): Un modelo de embedding multiling√ºe de alto rendimiento, usado para un tipo de clustering. Se aplica sobre <code class="code-mention">clean_body</code>.</li>
                        <li><code class="code-mention">Sentence-Transformers ('paraphrase-multilingual-MiniLM-L12-v2')</code> (Celda 13): Otro modelo eficiente para generar embeddings de frases, tambi√©n multiling√ºe. Estos embeddings se utilizar√°n principalmente para el sistema RAG y para otro conjunto de experimentos de clustering. Se aplica sobre <code class="code-mention">clean_body</code>.</li>
                    </ul>
                </li>
            </ul>
            <div class="concept-box">
                <strong>Concepto: Normalizaci√≥n de Vectores</strong>
                Despu√©s de generar los embeddings (tanto con BGE-M3 como con Sentence-Transformers), se aplica una normalizaci√≥n L2 (<code class="code-mention">normalize()</code>). Esto asegura que todos los vectores tengan la misma magnitud (longitud unitaria), lo cual es crucial para que las medidas de distancia (como la similitud del coseno o la distancia euclidiana) sean m√°s significativas, especialmente en algoritmos de clustering y b√∫squeda de similitud.
            </div>
        </div>

        <div class="arrow">‚¨áÔ∏è</div>

        <div class="flowchart-node">
            <h2><span class="icon">üß©</span> Fase 5: Clustering y Descubrimiento de Temas</h2>
            <p>(Celdas 9, 11, 14 del script)</p>
            <p>Con las noticias representadas como vectores, se aplican algoritmos de clustering para agrupar autom√°ticamente art√≠culos con contenido tem√°tico similar. Esto ayuda a descubrir los principales temas presentes en el corpus de noticias.</p>
            <h3>Algoritmos y Enfoques de Clustering:</h3>
            <ul>
                <li><strong>Clustering sobre Vectores TF-IDF (Celda 9):</strong>
                    <ul>
                        <li><strong>DBSCAN:</strong> Algoritmo basado en densidad que no requiere especificar el n√∫mero de clusters de antemano y puede identificar ruido.</li>
                        <li><strong>KMeans:</strong> Algoritmo que agrupa los datos en un n√∫mero predefinido de clusters (k).</li>
                    </ul>
                </li>
                <li><strong>Clustering sobre Embeddings BGE-M3 (Celda 11):</strong>
                    <ul>
                        <li><strong>KMeans y DBSCAN:</strong> Se aplican nuevamente estos algoritmos, pero esta vez sobre los embeddings sem√°nticos generados por BGE-M3, lo que puede capturar agrupaciones basadas en significado m√°s profundo.</li>
                    </ul>
                </li>
                <li><strong>Clustering sobre Embeddings Sentence-Transformer (MiniLM) (Celda 14):</strong>
                    <ul>
                        <li><strong>KMeans y DBSCAN:</strong> De forma similar, se realiza clustering con los embeddings del modelo MiniLM, que tambi√©n son sem√°nticamente ricos.</li>
                    </ul>
                </li>
            </ul>
            <div class="concept-box">
                <strong>Concepto: Par√°metros de Clustering</strong>
                Para DBSCAN, par√°metros como <code class="code-mention">eps</code> (distancia m√°xima entre muestras para que una se considere vecina de la otra) y <code class="code-mention">min_samples</code> (n√∫mero de muestras en una vecindad para que un punto sea considerado central) son cruciales y a menudo requieren ajuste experimental. Para KMeans, la elecci√≥n de <code class="code-mention">k</code> (n√∫mero de clusters) es fundamental.
            </div>
        </div>

        <div class="arrow">‚¨áÔ∏è</div>

        <div class="flowchart-node">
            <h2><span class="icon">üñºÔ∏è</span> Fase 6: Visualizaci√≥n Avanzada de Clusters</h2>
            <p>(Celdas 8, 12, 15 del script)</p>
            <p>Para interpretar y validar los resultados del clustering, se emplean t√©cnicas de visualizaci√≥n.</p>
            <h3>M√©todos de Visualizaci√≥n:</h3>
            <ul>
                <li><strong>Nubes de Palabras por Cluster (Funci√≥n <code class="code-mention">visualizar_clusters_detalle</code> en Celda 8):</strong> Para cada cluster identificado, se genera una nube de palabras espec√≠fica con el texto de las noticias agrupadas en √©l. Esto proporciona una visi√≥n tem√°tica de cada grupo. Adicionalmente, se listan los t√≠tulos de algunas noticias pertenecientes a cada cluster.</li>
                <li><strong>t-SNE (t-distributed Stochastic Neighbor Embedding) (Celdas 12, 15):</strong> Es una t√©cnica de reducci√≥n de dimensionalidad que proyecta los embeddings de alta dimensionalidad (cientos de dimensiones) a un espacio 2D. Esto permite graficar las noticias como puntos, donde idealmente, noticias del mismo tema (cluster) aparecer√°n agrupadas visualmente. Se aplica tanto a los embeddings BGE-M3 como a los de MiniLM.</li>
            </ul>
        </div>

        <div class="arrow">‚¨áÔ∏è</div>

        <div class="flowchart-node">
            <h2><span class="icon">üöÄ</span> Fase 7: Preparaci√≥n para RAG (Generaci√≥n Aumentada por Recuperaci√≥n)</h2>
            <p>(Celdas 16-17 del script)</p>
            <p>Esta fase configura el sistema para mejorar las respuestas de los Grandes Modelos de Lenguaje (LLMs) utilizando informaci√≥n espec√≠fica de las noticias recolectadas.</p>
            <h3>Componentes Clave:</h3>
            <ul>
                <li><strong>Configuraci√≥n de Ollama (Celda 16):</strong> Se inicia el servidor <code class="code-mention">Ollama</code>, una herramienta que permite ejecutar LLMs de c√≥digo abierto (como <code class="code-mention">gemma3:4b</code>, <code class="code-mention">deepseek-r1:1.5b</code>, <code class="code-mention">qwen3:1.7b</code>) localmente. El script verifica y descarga estos modelos si es necesario.</li>
                <li><strong>Creaci√≥n de Base de Datos Vectorial con ChromaDB (Celda 17):</strong>
                    <ol>
                        <li>Se utiliza una instancia de <code class="code-mention">ChromaDB</code>, una base de datos dise√±ada para almacenar y buscar eficientemente embeddings.</li>
                        <li>Los textos limpios de las noticias (<code class="code-mention">df_noticias['clean_body']</code>) son convertidos en embeddings utilizando el modelo <code class="code-mention">paraphrase-multilingual-MiniLM-L12-v2</code> (cargado en Celda 13).</li>
                        <li>Estos embeddings, junto con los textos originales como metadatos, se almacenan en una colecci√≥n de ChromaDB. Esto crea un √≠ndice de b√∫squeda sem√°ntica sobre el contenido de las noticias.</li>
                    </ol>
                </li>
            </ul>
            <div class="concept-box">
                <strong>Concepto: RAG (Retrieval Augmented Generation)</strong>
                Es una t√©cnica que combina un sistema de recuperaci√≥n de informaci√≥n (como ChromaDB buscando embeddings similares) con un modelo generativo (un LLM). Antes de que el LLM genere una respuesta, el sistema RAG recupera fragmentos de informaci√≥n relevante de una base de conocimientos (en este caso, las noticias indexadas) y los proporciona al LLM como contexto adicional. Esto ayuda a que las respuestas del LLM sean m√°s factuales, actualizadas y espec√≠ficas al dominio de los datos proporcionados, reduciendo las "alucinaciones".
            </div>
        </div>

        <div class="arrow">‚¨áÔ∏è</div>

        <div class="flowchart-node">
            <h2><span class="icon">üí¨</span> Fase 8: Interacci√≥n con LLMs mediante RAG</h2>
            <p>(Celda 18 del script)</p>
            <p>Con el sistema RAG configurado, el script demuestra c√≥mo interactuar con los LLMs locales para responder preguntas basadas en el corpus de noticias.</p>
            <h3>Flujo de Interacci√≥n RAG:</h3>
            <ol>
                <li><strong>Pregunta del Usuario:</strong> Se define una pregunta (ej: "¬øQu√© se sabe sobre la visita de Marco Rubio?").</li>
                <li><strong>Generaci√≥n de Embedding para la Pregunta:</strong> La pregunta del usuario se convierte en un vector embedding utilizando el mismo modelo <code class="code-mention">paraphrase-multilingual-MiniLM-L12-v2</code> que se us√≥ para indexar las noticias.</li>
                <li><strong>B√∫squeda Sem√°ntica en ChromaDB:</strong> El embedding de la pregunta se utiliza para consultar la colecci√≥n en ChromaDB. La base de datos devuelve los <code class="code-mention">k</code> documentos (textos de noticias) cuyos embeddings son m√°s similares (sem√°nticamente m√°s cercanos) al embedding de la pregunta.</li>
                <li><strong>Construcci√≥n del Prompt Aumentado:</strong> Los textos de las noticias recuperadas se incorporan en el prompt que se enviar√° al LLM, sirviendo como contexto. El prompt instruye al LLM a basar su respuesta en esta informaci√≥n.</li>
                <li><strong>Generaci√≥n de Respuesta por el LLM:</strong> El prompt aumentado se env√≠a a uno of los LLMs cargados a trav√©s de <code class="code-mention">Ollama</code> (ej: <code class="code-mention">gemma3:4b</code>). El LLM genera una respuesta basada tanto en la pregunta original como en el contexto espec√≠fico proporcionado por los documentos recuperados.</li>
                <li><strong>Visualizaci√≥n:</strong> Se muestra la respuesta del LLM y, opcionalmente, los documentos recuperados que sirvieron de contexto.</li>
            </ol>
            <div class="concept-box">
                <strong>Concepto: Beneficios del RAG</strong>
                <ul>
                    <li><strong>Reducci√≥n de Alucinaciones:</strong> Al "anclar" al LLM con informaci√≥n factual del contexto, se disminuye la probabilidad de que invente respuestas.</li>
                    <li><strong>Conocimiento Actualizado/Espec√≠fico:</strong> Permite al LLM responder sobre informaci√≥n muy reciente o espec√≠fica del dominio que no estaba en sus datos de entrenamiento originales.</li>
                    <li><strong>Transparencia:</strong> Al poder ver los documentos recuperados, se puede entender mejor la base de la respuesta del LLM.</li>
                </ul>
            </div>
        </div>

        <div class="conclusion">
            <h2><span class="icon">üéØ</span> Conclusi√≥n y Potencial</h2>
            <p>Este pipeline demuestra un flujo de trabajo completo y potente, desde la recolecci√≥n y preprocesamiento de datos textuales hasta la aplicaci√≥n de t√©cnicas avanzadas de machine learning como el clustering y la generaci√≥n aumentada por recuperaci√≥n con LLMs locales.</p>
            <p>Las t√©cnicas y herramientas presentadas tienen un amplio espectro de aplicaciones, incluyendo:</p>
            <ul>
                <li>Sistemas de respuesta a preguntas especializados.</li>
                <li>An√°lisis de tendencias y opiniones en grandes vol√∫menes de texto.</li>
                <li>Herramientas de investigaci√≥n inteligente y descubrimiento de informaci√≥n.</li>
                <li>Chatbots con conocimiento espec√≠fico de dominio.</li>
            </ul>
            <p>La combinaci√≥n de la comprensi√≥n sem√°ntica de los embeddings, la capacidad de agrupaci√≥n del clustering y el poder generativo de los LLMs locales potenciados por RAG abre un abanico de posibilidades para extraer valor e inteligencia a partir de datos textuales.</p>
        </div>
    </div>
</body>
</html>
