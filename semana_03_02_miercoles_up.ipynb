{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoyhwGSve02a4i8hoMyRnq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abxda/UP_Python_2025/blob/main/semana_03_02_miercoles_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **De Números a Mapas: La Magia de los Datos Geoespaciales**\n",
        "\n",
        "**Objetivo Principal:**\n",
        "\n",
        "- **Integración de Datos Censo y Geoespaciales:** Aprenderás el arte de fusionar datos **censales** (información demográfica y socioeconómica) con datos **geoespaciales** (información con ubicación geográfica) utilizando Python y la robusta base de datos DuckDB.\n",
        "  \n",
        "  - **¿Qué son los Datos Censales?**  \n",
        "    Son la columna vertebral de la estadística de un país. Provienen de censos de población y vivienda, recolectando información esencial sobre habitantes y hogares: edad, género, educación, empleo, características de vivienda, y mucho más.  \n",
        "    Piensa en ellos como un retrato detallado de la sociedad en un momento dado.\n",
        "  \n",
        "  - **¿Qué son los Datos Geoespaciales?**  \n",
        "    Son datos que tienen una ubicación asociada en la Tierra. Pueden representar entidades geográficas (manzanas, municipios, estados) o fenómenos que ocurren en un lugar específico. La clave es la referencia a la ubicación.\n",
        "\n",
        "- **Automatización del Flujo de Trabajo:**  \n",
        "  Dominarás las técnicas para automatizar cada etapa del proceso: desde la **descarga** de datos desde fuentes oficiales, la **extracción** de información relevante, la **conversión** a formatos eficientes, hasta la **unión** de datos para análisis avanzados.  \n",
        "  La automatización es muy importante para la eficiencia y la reproducibilidad en el manejo de datos.\n",
        "\n",
        "**¿Por qué es fundamental entender esto?**\n",
        "\n",
        "- **Análisis Profundo y Contextual:**  \n",
        "  Conocer **dónde** ocurren los fenómenos demográficos y socioeconómicos, y **quiénes** se ven afectados, es esencial para un análisis estadístico verdaderamente significativo. La ubicación añade una dimensión crítica a los datos.\n",
        "\n",
        "- **Planeación Urbana y Desarrollo Territorial Inteligente:**  \n",
        "  La combinación de datos censales y geoespaciales es indispensable para la **planeación urbana efectiva**, la **asignación de recursos**, el diseño de **políticas públicas** basadas en evidencia, y para promover un **desarrollo territorial más equitativo y sostenible**.\n",
        "\n",
        "- **Toma de Decisiones Informada y Basada en Evidencia:**  \n",
        "  Al mapear variables clave (población vulnerable, acceso a servicios, densidad de vivienda, etc.), podemos **identificar patrones espaciales**, **detectar desigualdades**, y **visualizar tendencias** que serían invisibles en tablas de datos aisladas.  \n",
        "  Los mapas son poderosas herramientas de comunicación y análisis.\n",
        "\n",
        "**En resumen, en esta clase aprenderás a:**\n",
        "1. **Descarga Programática de Datos:** Utilizar Python para acceder y descargar datos del INEGI (Instituto Nacional de Estadística y Geografía) de forma automática desde internet.  \n",
        "2. **Extracción Inteligente de Shapefiles:** Descomprimir y extraer los componentes esenciales de los archivos Shapefile, el formato estándar para datos geográficos vectoriales.  \n",
        "3. **DuckDB: Tu Base de Datos Personal y Potente:** Introducción a DuckDB, una base de datos analítica embebida, ideal para el manejo eficiente de grandes volúmenes de datos geoespaciales y tabulares, sin necesidad de servidores complejos.  \n",
        "4. **Unión de Datos y Geometría:** Combinar datos alfanuméricos del censo (estadísticas) con las representaciones geométricas de los Shapefiles (mapas digitales) para crear un conjunto de datos enriquecido y listo para el análisis espacial.  \n",
        "5. **Organización y Limpieza del Entorno de Trabajo:** Establecer prácticas de organización de archivos y limpieza de datos para asegurar un flujo de trabajo eficiente, reproducible y libre de errores.\n"
      ],
      "metadata": {
        "id": "EkrOYGrJw7CK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparando el Escenario - Configuración del Entorno y Librerías Esenciales**\n",
        "\n",
        "\n",
        "**Librerías Python: Tus Herramientas Clave**\n",
        "\n",
        "- **¿Qué son las Librerías en Python?** Piensa en las librerías como cajas de herramientas especializadas que extienden las capacidades básicas de Python. Cada librería contiene funciones y módulos pre-construidos que te ahorran tiempo y esfuerzo al realizar tareas específicas. En lugar de reinventar la rueda, ¡utilizamos herramientas probadas y optimizadas!\n",
        "  \n",
        "  - **`requests`:** **El Cliente Web Programático.** `requests` permite a Python comunicarse con servidores web a través de **HTTP** (Hypertext Transfer Protocol), el lenguaje de la web. Con `requests`, puedes enviar **solicitudes web** (como las que hace tu navegador) y recibir **respuestas** de servidores, lo que te permite descargar archivos, acceder a APIs (Interfaces de Programación de Aplicaciones), y mucho más. Esencial para la **descarga programática de datos**.\n",
        "  - **`tqdm`:** **La Barra de Progreso Amigable.** `tqdm` (que viene de \"taqaddum\", progreso en árabe) es una librería que crea **barras de progreso visuales** en tu terminal o entorno de Jupyter/Colab. Estas barras te dan retroalimentación en tiempo real sobre la duración de procesos largos (descargas, bucles, cálculos), mejorando la experiencia de usuario y permitiéndote monitorear el avance de tus scripts.\n",
        "  - **`os` y `shutil`:** **Los Gestores de Archivos y Carpetas.** El módulo `os` (Operative System) proporciona funciones para interactuar con el **sistema operativo**, permitiéndote realizar tareas como crear y eliminar **directorios (carpetas)**, manipular **rutas de archivos**, ejecutar comandos del sistema, y obtener información del entorno. `shutil` (Shell Utilities) extiende estas capacidades con funciones de **más alto nivel** para operaciones con archivos y carpetas, como **copiar, mover y eliminar directorios completos**. Indispensables para la **organización y gestión de datos**.\n",
        "  - **`zipfile`:** **El Descompresor de Archivos ZIP.** `zipfile` permite a Python trabajar con archivos **ZIP** (un formato de compresión común). Puedes usarlo para **crear archivos ZIP**, **extraer el contenido de archivos ZIP**, y **manipular archivos comprimidos** dentro de ZIPs. Fundamental para trabajar con datos del INEGI que a menudo se distribuyen en formato ZIP.\n",
        "  - **`duckdb`:** **La Base de Datos Analítica Embebida.** DuckDB es una base de datos **relacional (SQL)** diseñada para **análisis de datos de alto rendimiento**. Es **embebida**, lo que significa que no requiere un servidor separado; toda la base de datos reside en un **único archivo**. Es **columnar**, lo que la hace extremadamente rápida para consultas analíticas (agregaciones, filtros, etc.). Además, tiene **soporte espacial**, permitiéndote manejar datos geográficos directamente en la base de datos. Ideal para **análisis rápido y eficiente de datos censales y geoespaciales**.\n",
        "  - **`geopandas`:** **El Manipulador de Datos Geoespaciales en Python.** `geopandas` extiende las capacidades de `pandas` (la librería estrella para análisis de datos tabulares en Python) para manejar **datos geoespaciales**. Permite leer y escribir **formatos geoespaciales** como Shapefiles y GeoParquet, realizar **operaciones geoespaciales** (uniones espaciales, cálculos de área/distancia, etc.), y **visualizar datos geográficos** en mapas. Es la herramienta clave para **procesar y analizar mapas digitales**.\n",
        "  - **`folium`:** **El Creador de Mapas Interactivos Web.** `folium` facilita la creación de **mapas web interactivos** directamente desde Python, utilizando la librería **Leaflet.js** de JavaScript. Puedes generar mapas con **múltiples capas**, **marcadores**, **ventanas emergentes (popups)**, **capas de teselas base (basemaps)**, y mucho más. Perfecto para **visualizar datos geoespaciales** de forma interactiva en un navegador web.\n",
        "  - **`chardet`:** **El Detector de Codificación de Texto.** `chardet` ayuda a **detectar la codificación de caracteres** de archivos de texto. La codificación define cómo se representan los caracteres de texto en bytes (UTF-8, ISO-8859-1, etc.). Si un archivo de texto está en una codificación incorrecta, los caracteres especiales (acentos, símbolos) pueden aparecer incorrectamente. `chardet` ayuda a Python a **\"adivinar\" la codificación correcta** para leer archivos de texto sin problemas.\n",
        "  - **`pyarrow`:** **El Acelerador de Datos Columnar.** `pyarrow` proporciona una **plataforma de desarrollo para datos en memoria** y **formatos de intercambio de datos columnar**. Optimiza el rendimiento y la eficiencia en el manejo de grandes conjuntos de datos, especialmente para formatos columnares como **Parquet** y **GeoParquet**. Mejora la **velocidad de lectura y escritura de datos** entre diferentes sistemas y librerías.\n",
        "\n",
        "**Instalación de Librerías: ¡Equipando tu Entorno!**\n",
        "\n",
        "Para usar estas poderosas librerías, primero debes instalarlas en tu **entorno Python**. Un **entorno Python** es un espacio aislado donde se instalan las librerías necesarias para un proyecto específico, evitando conflictos entre diferentes proyectos. Puedes usar entornos virtuales (`venv`) o administradores de entornos como **Anaconda**.\n",
        "\n",
        "**Organización de Carpetas: ¡Tu Espacio de Trabajo Ordenado!**\n",
        "\n",
        "Una estructura de carpetas clara es fundamental para cualquier proyecto de datos. Te recomendamos crear la siguiente estructura:\n",
        "\n",
        "```\n",
        "inegi/                  # Carpeta raíz para datos del INEGI\n",
        "├── ccpvagebmza/        # Para descargas de CSV del Censo de Población y Vivienda (AGEB y Manzana)\n",
        "└── mgccpv/             # Para descargas del Marco Geoestadístico Censal (Shapefiles)\n",
        "    └── shp/            # Subcarpeta para guardar los Shapefiles extraídos\n",
        "        └── m/          # Subcarpeta para Shapefiles de Manzanas\n",
        "```\n",
        "\n",
        "Vamos a crear estas carpetas usando código Python.\n",
        "\n",
        "**¿Por qué la organización es tan importante?**\n",
        "\n",
        "- **Claridad y Mantenimiento:** Una estructura organizada hace que tu proyecto sea **fácil de entender y mantener**. Tú (y otros) sabrán dónde encontrar los datos originales, los archivos intermedios y los resultados finales.\n",
        "- **Reproducibilidad:** Un proyecto bien organizado es más **reproducible**. Si necesitas volver a ejecutar tu análisis en el futuro, o compartirlo con otros, la estructura de carpetas clara asegura que todos puedan entender y replicar tus pasos.\n",
        "- **Eficiencia:** Encontrar archivos rápidamente y tener una idea clara de la ubicación de cada componente ahorra tiempo y reduce la frustración. Un espacio de trabajo ordenado **aumenta tu productividad**."
      ],
      "metadata": {
        "id": "kiblFicv3GLI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_s8a_2c3vEA"
      },
      "outputs": [],
      "source": [
        "pip install duckdb geopandas fsspec folium geohexgrid matplotlib tqdm requests chardet pyarrow --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from zipfile import ZipFile\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import duckdb\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "import shutil\n",
        "import chardet"
      ],
      "metadata": {
        "id": "6FsaEBMx4AI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descarga Programática de Datos - ¡Python al Rescate de la Información en la Web!**\n",
        "\n",
        "**Descarga Programática: ¿Qué significa?**\n",
        "\n",
        "- **Descarga Manual vs. Programática:** Normalmente, para descargar un archivo de internet, abres tu navegador web, encuentras el enlace de descarga y haces clic. Esto es **descarga manual**. La **descarga programática** significa usar código (en este caso Python) para realizar este proceso **automáticamente**, sin intervención humana directa en cada paso.\n",
        "- **APIs y URLs: Las Direcciones de la Información Web:** Para descargar datos programáticamente, necesitas saber la **URL (Uniform Resource Locator)** del archivo que quieres descargar. Una URL es la dirección web específica de un recurso (un archivo, una página web, etc.). Muchos sitios web también ofrecen **APIs (Application Programming Interfaces)**, que son interfaces programáticas que permiten a las aplicaciones (como tu script de Python) interactuar con el servidor web de forma estructurada y automatizada, a menudo para obtener datos en formatos específicos (JSON, CSV, etc.). En nuestro caso, usaremos URLs directas a archivos ZIP del INEGI.\n",
        "- **HTTP Requests: Hablando el Lenguaje de la Web:** Cuando tu navegador web o tu script de Python quiere obtener información de un servidor web, utiliza **HTTP Requests**. `requests.get(url)` en Python envía una solicitud **GET** al servidor web en la `url` especificada. Una solicitud GET es la más común para **obtener recursos** (como descargar archivos o páginas web). El servidor web responde con una **HTTP Response**, que contiene el **código de estado** (indicando si la solicitud fue exitosa, si hubo un error, etc.) y, si la solicitud fue exitosa, el **contenido del recurso solicitado** (el archivo que queremos descargar).\n",
        "\n",
        "**La Función `download(url, directory)`: Tu Asistente de Descargas Inteligente**\n",
        "\n",
        "La función `download(url, directory)` que creamos encapsula la lógica para descargar archivos de forma programática. Veamos sus componentes clave:\n",
        "\n",
        "```python\n",
        "def download(url, directory):\n",
        "    # ... (código de la función) ...\n",
        "```\n",
        "\n",
        "- **`url` (Parámetro de Entrada):** Es la **URL completa** del archivo que quieres descargar. Por ejemplo: `\"https://www.inegi.org.mx/contenidos/productos/...\"`.\n",
        "- **`directory` (Parámetro de Entrada):** Es la **ruta de la carpeta** donde quieres guardar el archivo descargado en tu sistema de archivos. Por ejemplo: `\"./inegi/mgccpv/\"`.\n",
        "- **`filename = url.split('/')[-1]`:** Esta línea extrae el **nombre del archivo** de la URL. `url.split('/')` divide la URL en una lista de cadenas usando `/` como separador. `[-1]` accede al **último elemento** de la lista, que suele ser el nombre del archivo (ejemplo: `\"09_ciudaddemexico.zip\"`).\n",
        "- **`filepath = os.path.join(directory, filename)`:** Construye la **ruta completa del archivo** combinando el `directory` y el `filename` usando `os.path.join()`. `os.path.join()` es una forma **independiente del sistema operativo** de construir rutas de archivo (funciona tanto en Windows, macOS, Linux).\n",
        "- **`if os.path.exists(filepath): ... return`:** Implementa una **verificación de existencia**. Antes de iniciar la descarga, la función comprueba si el archivo ya existe en la `filepath`. Si existe, imprime un mensaje indicando que el archivo ya está descargado y usa `return` para salir de la función **sin realizar la descarga de nuevo**. Esto es crucial para evitar descargas redundantes y ahorrar tiempo.\n",
        "- **`response = requests.get(url, stream=True)`:** Realiza la **solicitud de descarga** usando `requests.get(url, stream=True)`. `stream=True` es importante para **descargas grandes**, ya que indica a `requests` que descargue el contenido en **\"streaming\" (flujo continuo)**, en lugar de cargar todo el archivo en memoria de una sola vez. Esto es más eficiente para archivos grandes y evita problemas de memoria.\n",
        "- **`with open(filepath, 'wb') as f, tqdm(...) as pbar:`:** Abre el archivo para **escritura binaria (`'wb'`)** y utiliza un **administrador de contexto (`with`)** para asegurar que el archivo se cierre correctamente después de la descarga, incluso si ocurren errores. También inicializa la barra de progreso `tqdm` dentro del mismo `with` para manejarla correctamente.\n",
        "  - **`tqdm(...)`:** Configura la barra de progreso:\n",
        "    - `total=total_size`: Establece el **tamaño total** de la barra de progreso, tomado del encabezado `Content-Length` de la respuesta HTTP (si está disponible).\n",
        "    - `unit='B'`, `unit_scale=True`: Configura las **unidades de la barra de progreso** a bytes y habilita el escalado automático a KB, MB, GB, etc., para una mejor legibilidad.\n",
        "    - `desc=filename`: Establece la **descripción de la barra de progreso** al nombre del archivo, para identificar claramente qué archivo se está descargando.\n",
        "  - **`for data in response.iter_content(chunk_size=1024):`:** Itera sobre el **contenido de la respuesta HTTP** en **bloques (chunks) de 1024 bytes (1KB)** usando `response.iter_content(chunk_size=1024)`. Esto permite procesar el contenido de la descarga **bloque por bloque**, lo cual es esencial para descargas en streaming y archivos grandes.\n",
        "  - **`f.write(data)`:** Escribe cada **bloque de datos (`data`)** en el archivo abierto (`f`).\n",
        "  - **`pbar.update(len(data))`:** **Actualiza la barra de progreso** con la cantidad de bytes descargados en el bloque actual (`len(data)`).\n",
        "- **`print(f\"Descarga completada: {filename}\")`:** Imprime un mensaje informando que la descarga del archivo ha finalizado con éxito.\n",
        "\n",
        "**Beneficios de la Descarga Programática:**\n",
        "\n",
        "- **Automatización y Eficiencia:** Automatiza tareas repetitivas y ahorra tiempo. Descarga archivos con una sola línea de código.\n",
        "- **Reproducibilidad y Consistencia:** Asegura que los datos se descarguen siempre de la misma fuente y manera, mejorando la reproducibilidad de tus análisis.\n",
        "- **Integración en Flujos de Trabajo:** Permite integrar la descarga de datos directamente en tus scripts de análisis, creando flujos de trabajo completos y automatizados.\n",
        "- **Escalabilidad:** Facilita la descarga de múltiples archivos o datos de forma masiva, sin intervención manual para cada archivo."
      ],
      "metadata": {
        "id": "2hTf7aaF3zAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 1. Función de descarga\n",
        "# ================\n",
        "def download(url, directory):\n",
        "    \"\"\"\n",
        "    Descarga un archivo desde la URL especificada y lo guarda en 'directory'.\n",
        "    Si el archivo ya existe, no realiza la descarga.\n",
        "    \"\"\"\n",
        "    filename = url.split('/')[-1]\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"El archivo {filename} ya existe, no se descarga de nuevo.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Descargando {filename} ...\")\n",
        "    time.sleep(2)  # Simulamos retardo\n",
        "    r = requests.get(url, stream=True)\n",
        "    total_size = int(r.headers.get('content-length', 0))\n",
        "    with open(filepath, 'wb') as f:\n",
        "        for data in tqdm(r.iter_content(1024),\n",
        "                        total=total_size/1024 if total_size else None,\n",
        "                        unit='KB', desc=filename):\n",
        "            f.write(data)\n",
        "    print(f\"Descarga completada: {filename}\\n\")"
      ],
      "metadata": {
        "id": "Q7eaCZ_L4Zma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extracción de Shapefiles desde Archivos ZIP**\n",
        "\n",
        "**Archivos Shapefile: El Formato Clásico para Datos Geoespaciales Vectoriales**\n",
        "\n",
        "- **¿Qué es un Shapefile?** Un Shapefile es un formato de archivo **popular y ampliamente utilizado** para almacenar **datos geoespaciales vectoriales**. Fue desarrollado por Esri (Environmental Systems Research Institute), una empresa líder en software GIS (Geographic Information System). Aunque es un formato \"clásico\" y tiene algunas limitaciones (que veremos más adelante), sigue siendo muy común para la distribución de datos geográficos.\n",
        "- **Vectorial vs. Raster: Dos Formas de Representar el Espacio.** Existen dos formas principales de representar información geoespacial:\n",
        "  - **Datos Vectoriales:** Representan entidades geográficas como **puntos, líneas y polígonos**. Por ejemplo, un punto puede representar una ciudad, una línea puede representar una carretera, y un polígono puede representar un límite de municipio o un edificio. Los Shapefiles son un formato para datos vectoriales.\n",
        "  - **Datos Raster:** Representan el espacio como una **cuadrícula de celdas o píxeles**. Cada celda tiene un valor que representa alguna característica en esa ubicación (ejemplo: elevación, temperatura, uso de suelo). Imágenes de satélite y fotografías aéreas son ejemplos de datos raster.\n",
        "- **Componentes de un Shapefile: Un Conjunto de Archivos Colaborativos.** Como mencionamos antes, un Shapefile no es un solo archivo, sino un **conjunto de archivos relacionados**, cada uno con una extensión específica, que trabajan juntos para definir la información geoespacial. Los componentes **esenciales** que necesitamos extraer son:\n",
        "  - **`.shp` (Shapefile Main File):** Contiene las **geometrías vectoriales** en sí mismas: las coordenadas que definen los puntos, líneas y polígonos que representan las entidades geográficas. Es el archivo **principal** del Shapefile.\n",
        "  - **`.shx` (Shapefile Index File):** Contiene un **índice espacial** que acelera las **consultas y la visualización** de las geometrías en el `.shp`. Permite acceder rápidamente a las geometrías necesarias sin tener que leer todo el archivo `.shp`.\n",
        "  - **`.dbf` (dBase Table):** Contiene la **tabla de atributos** asociada a las geometrías. Es una base de datos tabular en formato **dBase**, donde cada fila corresponde a una geometría en el `.shp`, y cada columna representa un **atributo** o característica de esa geometría (ejemplo: nombre de municipio, código de población, etc.). La conexión entre geometrías y atributos se basa en el **orden de las filas**.\n",
        "  - **`.prj` (Projection File):** Define el **sistema de coordenadas de referencia (CRS)** utilizado para las geometrías en el `.shp`. El CRS especifica cómo se proyectan las coordenadas geográficas (latitud y longitud) en un plano cartesiano (mapa plano). Es crucial para asegurar que los mapas se visualicen correctamente y se puedan superponer con otros datos geográficos en el mismo CRS.\n",
        "  - **`.cpg` (Code Page File - Opcional):** Especifica la **codificación de caracteres (encoding)** utilizada para los atributos de texto en el archivo `.dbf`. Asegura que los caracteres de texto (nombres, descripciones) se interpreten correctamente, especialmente si contienen caracteres especiales o acentos. Es **opcional**, y a veces puede faltar en un Shapefile.\n",
        "\n",
        "**La Función `extract_shapefile(...)`: Tu Descompresor de Mapas Automatizado**\n",
        "\n",
        "La función `extract_shapefile(estados_geo, directory, shp_dir, shape_type)` automatiza el proceso de extracción de los componentes esenciales del Shapefile desde archivos ZIP descargados. Veamos sus partes clave:\n",
        "\n",
        "```python\n",
        "def extract_shapefile(estados_geo, directory, shp_dir, shape_type):\n",
        "    # ... (código de la función) ...\n",
        "```\n",
        "\n",
        "- **`estados_geo` (Parámetro de Entrada):** Una **lista de nombres de archivos ZIP** que contienen Shapefiles para diferentes estados. Por ejemplo: `[\"09_ciudaddemexico.zip\", \"01_aguascalientes.zip\"]`.\n",
        "- **`directory` (Parámetro de Entrada):** La **ruta de la carpeta** donde se encuentran los archivos ZIP descargados (ejemplo: `mgc_directory`).\n",
        "- **`shp_dir` (Parámetro de Entrada):** La **ruta de la carpeta de destino** donde se extraerán los archivos Shapefile (ejemplo: `shp_dir`).\n",
        "- **`shape_type` (Parámetro de Entrada):** Una cadena que indica el **tipo de Shapefile** que se va a extraer (ejemplo: `\"m\"` para manzanas, `\"a\"` para AGEBs). Se usa para construir los nombres de archivo dentro del ZIP.\n",
        "- **`os.makedirs(shp_dir, exist_ok=True)`:** Crea el **directorio de destino `shp_dir`** si no existe, usando `os.makedirs(..., exist_ok=True)`.\n",
        "- **`for estado in estados_geo:`:** Itera sobre cada **nombre de archivo ZIP** en la lista `estados_geo`.\n",
        "- **`file = estado.split('_')[0]`:** Extrae el **código de estado** del nombre del archivo ZIP (ejemplo: `\"09\"` de `\"09_ciudaddemexico.zip\"`).\n",
        "- **`zip_file = os.path.join(directory, estado)`:** Construye la **ruta completa al archivo ZIP**.\n",
        "- **`shp_files = [...]`:** Define una **lista de nombres de archivo Shapefile** que se espera encontrar **dentro del archivo ZIP**. Estos nombres se construyen usando el `file` (código de estado) y el `shape_type` para que coincidan con la estructura de los archivos ZIP del INEGI.\n",
        "- **`if all(os.path.exists(os.path.join(shp_dir, f)) for f in shp_files): ... continue`:** Implementa una **verificación de existencia** para los archivos Shapefile. Comprueba si **todos** los archivos listados en `shp_files` ya existen en el directorio de destino `shp_dir`. Si todos existen, imprime un mensaje y usa `continue` para pasar al siguiente estado **sin re-extraer los Shapefiles**.\n",
        "- **`with ZipFile(zip_file, 'r') as zip_ref:`:** Abre el archivo ZIP en **modo lectura (`'r'`)** usando `ZipFile(zip_file, 'r')` y un administrador de contexto (`with`).\n",
        "- **`for f in shp_files:`:** Itera sobre la lista de nombres de archivo Shapefile (`shp_files`) que se van a extraer.\n",
        "- **`try: zip_ref.extract(f, shp_dir)`:** Intenta **extraer el archivo `f`** del archivo ZIP al directorio de destino `shp_dir` usando `zip_ref.extract(f, shp_dir)`. Esto puede generar un error `KeyError` si el archivo no se encuentra dentro del ZIP.\n",
        "- **`except KeyError: ...`:** **Maneja el error `KeyError`**. Si ocurre un `KeyError` (el archivo no se encontró en el ZIP), se ejecuta el bloque `except`:\n",
        "  - **`if f.endswith('.cpg'): ...`:** Comprueba si el archivo que faltaba era un **`.cpg` (archivo de codificación)**. El archivo `.cpg` es **opcional**, por lo que si falta, no es un error fatal. En este caso, se crea un archivo `.cpg` vacío en el directorio de destino y se escribe la codificación `\"ISO-8859-1\"` dentro, que es una codificación común para Shapefiles en México.\n",
        "  - **`else: print(f\"¡ERROR! Archivo Shapefile esencial '{f}' no encontrado en el ZIP.\")`:** Si el archivo que faltaba **no era un `.cpg`**, se imprime un mensaje de **error**, ya que los archivos `.shp`, `.dbf`, `.shx`, `.prj` son **esenciales** para un Shapefile válido.\n",
        "- **`print(f\"Shapefiles extraídos para {file}.\")`:** Imprime un mensaje informando que la extracción de los Shapefiles para el estado actual ha finalizado.\n",
        "\n",
        "**Beneficios de la Extracción Automatizada de Shapefiles:**\n",
        "\n",
        "- **Eficiencia y Precisión:** Automatiza un proceso que podría ser tedioso y propenso a errores si se hiciera manualmente (descomprimir ZIPs, buscar archivos específicos, etc.).\n",
        "- **Consistencia y Repetibilidad:** Asegura que los Shapefiles se extraigan siempre de la misma manera, mejorando la reproducibilidad de tu flujo de trabajo.\n",
        "- **Manejo de Errores Inteligente:** La función maneja el caso (común) de que el archivo `.cpg` sea opcional y crea uno por defecto si falta, evitando errores innecesarios. También informa si faltan archivos Shapefile más importantes.\n",
        "- **Preparación para el Análisis Geoespacial:** Prepara los datos Shapefile para su posterior procesamiento y análisis en GeoPandas y DuckDB."
      ],
      "metadata": {
        "id": "Xb0fm5YHc4UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 2. Función para extraer Shapefile\n",
        "# ================\n",
        "def extract_shapefile(estados_geo, directory, shp_dir, shape_type):\n",
        "    \"\"\"\n",
        "    Extrae shapefiles y componentes (shp,cpg,dbf,prj,shx) de los ZIP.\n",
        "    \"\"\"\n",
        "    os.makedirs(shp_dir, exist_ok=True)\n",
        "    for estado in estados_geo:\n",
        "        file = estado.split('_')[0]\n",
        "        zip_file = os.path.join(directory, estado)\n",
        "        shp_files = [\n",
        "            f'conjunto_de_datos/{file}{shape_type}.shp',\n",
        "            f'conjunto_de_datos/{file}{shape_type}.cpg',\n",
        "            f'conjunto_de_datos/{file}{shape_type}.dbf',\n",
        "            f'conjunto_de_datos/{file}{shape_type}.prj',\n",
        "            f'conjunto_de_datos/{file}{shape_type}.shx'\n",
        "        ]\n",
        "\n",
        "        if all(os.path.exists(os.path.join(shp_dir, f)) for f in shp_files):\n",
        "            print(f\"Todos los archivos para {file} ya están descomprimidos en {shp_dir}\")\n",
        "            continue\n",
        "\n",
        "        with ZipFile(zip_file, 'r') as zip_ref:\n",
        "            for f in shp_files:\n",
        "                try:\n",
        "                    zip_ref.extract(f, shp_dir)\n",
        "                except KeyError:\n",
        "                    if f.endswith('.cpg'):  # cpg opcional\n",
        "                        with open(os.path.join(shp_dir, f), 'w') as out_file:\n",
        "                            out_file.write(\"ISO 88591\")\n",
        "                    else:\n",
        "                        print(f\"El archivo {f} no se encontró en el ZIP.\")\n",
        "\n",
        "        print(f\"Archivos Shapefile extraídos para {file} en {shp_dir}.\\n\")"
      ],
      "metadata": {
        "id": "O-XZMdtw4vo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Variables, Carpetas y Organización de la Información**\n",
        "\n",
        "**Concepto Clave: Definir Variables y Estructura de Carpetas**  \n",
        "En cualquier proyecto de manejo de datos, **la organización del espacio de trabajo** es uno de los primeros pasos críticos. Esto asegura que las descargas, los archivos intermedios y los resultados finales estén ordenados, facilitando la reproducibilidad y la colaboración.\n",
        "\n",
        "1. **Variables de Configuración**\n",
        "  \n",
        "  - Se definen variables como `estados_geo` y `estados_num` para identificar qué archivos se van a descargar. Por ejemplo, en el caso del código, se define:\n",
        "    \n",
        "    ```python\n",
        "    estados_geo = [\"09_ciudaddemexico.zip\"]\n",
        "    estados_num = [9]\n",
        "    ```\n",
        "    \n",
        "    *¿Por qué es importante?*  \n",
        "    Porque al separar la lógica (qué entidades/estados voy a procesar) del propio código, podemos **escalar o modificar** fácilmente la selección de archivos.\n",
        "    \n",
        "2. **Rutas para Almacenar los Datos**\n",
        "  \n",
        "  - Se definen carpetas como `download_directory` y `csv_directory` para almacenar los **archivos CSV** del censo:\n",
        "    \n",
        "    ```python\n",
        "    download_directory = \"./inegi/ccpvagebmza/\"\n",
        "    csv_directory = os.path.join(download_directory, \"csv\")\n",
        "    ```\n",
        "    \n",
        "  - Se hace algo similar para los **shapefiles** (`mgc_directory`). *¿Por qué es fundamental?*\n",
        "    \n",
        "  - Mantener archivos de distintos propósitos (CSV vs. shapefiles) en directorios separados reduce confusiones y facilita encontrar lo que necesitamos.\n",
        "    \n",
        "3. **Limpieza Previa de Carpetas**\n",
        "  \n",
        "  - Antes de continuar, el código verifica si ya existen las carpetas `download_directory` o `mgc_directory` y, de ser así, **las elimina**. Esto garantiza un entorno “limpio” antes de cada corrida y evita el uso de archivos obsoletos.\n",
        "  - Comando clave: `shutil.rmtree(ruta)` para borrar una carpeta con todo su contenido. *¿Por qué hacerlo?*\n",
        "  - Evita inconsistencias entre distintas ejecuciones del script (por ejemplo, archivos incompletos, sobras de descargas anteriores, etc.).\n",
        "  - Asegura un **estado inicial congruente** cada vez que corremos el pipeline.\n",
        "\n",
        "Configurar variables y rutas para cada tipo de dato, así como limpiar los directorios antes de cada ejecución, son pasos esenciales para un flujo de trabajo ordenado, repetible y confiable.  \n",
        "La **disciplina en la organización** ahorra tiempo, facilita colaboraciones y marca la diferencia en proyectos de análisis de datos."
      ],
      "metadata": {
        "id": "xTbXYmUXgi37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 3. Variables y Carpetas\n",
        "# ================\n",
        "estados_geo = [\"09_ciudaddemexico.zip\"]  # Solo CDMX\n",
        "estados_num = [9]  # Solo número 9\n",
        "\n",
        "download_directory = \"./inegi/ccpvagebmza/\"\n",
        "csv_directory = os.path.join(download_directory, \"csv\")\n",
        "\n",
        "# Eliminar carpeta download_directory si existe antes de descargar\n",
        "if os.path.exists(download_directory):\n",
        "    shutil.rmtree(download_directory)\n",
        "\n",
        "os.makedirs(download_directory, exist_ok=True)\n",
        "os.makedirs(csv_directory, exist_ok=True)\n",
        "\n",
        "# Carpeta para shapefiles del marco geoestadístico\n",
        "mgc_directory = \"./inegi/mgccpv/\"\n",
        "# Eliminar carpeta mgc_directory si existe antes de descargar\n",
        "if os.path.exists(mgc_directory):\n",
        "    shutil.rmtree(mgc_directory)\n",
        "\n",
        "os.makedirs(mgc_directory, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "RHz_VMj04ztV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DuckDB + Spatial - Tu Base de Datos Analítica Embebida**\n",
        "\n",
        "\n",
        "**Concepto Clave: Bases de Datos Embebidas para Análisis y Datos Geográficos**\n",
        "\n",
        "1. **¿Qué es DuckDB y por qué es Útil?**\n",
        "  \n",
        "  - **DuckDB** es un sistema de gestión de bases de datos **analítico** y **embebido**:\n",
        "    - **Analítico**: Optimizado para consultas que involucran análisis de grandes volúmenes de datos, agregaciones y joins complejos.\n",
        "    - **Embebido**: No requiere un servidor externo; la base de datos vive en un **archivo** local (similar a SQLite), por lo que su instalación y mantenimiento son sencillos.\n",
        "  - Diseñado para trabajar con datos de manera muy eficiente, soportando **operaciones en memoria** y formato **columnar** interno.\n",
        "2. **Instalando y Cargando el Módulo Spatial**\n",
        "  \n",
        "  - En el código se ve la instrucción:\n",
        "    \n",
        "    ```python\n",
        "    con.execute(\"INSTALL spatial;\")\n",
        "    con.execute(\"LOAD spatial;\")\n",
        "    ```\n",
        "    \n",
        "    Esto activa la **extensión espacial** de DuckDB, que le permite manejar **columnas geométricas** (geom) y realizar operaciones geoespaciales como `ST_Intersects`, `ST_Union`, `ST_Buffer`, etc.\n",
        "    \n",
        "3. **Creación de la Conexión**\n",
        "  \n",
        "  - Se crea una conexión con:\n",
        "    \n",
        "    ```python\n",
        "    db_file = \"./datos_inegi.duckdb\"\n",
        "    con = duckdb.connect(db_file)\n",
        "    ```\n",
        "    \n",
        "    *¿Por qué un archivo `.duckdb`?*\n",
        "    \n",
        "    - Centraliza todos los datos **tabulares y geoespaciales** en un solo archivo.\n",
        "    - Facilita el transporte o la copia del proyecto completo a otro entorno: solo necesitas llevarte el archivo `.duckdb`.\n",
        "    - Permite un acceso ultra rápido a las tablas sin necesidad de instalar un servidor de base de datos.\n",
        "4. **Ventajas de Usar DuckDB**\n",
        "  \n",
        "  - **Velocidad:** Gracias a su arquitectura columnar, las consultas que seleccionan algunas columnas específicas son muy rápidas.\n",
        "  - **Soporte Geoespacial Integrado:** Con el módulo `spatial`, DuckDB se convierte en un **GIS ligero** para consultas SQL.\n",
        "  - **Simplicidad de Instalación:** Basta con `pip install duckdb`, no hay que configurar un servidor.\n",
        "  - **Altamente Portable:** El archivo resultante (por ejemplo, `datos_inegi.duckdb`) puede moverse a cualquier máquina con Python y DuckDB instalado.\n",
        "\n",
        "\n",
        "DuckDB es la pieza central de nuestro pipeline. Nos permite cargar, unir y manipular datos **censales** y **geoespaciales** de forma veloz y con sintaxis **SQL estándar**. Con él, tenemos todo el poder de una base de datos analítica sin la complejidad de un servidor dedicado."
      ],
      "metadata": {
        "id": "xLpZEVAahyBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 4. DuckDB + spatial\n",
        "# ================\n",
        "# Crear la conexión a la base de datos\n",
        "db_file = \"./datos_inegi.duckdb\"  # Nombre del archivo de la base de datos\n",
        "# Si quieres eliminar la base de datos existente, descomenta la siguiente linea\n",
        "# if os.path.exists(db_file):\n",
        "#    os.remove(db_file)\n",
        "con = duckdb.connect(db_file) # Conecta a una base de datos persistente\n",
        "con.execute(\"INSTALL spatial;\")\n",
        "con.execute(\"LOAD spatial;\")"
      ],
      "metadata": {
        "id": "FT41WoxH5Aaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Descarga y Extracción del CSV - Datos Tabulares del Censo**\n",
        "\n",
        "**Concepto Clave: Lectura de Datos Tabulares del INEGI**\n",
        "\n",
        "1. **Archivos CSV (Comma-Separated Values)**\n",
        "  \n",
        "  - El INEGI distribuye gran parte de los datos censales en **archivos CSV** comprimidos en formato ZIP.\n",
        "  - Un archivo CSV contiene **tablas de texto**, donde cada fila suele representar una entidad (por ejemplo, una manzana), y cada columna representa una variable (población total, número de viviendas, edad, escolaridad, etc.).\n",
        "2. **Estructura de Carpeta para los CSV**\n",
        "  \n",
        "  - En el código, la carpeta `csv_directory` almacena los CSV extraídos:\n",
        "    \n",
        "    ```python\n",
        "    csv_directory = os.path.join(download_directory, \"csv\")\n",
        "    ```\n",
        "    \n",
        "  - Se crea la carpeta, y luego se baja el archivo ZIP correspondiente a la entidad deseada (ej. `09` para CDMX), con nuestro confiable método `download(url, directory)`.\n",
        "    \n",
        "3. **Extracción del ZIP y Verificación**\n",
        "  \n",
        "  - Una vez descargado el ZIP, se utiliza:\n",
        "    \n",
        "    ```python\n",
        "    with ZipFile(zip_file_path, 'r') as z:\n",
        "       z.extract(target_csv, os.path.join(csv_directory))\n",
        "    ```\n",
        "    \n",
        "  - Esto **descomprime** el CSV dentro de la carpeta `csv_directory`, quedando listo para ser procesado.\n",
        "    \n",
        "  - *¿Por qué controlar manualmente la extracción?*\n",
        "    \n",
        "    - Para asegurarnos de **exactamente** qué archivos se extraen y **dónde** se ubican. Así evitamos problemas con estructuras de carpetas complejas dentro de los ZIPs del INEGI.\n",
        "4. **Comprobaciones Antes de Re-Descargar**\n",
        "  \n",
        "  - El script revisa si el CSV ya existe para **no duplicar descargas** ni extraer archivos repetidos. Este control ahorra ancho de banda y tiempo de ejecución.\n",
        "\n",
        "La descarga y extracción de los archivos CSV es un **paso muy importante** para obtener la información censal. Tener estos datos en formato tabular (CSV) nos permite cargarlos posteriormente a DuckDB y combinarlos con la información espacial."
      ],
      "metadata": {
        "id": "__BmekV7iil5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 5. Descargar CSV\n",
        "# ================\n",
        "\n",
        "i = 9  # Índice único\n",
        "estado_str = \"09\"\n",
        "zip_file_path = os.path.join(download_directory, f\"ageb_mza_urbana_{estado_str}_cpv2020_csv.zip\")\n",
        "csv_file_path = os.path.join(csv_directory, \"conjunto_de_datos\",f\"conjunto_de_datos_ageb_urbana_{estado_str}_cpv2020.csv\")\n",
        "\n",
        "os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
        "\n",
        "if not os.path.exists(csv_file_path):\n",
        "    # Descarga\n",
        "    url = (f\"https://www.inegi.org.mx/contenidos/programas/ccpv/2020/datosabiertos/\"\n",
        "            f\"ageb_manzana/ageb_mza_urbana_{estado_str}_cpv2020_csv.zip\")\n",
        "    if not os.path.exists(zip_file_path):\n",
        "        download(url, download_directory)\n",
        "    # Extraer CSV\n",
        "    with ZipFile(zip_file_path, 'r') as z:\n",
        "        folder_name = f\"ageb_mza_urbana_{estado_str}_cpv2020\"\n",
        "        target_csv = os.path.join(folder_name, \"conjunto_de_datos\",\n",
        "                                  f\"conjunto_de_datos_ageb_urbana_{estado_str}_cpv2020.csv\")\n",
        "        z.extract(target_csv, os.path.join(csv_directory))\n",
        "else:\n",
        "    print(f\"CSV de {estado_str} ya existe, no se descarga.\")"
      ],
      "metadata": {
        "id": "_Q_01pul54wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url"
      ],
      "metadata": {
        "id": "TESAipA-AMD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creando la Tabla `censo_all` en DuckDB - Consolidación de Datos Censales**\n",
        "\n",
        "**Concepto Clave: Almacenar Datos en una Estructura Eficiente para Análisis**\n",
        "\n",
        "1. **Creación de la Tabla `censo_all`**\n",
        "  \n",
        "  - Tras descargar el CSV, se realiza:\n",
        "    \n",
        "    ```sql\n",
        "    CREATE TABLE censo_all (... columnas ...)\n",
        "    ```\n",
        "    \n",
        "    Definiendo **todas las columnas** con su tipo de dato (VARCHAR, DOUBLE, etc.).\n",
        "    \n",
        "  - *¿Por qué crear la tabla con una instrucción `CREATE TABLE ...` en lugar de un simple `CREATE TABLE AS SELECT ...`?*\n",
        "    \n",
        "    - Para **definir con precisión** los nombres y tipos de columnas (ej. `POBTOT DOUBLE`), evitando inconsistencias de tipo o problemas de parsing.\n",
        "    - Así nos aseguramos que la tabla final tenga la **estructura correcta** para el análisis.\n",
        "2. **Insertar Datos desde el CSV**\n",
        "  \n",
        "  - El paso crítico es:\n",
        "    \n",
        "    ```sql\n",
        "    INSERT INTO censo_all\n",
        "    SELECT [...]\n",
        "    FROM read_csv_auto('{csv_file_path}', ...)\n",
        "    WHERE MZA != '0'\n",
        "    ```\n",
        "    \n",
        "    *¿Qué sucede aquí?*\n",
        "    \n",
        "    - **`read_csv_auto(...)`**: Función de DuckDB que lee automáticamente el CSV y **deduce** los tipos de columna.\n",
        "    - **`INSERT INTO censo_all SELECT ...`**: Selecciona las columnas deseadas del CSV y las **inserta** en la tabla `censo_all`.\n",
        "    - **`WHERE MZA != '0'`**: Ejemplo de filtro para excluir manzanas con valor ‘0’ si así lo requerimos.\n",
        "3. **Transacciones y Seguridad**\n",
        "  \n",
        "  - El código hace:\n",
        "    \n",
        "    ```python\n",
        "    con.execute(\"BEGIN TRANSACTION;\")\n",
        "    ...\n",
        "    con.execute(\"COMMIT;\")\n",
        "    ```\n",
        "    \n",
        "    Esto agrupa las operaciones en una **transacción**. Si algo falla en medio, se puede hacer un **ROLLBACK** y no se queda la tabla incompleta. Es buena práctica para mantener la base de datos en un **estado coherente**.\n",
        "    \n",
        "4. **¿Por qué Llevarnos el CSV a una Tabla SQL?**\n",
        "  \n",
        "  - **Consultas Veloces y Flexibles**: Una vez dentro de DuckDB, podemos **filtrar, agrupar, unir** y transformar datos con lenguaje **SQL** estándar.\n",
        "  - **Integración con Otros Datos**: Podemos unir sin problemas la información demográfica con otras tablas que tengamos en la misma base.\n",
        "  - **Menor Uso de Memoria**: DuckDB administra la lectura de datos de forma optimizada. No necesitamos cargar todo el CSV a memoria si no lo deseamos.\n",
        "\n",
        "\n",
        "Transformar el CSV en una tabla `censo_all` dentro de DuckDB nos brinda la **flexibilidad y potencia** del lenguaje SQL para explorar y analizar los indicadores censales. Este paso **convierte** nuestros datos crudos en una base analítica robusta y lista para uniones con los mapas."
      ],
      "metadata": {
        "id": "vY5v44YYja-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 6. Leer CSV con DuckDB y crear una sola tabla\n",
        "# ================\n",
        "# Vamos a crear una tabla \"censo_all\" consolidada en DuckDB\n",
        "con.execute(\"DROP TABLE IF EXISTS censo_all;\")\n",
        "con.execute(\"\"\"\n",
        "    CREATE TABLE censo_all (\n",
        "        ENTIDAD VARCHAR,\n",
        "        NOM_ENT VARCHAR,\n",
        "        MUN VARCHAR,\n",
        "        NOM_MUN VARCHAR,\n",
        "        LOC VARCHAR,\n",
        "        NOM_LOC VARCHAR,\n",
        "        AGEB VARCHAR,\n",
        "        MZA VARCHAR,\n",
        "        POBTOT DOUBLE,\n",
        "        POBFEM DOUBLE,\n",
        "        POBMAS DOUBLE,\n",
        "        P_0A2 DOUBLE,\n",
        "        P_0A2_F DOUBLE,\n",
        "        P_0A2_M DOUBLE,\n",
        "        P_3YMAS DOUBLE,\n",
        "        P_3YMAS_F DOUBLE,\n",
        "        P_3YMAS_M DOUBLE,\n",
        "        P_5YMAS DOUBLE,\n",
        "        P_5YMAS_F DOUBLE,\n",
        "        P_5YMAS_M DOUBLE,\n",
        "        P_12YMAS DOUBLE,\n",
        "        P_12YMAS_F DOUBLE,\n",
        "        P_12YMAS_M DOUBLE,\n",
        "        P_15YMAS DOUBLE,\n",
        "        P_15YMAS_F DOUBLE,\n",
        "        P_15YMAS_M DOUBLE,\n",
        "        P_18YMAS DOUBLE,\n",
        "        P_18YMAS_F DOUBLE,\n",
        "        P_18YMAS_M DOUBLE,\n",
        "        P_3A5 DOUBLE,\n",
        "        P_3A5_F DOUBLE,\n",
        "        P_3A5_M DOUBLE,\n",
        "        P_6A11 DOUBLE,\n",
        "        P_6A11_F DOUBLE,\n",
        "        P_6A11_M DOUBLE,\n",
        "        P_8A14 DOUBLE,\n",
        "        P_8A14_F DOUBLE,\n",
        "        P_8A14_M DOUBLE,\n",
        "        P_12A14 DOUBLE,\n",
        "        P_12A14_F DOUBLE,\n",
        "        P_12A14_M DOUBLE,\n",
        "        P_15A17 DOUBLE,\n",
        "        P_15A17_F DOUBLE,\n",
        "        P_15A17_M DOUBLE,\n",
        "        P_18A24 DOUBLE,\n",
        "        P_18A24_F DOUBLE,\n",
        "        P_18A24_M DOUBLE,\n",
        "        P_15A49_F DOUBLE,\n",
        "        P_60YMAS DOUBLE,\n",
        "        P_60YMAS_F DOUBLE,\n",
        "        P_60YMAS_M DOUBLE,\n",
        "        REL_H_M DOUBLE,\n",
        "        POB0_14 DOUBLE,\n",
        "        POB15_64 DOUBLE,\n",
        "        POB65_MAS DOUBLE,\n",
        "        PROM_HNV DOUBLE,\n",
        "        PNACENT DOUBLE,\n",
        "        PNACENT_F DOUBLE,\n",
        "        PNACENT_M DOUBLE,\n",
        "        PNACOE DOUBLE,\n",
        "        PNACOE_F DOUBLE,\n",
        "        PNACOE_M DOUBLE,\n",
        "        PRES2015 DOUBLE,\n",
        "        PRES2015_F DOUBLE,\n",
        "        PRES2015_M DOUBLE,\n",
        "        PRESOE15 DOUBLE,\n",
        "        PRESOE15_F DOUBLE,\n",
        "        PRESOE15_M DOUBLE,\n",
        "        P3YM_HLI DOUBLE,\n",
        "        P3YM_HLI_F DOUBLE,\n",
        "        P3YM_HLI_M DOUBLE,\n",
        "        P3HLINHE DOUBLE,\n",
        "        P3HLINHE_F DOUBLE,\n",
        "        P3HLINHE_M DOUBLE,\n",
        "        P3HLI_HE DOUBLE,\n",
        "        P3HLI_HE_F DOUBLE,\n",
        "        P3HLI_HE_M DOUBLE,\n",
        "        P5_HLI DOUBLE,\n",
        "        P5_HLI_NHE DOUBLE,\n",
        "        P5_HLI_HE DOUBLE,\n",
        "        PHOG_IND DOUBLE,\n",
        "        POB_AFRO DOUBLE,\n",
        "        POB_AFRO_F DOUBLE,\n",
        "        POB_AFRO_M DOUBLE,\n",
        "        PCON_DISC DOUBLE,\n",
        "        PCDISC_MOT DOUBLE,\n",
        "        PCDISC_VIS DOUBLE,\n",
        "        PCDISC_LENG DOUBLE,\n",
        "        PCDISC_AUD DOUBLE,\n",
        "        PCDISC_MOT2 DOUBLE,\n",
        "        PCDISC_MEN DOUBLE,\n",
        "        PCON_LIMI DOUBLE,\n",
        "        PCLIM_CSB DOUBLE,\n",
        "        PCLIM_VIS DOUBLE,\n",
        "        PCLIM_HACO DOUBLE,\n",
        "        PCLIM_OAUD DOUBLE,\n",
        "        PCLIM_MOT2 DOUBLE,\n",
        "        PCLIM_RE_CO DOUBLE,\n",
        "        PCLIM_PMEN DOUBLE,\n",
        "        PSIND_LIM DOUBLE,\n",
        "        P3A5_NOA DOUBLE,\n",
        "        P3A5_NOA_F DOUBLE,\n",
        "        P3A5_NOA_M DOUBLE,\n",
        "        P6A11_NOA DOUBLE,\n",
        "        P6A11_NOAF DOUBLE,\n",
        "        P6A11_NOAM DOUBLE,\n",
        "        P12A14NOA DOUBLE,\n",
        "        P12A14NOAF DOUBLE,\n",
        "        P12A14NOAM DOUBLE,\n",
        "        P15A17A DOUBLE,\n",
        "        P15A17A_F DOUBLE,\n",
        "        P15A17A_M DOUBLE,\n",
        "        P18A24A DOUBLE,\n",
        "        P18A24A_F DOUBLE,\n",
        "        P18A24A_M DOUBLE,\n",
        "        P8A14AN DOUBLE,\n",
        "        P8A14AN_F DOUBLE,\n",
        "        P8A14AN_M DOUBLE,\n",
        "        P15YM_AN DOUBLE,\n",
        "        P15YM_AN_F DOUBLE,\n",
        "        P15YM_AN_M DOUBLE,\n",
        "        P15YM_SE DOUBLE,\n",
        "        P15YM_SE_F DOUBLE,\n",
        "        P15YM_SE_M DOUBLE,\n",
        "        P15PRI_IN DOUBLE,\n",
        "        P15PRI_INF DOUBLE,\n",
        "        P15PRI_INM DOUBLE,\n",
        "        P15PRI_CO DOUBLE,\n",
        "        P15PRI_COF DOUBLE,\n",
        "        P15PRI_COM DOUBLE,\n",
        "        P15SEC_IN DOUBLE,\n",
        "        P15SEC_INF DOUBLE,\n",
        "        P15SEC_INM DOUBLE,\n",
        "        P15SEC_CO DOUBLE,\n",
        "        P15SEC_COF DOUBLE,\n",
        "        P15SEC_COM DOUBLE,\n",
        "        P18YM_PB DOUBLE,\n",
        "        P18YM_PB_F DOUBLE,\n",
        "        P18YM_PB_M DOUBLE,\n",
        "        GRAPROES DOUBLE,\n",
        "        GRAPROES_F DOUBLE,\n",
        "        GRAPROES_M DOUBLE,\n",
        "        PEA DOUBLE,\n",
        "        PEA_F DOUBLE,\n",
        "        PEA_M DOUBLE,\n",
        "        PE_INAC DOUBLE,\n",
        "        PE_INAC_F DOUBLE,\n",
        "        PE_INAC_M DOUBLE,\n",
        "        POCUPADA DOUBLE,\n",
        "        POCUPADA_F DOUBLE,\n",
        "        POCUPADA_M DOUBLE,\n",
        "        PDESOCUP DOUBLE,\n",
        "        PDESOCUP_F DOUBLE,\n",
        "        PDESOCUP_M DOUBLE,\n",
        "        PSINDER DOUBLE,\n",
        "        PDER_SS DOUBLE,\n",
        "        PDER_IMSS DOUBLE,\n",
        "        PDER_ISTE DOUBLE,\n",
        "        PDER_ISTEE DOUBLE,\n",
        "        PAFIL_PDOM DOUBLE,\n",
        "        PDER_SEGP DOUBLE,\n",
        "        PDER_IMSSB DOUBLE,\n",
        "        PAFIL_IPRIV DOUBLE,\n",
        "        PAFIL_OTRAI DOUBLE,\n",
        "        P12YM_SOLT DOUBLE,\n",
        "        P12YM_CASA DOUBLE,\n",
        "        P12YM_SEPA DOUBLE,\n",
        "        PCATOLICA DOUBLE,\n",
        "        PRO_CRIEVA DOUBLE,\n",
        "        POTRAS_REL DOUBLE,\n",
        "        PSIN_RELIG DOUBLE,\n",
        "        TOTHOG DOUBLE,\n",
        "        HOGJEF_F DOUBLE,\n",
        "        HOGJEF_M DOUBLE,\n",
        "        POBHOG DOUBLE,\n",
        "        PHOGJEF_F DOUBLE,\n",
        "        PHOGJEF_M DOUBLE,\n",
        "        VIVTOT DOUBLE,\n",
        "        TVIVHAB DOUBLE,\n",
        "        TVIVPAR DOUBLE,\n",
        "        VIVPAR_HAB DOUBLE,\n",
        "        VIVPARH_CV DOUBLE,\n",
        "        TVIVPARHAB DOUBLE,\n",
        "        VIVPAR_DES DOUBLE,\n",
        "        VIVPAR_UT DOUBLE,\n",
        "        OCUPVIVPAR DOUBLE,\n",
        "        PROM_OCUP DOUBLE,\n",
        "        PRO_OCUP_C DOUBLE,\n",
        "        VPH_PISODT DOUBLE,\n",
        "        VPH_PISOTI DOUBLE,\n",
        "        VPH_1DOR DOUBLE,\n",
        "        VPH_2YMASD DOUBLE,\n",
        "        VPH_1CUART DOUBLE,\n",
        "        VPH_2CUART DOUBLE,\n",
        "        VPH_3YMASC DOUBLE,\n",
        "        VPH_C_ELEC DOUBLE,\n",
        "        VPH_S_ELEC DOUBLE,\n",
        "        VPH_AGUADV DOUBLE,\n",
        "        VPH_AEASP DOUBLE,\n",
        "        VPH_AGUAFV DOUBLE,\n",
        "        VPH_TINACO DOUBLE,\n",
        "        VPH_CISTER DOUBLE,\n",
        "        VPH_EXCSA DOUBLE,\n",
        "        VPH_LETR DOUBLE,\n",
        "        VPH_DRENAJ DOUBLE,\n",
        "        VPH_NODREN DOUBLE,\n",
        "        VPH_C_SERV DOUBLE,\n",
        "        VPH_NDEAED DOUBLE,\n",
        "        VPH_DSADMA DOUBLE,\n",
        "        VPH_NDACMM DOUBLE,\n",
        "        VPH_SNBIEN DOUBLE,\n",
        "        VPH_REFRI DOUBLE,\n",
        "        VPH_LAVAD DOUBLE,\n",
        "        VPH_HMICRO DOUBLE,\n",
        "        VPH_AUTOM DOUBLE,\n",
        "        VPH_MOTO DOUBLE,\n",
        "        VPH_BICI DOUBLE,\n",
        "        VPH_RADIO DOUBLE,\n",
        "        VPH_TV DOUBLE,\n",
        "        VPH_PC DOUBLE,\n",
        "        VPH_TELEF DOUBLE,\n",
        "        VPH_CEL DOUBLE,\n",
        "        VPH_INTER DOUBLE,\n",
        "        VPH_STVP DOUBLE,\n",
        "        VPH_SPMVPI DOUBLE,\n",
        "        VPH_CVJ DOUBLE,\n",
        "        VPH_SINRTV DOUBLE,\n",
        "        VPH_SINLTC DOUBLE,\n",
        "        VPH_SINCINT DOUBLE,\n",
        "        VPH_SINTIC DOUBLE\n",
        "    );\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "estado_str = \"09\"\n",
        "csv_file_path = os.path.join(csv_directory, f\"ageb_mza_urbana_{estado_str}_cpv2020\",\n",
        "                              \"conjunto_de_datos\",f\"conjunto_de_datos_ageb_urbana_{estado_str}_cpv2020.csv\")\n",
        "\n",
        "    # --- Intento de inserción con el archivo original ---\n",
        "con.execute(\"BEGIN TRANSACTION;\")\n",
        "\n",
        "# Insertar en censo_all\n",
        "con.execute(f\"\"\"\n",
        "    INSERT INTO censo_all\n",
        "    SELECT\n",
        "        ENTIDAD,\n",
        "        NOM_ENT,\n",
        "        MUN,\n",
        "        NOM_MUN,\n",
        "        LOC,\n",
        "        NOM_LOC,\n",
        "        AGEB,\n",
        "        MZA,\n",
        "        POBTOT,\n",
        "        POBFEM,\n",
        "        POBMAS,\n",
        "        P_0A2,\n",
        "        P_0A2_F,\n",
        "        P_0A2_M,\n",
        "        P_3YMAS,\n",
        "        P_3YMAS_F,\n",
        "        P_3YMAS_M,\n",
        "        P_5YMAS,\n",
        "        P_5YMAS_F,\n",
        "        P_5YMAS_M,\n",
        "        P_12YMAS,\n",
        "        P_12YMAS_F,\n",
        "        P_12YMAS_M,\n",
        "        P_15YMAS,\n",
        "        P_15YMAS_F,\n",
        "        P_15YMAS_M,\n",
        "        P_18YMAS,\n",
        "        P_18YMAS_F,\n",
        "        P_18YMAS_M,\n",
        "        P_3A5,\n",
        "        P_3A5_F,\n",
        "        P_3A5_M,\n",
        "        P_6A11,\n",
        "        P_6A11_F,\n",
        "        P_6A11_M,\n",
        "        P_8A14,\n",
        "        P_8A14_F,\n",
        "        P_8A14_M,\n",
        "        P_12A14,\n",
        "        P_12A14_F,\n",
        "        P_12A14_M,\n",
        "        P_15A17,\n",
        "        P_15A17_F,\n",
        "        P_15A17_M,\n",
        "        P_18A24,\n",
        "        P_18A24_F,\n",
        "        P_18A24_M,\n",
        "        P_15A49_F,\n",
        "        P_60YMAS,\n",
        "        P_60YMAS_F,\n",
        "        P_60YMAS_M,\n",
        "        REL_H_M,\n",
        "        POB0_14,\n",
        "        POB15_64,\n",
        "        POB65_MAS,\n",
        "        PROM_HNV,\n",
        "        PNACENT,\n",
        "        PNACENT_F,\n",
        "        PNACENT_M,\n",
        "        PNACOE,\n",
        "        PNACOE_F,\n",
        "        PNACOE_M,\n",
        "        PRES2015,\n",
        "        PRES2015_F,\n",
        "        PRES2015_M,\n",
        "        PRESOE15,\n",
        "        PRESOE15_F,\n",
        "        PRESOE15_M,\n",
        "        P3YM_HLI,\n",
        "        P3YM_HLI_F,\n",
        "        P3YM_HLI_M,\n",
        "        P3HLINHE,\n",
        "        P3HLINHE_F,\n",
        "        P3HLINHE_M,\n",
        "        P3HLI_HE,\n",
        "        P3HLI_HE_F,\n",
        "        P3HLI_HE_M,\n",
        "        P5_HLI,\n",
        "        P5_HLI_NHE,\n",
        "        P5_HLI_HE,\n",
        "        PHOG_IND,\n",
        "        POB_AFRO,\n",
        "        POB_AFRO_F,\n",
        "        POB_AFRO_M,\n",
        "        PCON_DISC,\n",
        "        PCDISC_MOT,\n",
        "        PCDISC_VIS,\n",
        "        PCDISC_LENG,\n",
        "        PCDISC_AUD,\n",
        "        PCDISC_MOT2,\n",
        "        PCDISC_MEN,\n",
        "        PCON_LIMI,\n",
        "        PCLIM_CSB,\n",
        "        PCLIM_VIS,\n",
        "        PCLIM_HACO,\n",
        "        PCLIM_OAUD,\n",
        "        PCLIM_MOT2,\n",
        "        PCLIM_RE_CO,\n",
        "        PCLIM_PMEN,\n",
        "        PSIND_LIM,\n",
        "        P3A5_NOA,\n",
        "        P3A5_NOA_F,\n",
        "        P3A5_NOA_M,\n",
        "        P6A11_NOA,\n",
        "        P6A11_NOAF,\n",
        "        P6A11_NOAM,\n",
        "        P12A14NOA,\n",
        "        P12A14NOAF,\n",
        "        P12A14NOAM,\n",
        "        P15A17A,\n",
        "        P15A17A_F,\n",
        "        P15A17A_M,\n",
        "        P18A24A,\n",
        "        P18A24A_F,\n",
        "        P18A24A_M,\n",
        "        P8A14AN,\n",
        "        P8A14AN_F,\n",
        "        P8A14AN_M,\n",
        "        P15YM_AN,\n",
        "        P15YM_AN_F,\n",
        "        P15YM_AN_M,\n",
        "        P15YM_SE,\n",
        "        P15YM_SE_F,\n",
        "        P15YM_SE_M,\n",
        "        P15PRI_IN,\n",
        "        P15PRI_INF,\n",
        "        P15PRI_INM,\n",
        "        P15PRI_CO,\n",
        "        P15PRI_COF,\n",
        "        P15PRI_COM,\n",
        "        P15SEC_IN,\n",
        "        P15SEC_INF,\n",
        "        P15SEC_INM,\n",
        "        P15SEC_CO,\n",
        "        P15SEC_COF,\n",
        "        P15SEC_COM,\n",
        "        P18YM_PB,\n",
        "        P18YM_PB_F,\n",
        "        P18YM_PB_M,\n",
        "        GRAPROES,\n",
        "        GRAPROES_F,\n",
        "        GRAPROES_M,\n",
        "        PEA,\n",
        "        PEA_F,\n",
        "        PEA_M,\n",
        "        PE_INAC,\n",
        "        PE_INAC_F,\n",
        "        PE_INAC_M,\n",
        "        POCUPADA,\n",
        "        POCUPADA_F,\n",
        "        POCUPADA_M,\n",
        "        PDESOCUP,\n",
        "        PDESOCUP_F,\n",
        "        PDESOCUP_M,\n",
        "        PSINDER,\n",
        "        PDER_SS,\n",
        "        PDER_IMSS,\n",
        "        PDER_ISTE,\n",
        "        PDER_ISTEE,\n",
        "        PAFIL_PDOM,\n",
        "        PDER_SEGP,\n",
        "        PDER_IMSSB,\n",
        "        PAFIL_IPRIV,\n",
        "        PAFIL_OTRAI,\n",
        "        P12YM_SOLT,\n",
        "        P12YM_CASA,\n",
        "        P12YM_SEPA,\n",
        "        PCATOLICA,\n",
        "        PRO_CRIEVA,\n",
        "        POTRAS_REL,\n",
        "        PSIN_RELIG,\n",
        "        TOTHOG,\n",
        "        HOGJEF_F,\n",
        "        HOGJEF_M,\n",
        "        POBHOG,\n",
        "        PHOGJEF_F,\n",
        "        PHOGJEF_M,\n",
        "        VIVTOT,\n",
        "        TVIVHAB,\n",
        "        TVIVPAR,\n",
        "        VIVPAR_HAB,\n",
        "        VIVPARH_CV,\n",
        "        TVIVPARHAB,\n",
        "        VIVPAR_DES,\n",
        "        VIVPAR_UT,\n",
        "        OCUPVIVPAR,\n",
        "        PROM_OCUP,\n",
        "        PRO_OCUP_C,\n",
        "        VPH_PISODT,\n",
        "        VPH_PISOTI,\n",
        "        VPH_1DOR,\n",
        "        VPH_2YMASD,\n",
        "        VPH_1CUART,\n",
        "        VPH_2CUART,\n",
        "        VPH_3YMASC,\n",
        "        VPH_C_ELEC,\n",
        "        VPH_S_ELEC,\n",
        "        VPH_AGUADV,\n",
        "        VPH_AEASP,\n",
        "        VPH_AGUAFV,\n",
        "        VPH_TINACO,\n",
        "        VPH_CISTER,\n",
        "        VPH_EXCSA,\n",
        "        VPH_LETR,\n",
        "        VPH_DRENAJ,\n",
        "        VPH_NODREN,\n",
        "        VPH_C_SERV,\n",
        "        VPH_NDEAED,\n",
        "        VPH_DSADMA,\n",
        "        VPH_NDACMM,\n",
        "        VPH_SNBIEN,\n",
        "        VPH_REFRI,\n",
        "        VPH_LAVAD,\n",
        "        VPH_HMICRO,\n",
        "        VPH_AUTOM,\n",
        "        VPH_MOTO,\n",
        "        VPH_BICI,\n",
        "        VPH_RADIO,\n",
        "        VPH_TV,\n",
        "        VPH_PC,\n",
        "        VPH_TELEF,\n",
        "        VPH_CEL,\n",
        "        VPH_INTER,\n",
        "        VPH_STVP,\n",
        "        VPH_SPMVPI,\n",
        "        VPH_CVJ,\n",
        "        VPH_SINRTV,\n",
        "        VPH_SINLTC,\n",
        "        VPH_SINCINT,\n",
        "        VPH_SINTIC\n",
        "    FROM read_csv_auto('{csv_file_path}',\n",
        "    NULLSTR=['N/A','N/D','*'], SAMPLE_SIZE=-1)\n",
        "    WHERE MZA != '0'\n",
        "\"\"\")\n",
        "con.execute(\"COMMIT;\")\n",
        "print(f\"  Datos de '{csv_file_path}' insertados en la tabla 'censo_all'.\")"
      ],
      "metadata": {
        "id": "BrAnTJh86bGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Segunda Descarga y Extracción del Shapefile - Manzanas de la CDMX**\n",
        "\n",
        "**Concepto Clave: Adición de Información Geográfica Específica**\n",
        "\n",
        "1. **Descarga Focalizada**\n",
        "  \n",
        "  - El script solo descarga el ZIP de la **Ciudad de México (`09_ciudaddemexico.zip`)**. Por ejemplo:\n",
        "    \n",
        "    ```python\n",
        "    download(url_mgccpv + \"09_ciudaddemexico.zip\", mgc_directory)\n",
        "    ```\n",
        "    \n",
        "  - Se ubican estos archivos en la carpeta `mgc_directory`, que concentra el “Marco Geoestadístico” para la entidad.\n",
        "    \n",
        "2. **Extracción del Shapefile de Manzanas**\n",
        "  \n",
        "  - Se usa la función `extract_shapefile`, con `shape_type = \"m\"` (m de “manzana”).\n",
        "  - *¿Por qué la distinción con `shape_type`?*\n",
        "    - El INEGI provee diversos niveles de granularidad geográfica (AGEBs, localidades, municipios). Cada uno tiene un sufijo diferente (`\"a\"`, `\"l\"`, `\"m\"`, etc.).\n",
        "    - Usar un parámetro evita duplicar lógica y hace fácil cambiar la extracción de manzanas a AGEBs, si se requiere.\n",
        "3. **Archivos Esenciales**\n",
        "  \n",
        "  - Se verifica la existencia de los archivos `.shp, .dbf, .shx, .prj` y se crea uno `.cpg` si no existe.\n",
        "  - Esto es **vital** para conservar la integridad de los datos geoespaciales: sin `.prj` no sabríamos el sistema de coordenadas, y sin `.dbf` no hay atributos para cada polígono de manzana.\n",
        "4. **Importancia de las Manzanas**\n",
        "  \n",
        "  - La manzana es la **unidad geográfica más pequeña** de desagregación que el INEGI ofrece en un censo urbano.\n",
        "  - Nos permite analizar con gran **detalle espacial** (por ejemplo, población por cuadra) para estudios de planeación urbana, movilidad, accesibilidad a servicios, etc.\n",
        "\n",
        "\n",
        "La descarga y extracción del Shapefile de manzanas de la CDMX nos acerca a la **unidad básica de estudio geoespacial**. Con esto, tendremos la geometría (los polígonos) de cada manzana para relacionarlos con los datos censales que ya están en DuckDB."
      ],
      "metadata": {
        "id": "0VoiKmA6kPdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 7. Descargar y Extraer Shapefiles\n",
        "# ================\n",
        "# Descargar solo el shapefile de CDMX\n",
        "url_mgccpv = \"https://www.inegi.org.mx/contenidos/productos/prod_serv/contenidos/espanol/bvinegi/productos/geografia/marcogeo/889463807469/\"\n",
        "download(url_mgccpv + \"09_ciudaddemexico.zip\", mgc_directory)\n",
        "\n",
        "# Extraer solo CDMX\n",
        "# Manzanas\n",
        "shape_type = \"m\"\n",
        "directory = mgc_directory\n",
        "shp_dir = os.path.join(directory, \"shp\", shape_type)\n",
        "extract_shapefile([\"09_ciudaddemexico.zip\"], directory, shp_dir, shape_type)"
      ],
      "metadata": {
        "id": "AFe5u__1-HYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conversión a GeoParquet - Eficiencia y Modernidad en Datos Geoespaciales**\n",
        "\n",
        "**Concepto Clave: GeoParquet, el Formato Columna para Geometrías**\n",
        "\n",
        "1. **¿Qué es Parquet y GeoParquet?**\n",
        "  \n",
        "  - **Parquet** es un formato **columna** (columnar) diseñado para el **almacenamiento y la consulta eficientes** de datos. Fue impulsado por Apache Hadoop/Arrow, permitiendo comprimir y dividir datos por columnas para acelerar las lecturas selectivas.\n",
        "  - **GeoParquet** es Parquet **adaptado** a datos geoespaciales. Incluye metadatos para la columna de geometría, de manera que se reconozca su CRS (Sistema de Referencia de Coordenadas) y se almacenen las geometrías de forma compacta.\n",
        "2. **Por qué Pasar de Shapefile a GeoParquet**\n",
        "  \n",
        "  - **Rendimiento:** Los Shapefiles son más viejos y están fragmentados en varios archivos, mientras que un solo archivo GeoParquet es **mucho más rápido** al leer y consultar.\n",
        "  - **Tamaño Reducido:** Parquet puede comprimir datos de manera más efectiva, ocupando menos espacio en disco.\n",
        "  - **Compatibilidad Creciente:** Herramientas modernas (incluido DuckDB) leen Parquet de forma nativa y muy veloz.\n",
        "3. **Proceso de Conversión con GeoPandas**\n",
        "  \n",
        "  - El código:\n",
        "    \n",
        "    ```python\n",
        "    gdf = gpd.read_file(shp_path, encoding='ISO-8859-1')\n",
        "    gdf = gdf.to_crs(\"EPSG:4326\")\n",
        "    gdf.to_parquet(parquet_path, index=False)\n",
        "    ```\n",
        "    \n",
        "  - *¿Qué sucede?*\n",
        "    \n",
        "    1. **`gpd.read_file(...)`**: Lee el Shapefile y crea un `GeoDataFrame` (tabla geoespacial).\n",
        "    2. **`to_crs(\"EPSG:4326\")`**: Reproyecta las geometrías a **WGS 84** (coordenadas lat/long), un estándar mundial.\n",
        "    3. **`to_parquet(...)`**: Guarda el `GeoDataFrame` como un archivo `.parquet` con metadatos geoespaciales, **creando el GeoParquet**.\n",
        "4. **Verificación de CRS**\n",
        "  \n",
        "  - A menudo, los datos del INEGI vienen en proyecciones no estándar o CRS locales. Es crucial **verificar** si `gdf.crs` está definido y, si no, asignarlo manualmente.\n",
        "  - Reproyectar a un CRS **común** (EPSG:4326) facilita la combinación con otras fuentes de datos geográficos mundiales.\n",
        "\n",
        "\n",
        "El **GeoParquet** es el formato moderno que combina la eficacia de Parquet con la información geoespacial. Convertir Shapefiles a GeoParquet **optimiza** enormemente la lectura y el análisis, integrando sin problemas con herramientas de big data y flujos de trabajo analíticos contemporáneos."
      ],
      "metadata": {
        "id": "QiYk3mNKlrA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 7.5 Convertir a GeoParquet\n",
        "# ================\n",
        "# Procesar solo CDMX\n",
        "file = \"09\"\n",
        "shp_path = os.path.join(shp_dir, \"conjunto_de_datos\", f\"{file}m.shp\")\n",
        "parquet_path = os.path.join(shp_dir, \"conjunto_de_datos\", \"09m.geoparquet\")\n",
        "if os.path.exists(parquet_path):\n",
        "    print(f\"GeoParquet de CDMX ya existe.\")\n",
        "else:\n",
        "  print(f\"Convirtiendo {shp_path} a GeoParquet...\")\n",
        "  try:\n",
        "      # Leer el Shapefile sin el parámetro 'crs'\n",
        "      gdf = gpd.read_file(shp_path, encoding='ISO-8859-1')\n",
        "\n",
        "      # Asignar CRS original si es necesario\n",
        "      if gdf.crs is None:\n",
        "          gdf.set_crs(\"EPSG:6372\", inplace=True)\n",
        "\n",
        "      # Transformar a EPSG:4326 (WGS 84), CRS común para datos geoespaciales\n",
        "      gdf = gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "      # Guardar como GeoParquet\n",
        "      gdf.to_parquet(parquet_path, index=False)\n",
        "      print(f\"  GeoParquet guardado en: {parquet_path}\")\n",
        "  except Exception as e:\n",
        "      print(f\"  Error al convertir {shp_path} a GeoParquet: {e}\")"
      ],
      "metadata": {
        "id": "vYi2L4Q_-x0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creación de la Tabla `manzanas` en DuckDB - El Vínculo entre SQL y GeoParquet**\n",
        "\n",
        "**Concepto Clave: Integra Shapefiles Convertidos a una Tabla SQL**\n",
        "\n",
        "1. **Lectura Directa de GeoParquet**\n",
        "  \n",
        "  - DuckDB puede leer Parquet nativamente:\n",
        "    \n",
        "    ```sql\n",
        "    SELECT *\n",
        "    FROM read_parquet('09m.geoparquet')\n",
        "    LIMIT 0\n",
        "    ```\n",
        "    \n",
        "    - `LIMIT 0` permite **inspeccionar** la estructura de columnas sin cargar datos completos.\n",
        "2. **Creación de la Tabla `manzanas`**\n",
        "  \n",
        "  - Para persistir estos datos en DuckDB, se hace:\n",
        "    \n",
        "    ```sql\n",
        "    CREATE TABLE manzanas AS\n",
        "    SELECT {select_cols}\n",
        "    FROM read_parquet('{parquet_path}');\n",
        "    ```\n",
        "    \n",
        "  - Se incluyen todas las columnas, incluyendo la columna `geometry`.\n",
        "    \n",
        "3. **Columna `geometry`**\n",
        "  \n",
        "  - Este es el **campo geoespacial** que contiene los polígonos de cada manzana.\n",
        "  - Con la extensión `spatial` de DuckDB, podemos ejecutar **funciones espaciales** sobre esta columna (`ST_Area(geometry)`, `ST_Intersection`, etc.).\n",
        "4. **¿Por Qué Tabla en Lugar de Consultas Temporales?**\n",
        "  \n",
        "  - Al crear una tabla permanente (`CREATE TABLE ...`), podemos reutilizarla **fácilmente** en posteriores consultas SQL, sin depender de leer el archivo Parquet una y otra vez.\n",
        "  - También, si se necesitan **índices** u optimizaciones adicionales, se podrían aplicar en la tabla creada.\n",
        "\n",
        "\n",
        "DuckDB, potenciado con la extensión `spatial`, **traduce** el archivo GeoParquet a una tabla SQL con columna de geometría. Así unificamos el mundo tabular (SQL) con el geoespacial (mapas), **abriendo puertas** para consultas y análisis espaciales de siguiente nivel."
      ],
      "metadata": {
        "id": "l4MWEPBKmXmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 8 Crear Tabla Geo\n",
        "# ================\n",
        "\n",
        "con.execute(\"DROP TABLE IF EXISTS manzanas;\")\n",
        "print(\"Tabla 'manzanas' eliminada si existía previamente.\")\n",
        "# Obtener las columnas del GeoParquet\n",
        "columns = con.execute(f\"SELECT * FROM read_parquet('{parquet_path}') LIMIT 0\").description\n",
        "column_names = [col[0] for col in columns]\n",
        "print(\"Columnas en el GeoParquet:\", column_names)\n",
        "\n",
        "# Verificar si 'geometry' está presente\n",
        "if 'geometry' in column_names:\n",
        "    non_geometry_cols = [col for col in column_names if col != 'geometry']\n",
        "    # Crear una cadena separada por comas de las columnas excluyendo 'geometry'\n",
        "    select_cols = ', '.join(non_geometry_cols) + ', geometry'\n",
        "else:\n",
        "    select_cols = ', '.join(column_names)\n",
        "\n",
        "con.execute(f\"\"\"\n",
        "  CREATE TABLE manzanas AS\n",
        "  SELECT\n",
        "      {select_cols}\n",
        "  FROM read_parquet('{parquet_path}');\n",
        "\"\"\")\n",
        "print(\"Tabla 'manzanas' creada con éxito.\")"
      ],
      "metadata": {
        "id": "b7KBH88TDomh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Unión de Datos Censales y Geometrías - Creando `censo_geo`**\n",
        "\n",
        "**Concepto Clave: JOIN Espacial a través de Claves de Identificación (CVEGEO)**\n",
        "\n",
        "1. **La Clave `CVEGEO`: La Llave Maestra**\n",
        "  \n",
        "  - Cada manzana tiene una **clave geográfica** (CVEGEO) única, construida con la concatenación de:\n",
        "    \n",
        "    ```\n",
        "    [2 dígitos de ENTIDAD] +\n",
        "    [3 dígitos de MUNICIPIO] +\n",
        "    [4 dígitos de LOCALIDAD] +\n",
        "    [4 dígitos de AGEB] +\n",
        "    [3 dígitos de MZA]\n",
        "    ```\n",
        "    \n",
        "  - El censo tabular también usa esta misma clave para identificar la manzana.\n",
        "    \n",
        "  - Por eso, la unión **no necesita** un “join espacial” (basado en geometría), sino un **join relacional** basado en `CVEGEO`.\n",
        "    \n",
        "2. **Construcción de `censo_geo`**\n",
        "  \n",
        "  - El script hace un `WITH censo AS (...)`, creando la columna `CVEGEO` dentro de la tabla `censo_all`, y un `WITH shp AS (...)` para la tabla `manzanas`.\n",
        "    \n",
        "  - Finalmente:\n",
        "    \n",
        "    ```sql\n",
        "    SELECT\n",
        "      c.CVEGEO,\n",
        "      c.[otras columnas censales],\n",
        "      s.geometry\n",
        "    FROM censo c\n",
        "    LEFT JOIN shp s USING (CVEGEO)\n",
        "    ```\n",
        "    \n",
        "  - *¿Qué hace `LEFT JOIN`?*\n",
        "    \n",
        "    - Toma todos los registros de `censo` (lado izquierdo), y si hay coincidencia de `CVEGEO` en `shp`, une la geometría. Si no hay coincidencia, no elimina las filas del censo (pero geometry quedaría nulo).\n",
        "  - El resultado se guarda en la nueva tabla `censo_geo`.\n",
        "    \n",
        "3. **¿Por Qué una Nueva Tabla?**\n",
        "  \n",
        "  - `censo_geo` combina la **dimensión tabular** (población, viviendas, etc.) con la **dimensión geoespacial** (polígonos de manzana).\n",
        "  - Al quedar en una sola tabla, se facilita la **visualización**, la **exportación** y las **consultas** (por ejemplo, “SELECT * WHERE POBTOT > 1000 Y geometry...”).\n",
        "4. **Importancia de la Unión**\n",
        "  \n",
        "  - Esta fusión **convierte** cada manzana en un objeto geoespacial **enriquecido** con datos censales.\n",
        "  - Es el **núcleo** de cualquier análisis posterior: de aquí podemos derivar mapas temáticos, estimar distancias, densidades, etc.\n",
        "\n",
        "\n",
        "La tabla `censo_geo` es la **joya** de la integración de datos: cada polígono de manzana se vincula a sus atributos demográficos correspondientes. A partir de aquí, la información **\"vive\"** tanto en el mundo espacial como en el mundo tabular, lista para ser explorada y analizada."
      ],
      "metadata": {
        "id": "joeR45HtnT9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 9. Crear CVEGEO y Unir con censo_all\n",
        "# ================\n",
        "con.execute(\"DROP TABLE IF EXISTS censo_geo;\")\n",
        "\n",
        "# Obtener la lista de columnas de la tabla censo_all\n",
        "# para evitar colocar todos los encabezados de forma manual.\n",
        "column_names = [row[0] for row in con.execute(\"DESCRIBE censo_all;\").fetchall()]\n",
        "\n",
        "# Excluir las columnas que no se necesitan en la tabla censo\n",
        "exclude_columns = ['ENTIDAD', 'MUN', 'LOC', 'AGEB', 'MZA']\n",
        "select_columns_censo = [col for col in column_names if col not in exclude_columns]\n",
        "\n",
        "# Construir la consulta SQL dinámicamente\n",
        "con.execute(f\"\"\"\n",
        "    CREATE TABLE censo_geo AS\n",
        "    WITH censo AS (\n",
        "        SELECT\n",
        "            *,\n",
        "            CONCAT( LPAD(ENTIDAD,2,'0'),\n",
        "                    LPAD(MUN,3,'0'),\n",
        "                    LPAD(LOC,4,'0'),\n",
        "                    LPAD(AGEB,4,'0'),\n",
        "                    LPAD(MZA,3,'0')) AS CVEGEO\n",
        "        FROM censo_all\n",
        "    ),\n",
        "    shp AS (\n",
        "        SELECT\n",
        "            CVE_ENT,\n",
        "            CVE_MUN,\n",
        "            CVE_LOC,\n",
        "            CVE_AGEB,\n",
        "            CVE_MZA,\n",
        "            CONCAT( LPAD(CVE_ENT,2,'0'),\n",
        "                    LPAD(CVE_MUN,3,'0'),\n",
        "                    LPAD(CVE_LOC,4,'0'),\n",
        "                    LPAD(CVE_AGEB,4,'0'),\n",
        "                    LPAD(CVE_MZA,3,'0')) AS CVEGEO,\n",
        "            geometry\n",
        "        FROM manzanas\n",
        "    )\n",
        "    SELECT\n",
        "        c.CVEGEO,\n",
        "        {', '.join(['c.' + col for col in select_columns_censo])},\n",
        "        s.geometry\n",
        "    FROM censo c\n",
        "    LEFT JOIN shp s USING (CVEGEO)\n",
        "    WHERE s.CVEGEO IS NOT NULL;\n",
        "\"\"\")\n",
        "\n",
        "print(\"Tabla 'censo_geo' creada con la unión de censo + shapefile en DuckDB.\")"
      ],
      "metadata": {
        "id": "fdazHuWbFgWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Limpieza Final y Revisión - Puliendo el Entorno de Trabajo**\n",
        "\n",
        "**Concepto Clave: Buenas Prácticas al Terminar un Proceso de ETL**\n",
        "\n",
        "1. **Eliminación de Tablas Intermedias**\n",
        "  \n",
        "  - El código:\n",
        "    \n",
        "    ```sql\n",
        "    DROP TABLE IF EXISTS censo_all;\n",
        "    DROP TABLE IF EXISTS manzanas;\n",
        "    ```\n",
        "    \n",
        "  - Tras la creación de `censo_geo`, las tablas `censo_all` y `manzanas` pueden considerarse **intermedias** (ya no se requieren). Al eliminarlas, **limpiamos** la base de datos de objetos que no se usarán más, **evitando confusiones** y reduciendo el tamaño del archivo `.duckdb`.\n",
        "    \n",
        "2. **Verificación de Tablas Restantes**\n",
        "  \n",
        "  - `SHOW TABLES;` nos confirma que solo queda `censo_geo` (o las tablas que queramos retener).\n",
        "  - Conocer tu inventario de tablas es esencial para **transparencia** y para **documentar** qué contiene tu base final.\n",
        "3. **Commit y Cierre de Conexión**\n",
        "  \n",
        "  - Un `con.commit()` final y `con.close()` aseguran que se **guarden los cambios** y se liberen los recursos usados por la conexión a la base de datos.\n",
        "  - *¿Por qué es importante?*\n",
        "    - Prevenir bloqueos y **asegurar** que no queden transacciones abiertas.\n",
        "    - Es parte de un **ciclo de vida saludable** de la base de datos.\n",
        "4. **Ventajas de un Flujo \"Limpio\"**\n",
        "  \n",
        "  - Facilita que el archivo `.duckdb` sea más ligero y contenga **solo** las tablas relevantes.\n",
        "  - Permite retomar el trabajo después sin “ruido” o tablas duplicadas.\n",
        "  - Muestra una **metodología clara** de inicio y fin del proceso de ETL (Extract, Transform, Load).\n",
        "\n",
        "\n",
        "Finalizar con un ambiente limpio y ordenado es **tan importante** como comenzar con buena organización. Al eliminar lo innecesario y cerrar la conexión, garantizamos que nuestro proceso sea **seguro, reproducible** y listo para ser retomado sin contratiempos en el futuro."
      ],
      "metadata": {
        "id": "lI75X3bKn00b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================\n",
        "# 10. Eliminar Tablas Intermedias\n",
        "# ================\n",
        "# Eliminar tablas que ya no son necesarias\n",
        "con.execute(\"DROP TABLE IF EXISTS censo_all;\")\n",
        "con.execute(\"DROP TABLE IF EXISTS manzanas;\")\n",
        "\n",
        "# Verificar tablas restantes\n",
        "tablas = con.execute(\"SHOW TABLES;\").fetchdf()\n",
        "print(\"Tablas en la base de datos final:\")\n",
        "print(tablas)"
      ],
      "metadata": {
        "id": "nuFtyhD8Fu5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Confirmar transacciones pendientes\n",
        "con.commit()  # Aunque DuckDB auto-comitea, es buena práctica explícita\n",
        "\n",
        "# 2. Cerrar conexión\n",
        "con.close()"
      ],
      "metadata": {
        "id": "J12y23LjGhmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fin\")"
      ],
      "metadata": {
        "id": "YCL_06GMG3Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusiones y Próximos Pasos**\n",
        "\n",
        "**Recapitulación Breve de lo Aprendido**\n",
        "\n",
        "1. **Organización y Estructura**\n",
        "  - Mantener un directorio limpio y bien definido para descargas de CSV, shapefiles, y resultados.\n",
        "2. **Descarga Programática**\n",
        "  - Con `requests` y `tqdm`, automatizamos la adquisición de datos, ahorrando tiempo y evitando descargas redundantes.\n",
        "3. **Extracción de ZIPs y Shapefiles**\n",
        "  - Uso de `zipfile` y funciones específicas para conservar la integridad de los archivos .shp, .dbf, .shx, .prj, etc.\n",
        "4. **Conversión a Formatos Eficientes**\n",
        "  - `GeoParquet` para reemplazar Shapefiles y optimizar el análisis.\n",
        "5. **Integración en DuckDB**\n",
        "  - Creación y unión de tablas, manejo simultáneo de datos tabulares y espaciales, con **SQL estándar**.\n",
        "6. **Limpieza Final**\n",
        "  - Depuración de tablas intermedias y cierre de conexiones para una base de datos confiable y sostenible.\n",
        "\n",
        "**¿Qué sigue?**\n",
        "\n",
        "- **Análisis y Consultas Espaciales:** Explorar tus datos con SQL espacial (funciones `ST_Intersects`, `ST_Buffer`, etc.).\n",
        "- **Visualizaciones Interactivas:** Combinar `censo_geo` con librerías como `folium` o `geoplot` para crear mapas temáticos donde se muestren variables sociodemográficas.\n",
        "- **Integración con Datos Externos:** Añadir capas de información (transporte, salud, educación) y cruzar con la base de `censo_geo`.\n",
        "- **Machine Learning Geoespacial:** Emplear clustering espacial, modelos predictivos que incluyan la componente de ubicación, etc.\n",
        "- **Automatizar Flujo Completo:** Expandir la idea a un pipeline más grande que corra diariamente o periódicamente si se requiere data fresca (para datos no estáticos, como estadísticas de movilidad o incidentes de seguridad).\n",
        "\n",
        "Has aprendido un método robusto para **bajar, organizar, analizar y unir** datos censales y geoespaciales, todo en un solo flujo de trabajo en Python. Esta base te abrirá un **mundo** de posibilidades: desde generar mapas de calor sobre densidad poblacional hasta investigaciones más avanzadas en planeación urbana, economía y políticas públicas."
      ],
      "metadata": {
        "id": "_ziiRAB3oDrs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsjjJjb0oKAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}